{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2671f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from transformers import BartConfig, FlaxAutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae0d6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35851f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlaxAutoModelForSeq2SeqLM_from_fnlp_config  fnlp-bart-large-chinese-pytorch\r\n",
      "Untitled1.ipynb\t\t\t\t    fnlp_bart.config\r\n",
      "Untitled2.ipynb\t\t\t\t    fnlp_jax_version\r\n",
      "create_JAX_Chinese_BART.ipynb\t\t    test-serialize.msgpack\r\n",
      "flax_facebook_bart\t\t\t    vqgan-jax\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c3de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = BartConfig.from_pretrained(\"fnlp_bart.config\")  # 利用fnlp/bart-large-chinese的config.json来创建结构一样的Flax模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ee0416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 模型结构是 FlaxAutoModelForSeq2SeqLM\n",
    "flax_model = FlaxAutoModelForSeq2SeqLM.from_config(bart_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aab3114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['final_logits_bias', 'model'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flax_model.params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843821aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.DeviceArray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flax_model.params['final_logits_bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820698aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21128, 1024)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flax_model.params['model']['shared']['embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e75b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8355f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载pytorch版本的fnlp/bart-large-chinese\n",
    "from transformers import BertTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "250f7591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"fnlp/bart-large-chinese\")\n",
    "pytorch_model = BartForConditionalGeneration.from_pretrained(\"fnlp/bart-large-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d402da6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '北 京 是 中 华 人 民 共 和 国 的 首 都'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2text_generator = Text2TextGenerationPipeline(pytorch_model, tokenizer)  \n",
    "text2text_generator(\"北京是[MASK]的首都\", max_length=50, do_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a3bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch_model.save_pretrained(\"fnlp-bart-large-chinese-pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eca21ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t   special_tokens_map.json  vocab.txt\r\n",
      "pytorch_model.bin  tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls fnlp-bart-large-chinese-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b561e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained(\"fnlp-bart-large-chinese-pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a51c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t   special_tokens_map.json  vocab.txt\r\n",
      "pytorch_model.bin  tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls fnlp-bart-large-chinese-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e75f1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用pytorch的参数权重对flax的参数权重进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a5b3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_params = {}\n",
    "for k, v in pytorch_model.named_parameters():\n",
    "    pytorch_params[k] = v.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48f13053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bart.modeling_bart.BartForConditionalGeneration"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pytorch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e61da69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    final_logits_bias: ShapeDtypeStruct(shape=(1, 21128), dtype=float32),\n",
       "    model: {\n",
       "        decoder: {\n",
       "            embed_positions: {\n",
       "                embedding: ShapeDtypeStruct(shape=(514, 1024), dtype=float32),\n",
       "            },\n",
       "            layernorm_embedding: {\n",
       "                bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "            },\n",
       "            layers: {\n",
       "                0: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                1: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                10: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                11: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                2: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                3: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                4: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                5: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                6: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                7: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                8: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                9: {\n",
       "                    encoder_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    encoder_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "            },\n",
       "        },\n",
       "        encoder: {\n",
       "            embed_positions: {\n",
       "                embedding: ShapeDtypeStruct(shape=(514, 1024), dtype=float32),\n",
       "            },\n",
       "            layernorm_embedding: {\n",
       "                bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "            },\n",
       "            layers: {\n",
       "                0: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                1: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                10: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                11: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                2: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                3: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                4: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                5: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                6: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                7: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                8: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "                9: {\n",
       "                    fc1: {\n",
       "                        bias: ShapeDtypeStruct(shape=(4096,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(1024, 4096), dtype=float32),\n",
       "                    },\n",
       "                    fc2: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        kernel: ShapeDtypeStruct(shape=(4096, 1024), dtype=float32),\n",
       "                    },\n",
       "                    final_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                    self_attn: {\n",
       "                        k_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        out_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        q_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                        v_proj: {\n",
       "                            bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                            kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
       "                        },\n",
       "                    },\n",
       "                    self_attn_layer_norm: {\n",
       "                        bias: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                        scale: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
       "                    },\n",
       "                },\n",
       "            },\n",
       "        },\n",
       "        shared: {\n",
       "            embedding: ShapeDtypeStruct(shape=(21128, 1024), dtype=float32),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flax_model.params_shape_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a9b8b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=21128, bias=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f9ebabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55985069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af362469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21128, 1024)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flax_model.params['model']['shared']['embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1718a014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21128, 1024)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.asarray(pytorch_params['model.shared.weight']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03e2b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "flax_model.params['model']['shared']['embedding'] = jnp.asarray(pytorch_params['model.shared.weight'])\n",
    "\n",
    "flax_model.params['model']['encoder']['embed_positions']['embedding'] = \\\n",
    "    jnp.asarray(pytorch_params['model.encoder.embed_positions.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d0bad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "flax_model.params['model']['encoder']['layernorm_embedding']['scale'] = \\\n",
    "    jnp.asarray(pytorch_params['model.encoder.layernorm_embedding.weight'])\n",
    "\n",
    "flax_model.params['model']['encoder']['layernorm_embedding']['bias'] = \\\n",
    "    jnp.asarray(pytorch_params['model.encoder.layernorm_embedding.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5233b4ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.shared.weight torch.float32\n",
      "model.encoder.embed_positions.weight torch.float32\n",
      "model.encoder.layers.0.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.0.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.0.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.0.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.0.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.0.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.0.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.0.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.0.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.0.fc1.weight torch.float32\n",
      "model.encoder.layers.0.fc1.bias torch.float32\n",
      "model.encoder.layers.0.fc2.weight torch.float32\n",
      "model.encoder.layers.0.fc2.bias torch.float32\n",
      "model.encoder.layers.0.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.0.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.1.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.1.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.1.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.1.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.1.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.1.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.1.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.1.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.1.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.1.fc1.weight torch.float32\n",
      "model.encoder.layers.1.fc1.bias torch.float32\n",
      "model.encoder.layers.1.fc2.weight torch.float32\n",
      "model.encoder.layers.1.fc2.bias torch.float32\n",
      "model.encoder.layers.1.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.1.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.2.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.2.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.2.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.2.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.2.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.2.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.2.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.2.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.2.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.2.fc1.weight torch.float32\n",
      "model.encoder.layers.2.fc1.bias torch.float32\n",
      "model.encoder.layers.2.fc2.weight torch.float32\n",
      "model.encoder.layers.2.fc2.bias torch.float32\n",
      "model.encoder.layers.2.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.2.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.3.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.3.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.3.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.3.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.3.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.3.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.3.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.3.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.3.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.3.fc1.weight torch.float32\n",
      "model.encoder.layers.3.fc1.bias torch.float32\n",
      "model.encoder.layers.3.fc2.weight torch.float32\n",
      "model.encoder.layers.3.fc2.bias torch.float32\n",
      "model.encoder.layers.3.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.3.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.4.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.4.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.4.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.4.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.4.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.4.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.4.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.4.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.4.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.4.fc1.weight torch.float32\n",
      "model.encoder.layers.4.fc1.bias torch.float32\n",
      "model.encoder.layers.4.fc2.weight torch.float32\n",
      "model.encoder.layers.4.fc2.bias torch.float32\n",
      "model.encoder.layers.4.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.4.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.5.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.5.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.5.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.5.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.5.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.5.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.5.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.5.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.5.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.5.fc1.weight torch.float32\n",
      "model.encoder.layers.5.fc1.bias torch.float32\n",
      "model.encoder.layers.5.fc2.weight torch.float32\n",
      "model.encoder.layers.5.fc2.bias torch.float32\n",
      "model.encoder.layers.5.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.5.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.6.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.6.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.6.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.6.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.6.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.6.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.6.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.6.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.6.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.6.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.6.fc1.weight torch.float32\n",
      "model.encoder.layers.6.fc1.bias torch.float32\n",
      "model.encoder.layers.6.fc2.weight torch.float32\n",
      "model.encoder.layers.6.fc2.bias torch.float32\n",
      "model.encoder.layers.6.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.6.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.7.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.7.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.7.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.7.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.7.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.7.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.7.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.7.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.7.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.7.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.7.fc1.weight torch.float32\n",
      "model.encoder.layers.7.fc1.bias torch.float32\n",
      "model.encoder.layers.7.fc2.weight torch.float32\n",
      "model.encoder.layers.7.fc2.bias torch.float32\n",
      "model.encoder.layers.7.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.7.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.8.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.8.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.8.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.8.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.8.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.8.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.8.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.8.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.8.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.8.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.8.fc1.weight torch.float32\n",
      "model.encoder.layers.8.fc1.bias torch.float32\n",
      "model.encoder.layers.8.fc2.weight torch.float32\n",
      "model.encoder.layers.8.fc2.bias torch.float32\n",
      "model.encoder.layers.8.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.8.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.9.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.9.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.9.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.9.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.9.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.9.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.9.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.9.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.9.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.9.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.9.fc1.weight torch.float32\n",
      "model.encoder.layers.9.fc1.bias torch.float32\n",
      "model.encoder.layers.9.fc2.weight torch.float32\n",
      "model.encoder.layers.9.fc2.bias torch.float32\n",
      "model.encoder.layers.9.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.9.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.10.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.10.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.10.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.10.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.10.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.10.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.10.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.10.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.10.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.10.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.10.fc1.weight torch.float32\n",
      "model.encoder.layers.10.fc1.bias torch.float32\n",
      "model.encoder.layers.10.fc2.weight torch.float32\n",
      "model.encoder.layers.10.fc2.bias torch.float32\n",
      "model.encoder.layers.10.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.10.final_layer_norm.bias torch.float32\n",
      "model.encoder.layers.11.self_attn.k_proj.weight torch.float32\n",
      "model.encoder.layers.11.self_attn.k_proj.bias torch.float32\n",
      "model.encoder.layers.11.self_attn.v_proj.weight torch.float32\n",
      "model.encoder.layers.11.self_attn.v_proj.bias torch.float32\n",
      "model.encoder.layers.11.self_attn.q_proj.weight torch.float32\n",
      "model.encoder.layers.11.self_attn.q_proj.bias torch.float32\n",
      "model.encoder.layers.11.self_attn.out_proj.weight torch.float32\n",
      "model.encoder.layers.11.self_attn.out_proj.bias torch.float32\n",
      "model.encoder.layers.11.self_attn_layer_norm.weight torch.float32\n",
      "model.encoder.layers.11.self_attn_layer_norm.bias torch.float32\n",
      "model.encoder.layers.11.fc1.weight torch.float32\n",
      "model.encoder.layers.11.fc1.bias torch.float32\n",
      "model.encoder.layers.11.fc2.weight torch.float32\n",
      "model.encoder.layers.11.fc2.bias torch.float32\n",
      "model.encoder.layers.11.final_layer_norm.weight torch.float32\n",
      "model.encoder.layers.11.final_layer_norm.bias torch.float32\n",
      "model.encoder.layernorm_embedding.weight torch.float32\n",
      "model.encoder.layernorm_embedding.bias torch.float32\n",
      "model.decoder.embed_positions.weight torch.float32\n",
      "model.decoder.layers.0.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.0.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.0.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.0.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.0.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.0.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.0.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.0.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.0.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.0.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.0.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.0.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.0.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.0.fc1.weight torch.float32\n",
      "model.decoder.layers.0.fc1.bias torch.float32\n",
      "model.decoder.layers.0.fc2.weight torch.float32\n",
      "model.decoder.layers.0.fc2.bias torch.float32\n",
      "model.decoder.layers.0.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.0.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.1.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.1.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.1.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.1.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.1.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.1.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.1.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.1.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.1.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.1.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.1.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.1.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.1.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.1.fc1.weight torch.float32\n",
      "model.decoder.layers.1.fc1.bias torch.float32\n",
      "model.decoder.layers.1.fc2.weight torch.float32\n",
      "model.decoder.layers.1.fc2.bias torch.float32\n",
      "model.decoder.layers.1.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.1.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.2.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.2.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.2.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.2.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.2.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.2.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.2.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.2.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.2.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.2.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.2.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.2.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.2.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.2.fc1.weight torch.float32\n",
      "model.decoder.layers.2.fc1.bias torch.float32\n",
      "model.decoder.layers.2.fc2.weight torch.float32\n",
      "model.decoder.layers.2.fc2.bias torch.float32\n",
      "model.decoder.layers.2.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.2.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.3.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.3.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.3.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.3.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.3.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.3.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.3.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.3.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.3.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.3.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.3.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.3.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.3.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.3.fc1.weight torch.float32\n",
      "model.decoder.layers.3.fc1.bias torch.float32\n",
      "model.decoder.layers.3.fc2.weight torch.float32\n",
      "model.decoder.layers.3.fc2.bias torch.float32\n",
      "model.decoder.layers.3.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.3.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.4.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.4.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.4.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.4.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.4.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.4.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.4.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.4.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.4.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.4.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.4.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.4.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.4.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.4.fc1.weight torch.float32\n",
      "model.decoder.layers.4.fc1.bias torch.float32\n",
      "model.decoder.layers.4.fc2.weight torch.float32\n",
      "model.decoder.layers.4.fc2.bias torch.float32\n",
      "model.decoder.layers.4.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.4.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.5.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.5.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.5.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.5.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.5.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.5.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.5.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.5.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.5.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.5.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.5.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.5.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.5.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.5.fc1.weight torch.float32\n",
      "model.decoder.layers.5.fc1.bias torch.float32\n",
      "model.decoder.layers.5.fc2.weight torch.float32\n",
      "model.decoder.layers.5.fc2.bias torch.float32\n",
      "model.decoder.layers.5.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.5.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.6.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.6.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.6.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.6.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.6.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.6.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.6.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.6.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.6.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.6.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.6.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.6.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.6.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.6.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.6.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.6.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.6.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.6.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.6.fc1.weight torch.float32\n",
      "model.decoder.layers.6.fc1.bias torch.float32\n",
      "model.decoder.layers.6.fc2.weight torch.float32\n",
      "model.decoder.layers.6.fc2.bias torch.float32\n",
      "model.decoder.layers.6.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.6.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.7.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.7.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.7.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.7.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.7.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.7.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.7.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.7.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.7.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.7.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.7.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.7.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.7.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.7.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.7.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.7.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.7.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.7.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.7.fc1.weight torch.float32\n",
      "model.decoder.layers.7.fc1.bias torch.float32\n",
      "model.decoder.layers.7.fc2.weight torch.float32\n",
      "model.decoder.layers.7.fc2.bias torch.float32\n",
      "model.decoder.layers.7.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.7.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.8.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.8.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.8.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.8.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.8.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.8.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.8.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.8.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.8.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.8.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.8.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.8.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.8.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.8.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.8.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.8.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.8.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.8.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.8.fc1.weight torch.float32\n",
      "model.decoder.layers.8.fc1.bias torch.float32\n",
      "model.decoder.layers.8.fc2.weight torch.float32\n",
      "model.decoder.layers.8.fc2.bias torch.float32\n",
      "model.decoder.layers.8.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.8.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.9.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.9.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.9.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.9.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.9.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.9.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.9.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.9.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.9.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.9.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.9.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.9.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.9.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.9.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.9.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.9.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.9.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.9.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.9.fc1.weight torch.float32\n",
      "model.decoder.layers.9.fc1.bias torch.float32\n",
      "model.decoder.layers.9.fc2.weight torch.float32\n",
      "model.decoder.layers.9.fc2.bias torch.float32\n",
      "model.decoder.layers.9.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.9.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.10.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.10.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.10.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.10.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.10.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.10.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.10.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.10.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.10.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.10.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.10.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.10.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.10.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.10.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.10.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.10.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.10.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.10.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.10.fc1.weight torch.float32\n",
      "model.decoder.layers.10.fc1.bias torch.float32\n",
      "model.decoder.layers.10.fc2.weight torch.float32\n",
      "model.decoder.layers.10.fc2.bias torch.float32\n",
      "model.decoder.layers.10.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.10.final_layer_norm.bias torch.float32\n",
      "model.decoder.layers.11.self_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.11.self_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.11.self_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.11.self_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.11.self_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.11.self_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.11.self_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.11.self_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.11.self_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.11.self_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.11.encoder_attn.k_proj.weight torch.float32\n",
      "model.decoder.layers.11.encoder_attn.k_proj.bias torch.float32\n",
      "model.decoder.layers.11.encoder_attn.v_proj.weight torch.float32\n",
      "model.decoder.layers.11.encoder_attn.v_proj.bias torch.float32\n",
      "model.decoder.layers.11.encoder_attn.q_proj.weight torch.float32\n",
      "model.decoder.layers.11.encoder_attn.q_proj.bias torch.float32\n",
      "model.decoder.layers.11.encoder_attn.out_proj.weight torch.float32\n",
      "model.decoder.layers.11.encoder_attn.out_proj.bias torch.float32\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.weight torch.float32\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.bias torch.float32\n",
      "model.decoder.layers.11.fc1.weight torch.float32\n",
      "model.decoder.layers.11.fc1.bias torch.float32\n",
      "model.decoder.layers.11.fc2.weight torch.float32\n",
      "model.decoder.layers.11.fc2.bias torch.float32\n",
      "model.decoder.layers.11.final_layer_norm.weight torch.float32\n",
      "model.decoder.layers.11.final_layer_norm.bias torch.float32\n",
      "model.decoder.layernorm_embedding.weight torch.float32\n",
      "model.decoder.layernorm_embedding.bias torch.float32\n"
     ]
    }
   ],
   "source": [
    "for k, v in pytorch_model.named_parameters():\n",
    "    print(k, v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "775eafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_encoder_param_to_flax(layer_num, transpose=False):\n",
    "    \"\"\"from '0' to '11'\n",
    "    \"\"\"\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn']['k_proj']['kernel'] = \\\n",
    "    jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn.k_proj.weight'.format(layer_num)].T)  # 这里虽然是(1024, 1024)，但是还是用transpose尝试下\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn']['k_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn.k_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn']['v_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn.v_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn']['v_proj']['bias'] = \\\n",
    "            jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn.v_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn']['q_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn.q_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn']['q_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn.q_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn']['out_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn.out_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn']['out_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn.out_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn_layer_norm']['scale'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn_layer_norm.weight'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['self_attn_layer_norm']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.self_attn_layer_norm.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['fc1']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.fc1.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['fc1']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.fc1.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['fc2']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.fc2.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['fc2']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.fc2.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['final_layer_norm']['scale'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.final_layer_norm.weight'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['encoder']['layers'][layer_num]['final_layer_norm']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.encoder.layers.{}.final_layer_norm.bias'.format(layer_num)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d6ca7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "#     print(str(i))\n",
    "    pytorch_encoder_param_to_flax(str(i), transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59b68878",
   "metadata": {},
   "outputs": [],
   "source": [
    "flax_model.params['model']['decoder']['embed_positions']['embedding'] = \\\n",
    "   jnp.asarray(pytorch_params['model.decoder.embed_positions.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20a2628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_to_flax_decoder(layer_num, transpose=False):\n",
    "    \"\"\"from '0' to '11' \"\"\"\n",
    "    \n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn']['k_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn.k_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn']['k_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn.k_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn']['v_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn.v_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn']['v_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn.v_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn']['q_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn.q_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn']['q_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn.q_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn']['out_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn.out_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn']['out_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn.out_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn_layer_norm']['scale'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn_layer_norm.weight'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['self_attn_layer_norm']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.self_attn_layer_norm.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn']['k_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn.k_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn']['k_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn.k_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn']['v_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn.v_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn']['v_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn.v_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn']['q_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn.q_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn']['q_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn.q_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn']['out_proj']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn.out_proj.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn']['out_proj']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn.out_proj.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn_layer_norm']['scale'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn_layer_norm.weight'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['encoder_attn_layer_norm']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.encoder_attn_layer_norm.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['fc1']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.fc1.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['fc1']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.fc1.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['fc2']['kernel'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.fc2.weight'.format(layer_num)].T)\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['fc2']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.fc2.bias'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['final_layer_norm']['scale'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.final_layer_norm.weight'.format(layer_num)])\n",
    "\n",
    "    flax_model.params['model']['decoder']['layers'][layer_num]['final_layer_norm']['bias'] = \\\n",
    "        jnp.asarray(pytorch_params['model.decoder.layers.{}.final_layer_norm.bias'.format(layer_num)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7511ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    pytorch_to_flax_decoder(str(i), transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb1fa430",
   "metadata": {},
   "outputs": [],
   "source": [
    "flax_model.params['model']['decoder']['layernorm_embedding']['scale'] = \\\n",
    "    jnp.asarray(pytorch_params['model.decoder.layernorm_embedding.weight'])\n",
    "\n",
    "flax_model.params['model']['decoder']['layernorm_embedding']['bias'] = \\\n",
    "    jnp.asarray(pytorch_params['model.decoder.layernorm_embedding.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d7692e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flax_inputs = tokenizer(\"北京是[MASK]的首都\", max_length=50, truncation=True, return_tensors=\"np\")\n",
    "pytorch_inputs = tokenizer(\"北京是[MASK]的首都\", max_length=50, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27646409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.tokenization_utils_base.BatchEncoding,\n",
       " transformers.tokenization_utils_base.BatchEncoding)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flax_inputs), type(pytorch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22413cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1266,  776, 3221,  103, 4638, 7674, 6963,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dade885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[ 101, 1266,  776, 3221,  103, 4638, 7674, 6963,  102]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flax_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a421a170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flax_inputs.pop(\"token_type_ids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93743608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_inputs.pop(\"token_type_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52e6b7ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pytorch_model(**pytorch_inputs, output_hidden_states=True).encoder_hidden_states)\n",
    "# encoder有12层，怎么这里13个tensor? 第一个embedding，也就是模型encoder的input，是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33fe02a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00570121, -0.04509331, -0.02713029, ...,  0.00194991,\n",
       "         -0.00537348,  0.03764018],\n",
       "        [-0.00569891, -0.04485099, -0.02726786, ...,  0.00204078,\n",
       "         -0.00477894,  0.03749378],\n",
       "        [ 0.19315398, -0.70756376,  0.19373162, ...,  0.3463536 ,\n",
       "         -0.4556533 , -0.2645676 ],\n",
       "        ...,\n",
       "        [ 0.30669293, -0.2734287 , -0.24406989, ...,  0.67741376,\n",
       "          0.3688047 ,  0.02863172],\n",
       "        [ 0.18938838,  0.06320691, -0.02779032, ...,  0.5631877 ,\n",
       "          0.06248561,  0.255242  ],\n",
       "        [ 0.18506819, -0.29937473, -0.2105657 , ..., -0.0947461 ,\n",
       "          0.3477654 ,  0.3963469 ]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model(**pytorch_inputs, output_hidden_states=True).encoder_hidden_states[-1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32983d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[-0.00565069, -0.04506724, -0.02707809, ...,  0.00201641,\n",
       "               -0.00540399,  0.03763571],\n",
       "              [-0.00570531, -0.04482179, -0.02721935, ...,  0.00211693,\n",
       "               -0.00483827,  0.03747253],\n",
       "              [ 0.19419202, -0.7112492 ,  0.19369853, ...,  0.34371915,\n",
       "               -0.4576627 , -0.2657903 ],\n",
       "              ...,\n",
       "              [ 0.30658963, -0.27588403, -0.24333625, ...,  0.67852193,\n",
       "                0.36913538,  0.02680317],\n",
       "              [ 0.18850312,  0.06072142, -0.02829674, ...,  0.5640527 ,\n",
       "                0.06140786,  0.25440294],\n",
       "              [ 0.18477756, -0.30004922, -0.20897329, ..., -0.09487949,\n",
       "                0.3451631 ,  0.39601344]]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flax_model(**flax_inputs, output_hidden_states=True).encoder_hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bae79806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=None, logits=tensor([[[-3.4801, -3.2454, -3.7454,  ..., -3.9595, -3.5990, -2.3568],\n",
       "         [-7.4768, -7.1581, -6.7530,  ..., -6.5231, -6.8271, -6.5757],\n",
       "         [-6.8346, -6.9497, -6.6919,  ..., -6.6677, -6.6467, -6.6120],\n",
       "         ...,\n",
       "         [-7.5542, -7.1186, -7.3049,  ..., -7.2940, -7.2962, -6.3193],\n",
       "         [-8.0730, -6.9977, -6.8776,  ..., -8.2508, -7.0762, -6.7484],\n",
       "         [-6.2054, -6.2399, -6.0311,  ..., -5.7494, -6.4311, -5.4911]]],\n",
       "       grad_fn=<AddBackward0>), past_key_values=((tensor([[[[ 2.6798e-01, -1.1209e+00, -3.2103e-01,  ...,  1.8051e+00,\n",
       "           -1.7247e+00,  5.5357e-01],\n",
       "          [ 5.6914e-01, -8.4339e-01, -4.0476e-01,  ...,  1.6013e+00,\n",
       "           -1.5293e+00,  5.0177e-01],\n",
       "          [ 4.8445e-01,  9.3623e-01,  2.1530e+00,  ..., -4.6448e+00,\n",
       "            2.1018e+00,  1.3145e+00],\n",
       "          ...,\n",
       "          [ 3.7149e-01,  3.9733e-01, -3.6626e-01,  ..., -6.3848e+00,\n",
       "            1.5529e+00,  5.8073e-04],\n",
       "          [-8.5041e-01,  2.5869e+00,  2.0783e+00,  ..., -3.5160e+00,\n",
       "            1.4315e+00,  1.5584e+00],\n",
       "          [-4.0818e-01, -5.7568e-01,  1.1887e+00,  ..., -3.8826e+00,\n",
       "            2.8712e+00,  1.5800e+00]],\n",
       "\n",
       "         [[ 7.3491e-01, -5.3955e-01,  4.6838e-02,  ...,  3.4127e-01,\n",
       "            4.5408e-01,  2.3745e-01],\n",
       "          [ 5.0594e-01, -4.2260e-03,  8.1939e-01,  ..., -8.7194e-02,\n",
       "            2.4259e-02, -2.7640e-01],\n",
       "          [-7.3247e-01,  4.0111e-01,  8.8245e-01,  ..., -2.9291e-01,\n",
       "            3.0786e-01,  1.0269e+00],\n",
       "          ...,\n",
       "          [-5.3537e-02,  1.5162e+00, -5.5354e-01,  ..., -1.0982e+00,\n",
       "           -1.3181e+00, -3.1516e-01],\n",
       "          [-5.1445e-01, -6.0329e-01,  4.0771e-01,  ..., -5.4102e-01,\n",
       "            9.7206e-01, -3.3505e-01],\n",
       "          [-9.1448e-02,  3.8053e-01,  4.2175e-01,  ...,  3.2092e-01,\n",
       "            1.4742e-01, -7.3746e-02]],\n",
       "\n",
       "         [[ 5.7300e-01, -4.1195e-03,  1.7010e+00,  ...,  1.2224e+00,\n",
       "            1.3379e+00, -1.3242e+00],\n",
       "          [-7.1910e-02,  1.2828e-01,  4.1429e-01,  ...,  1.4539e+00,\n",
       "            8.8213e-01, -9.6381e-01],\n",
       "          [ 3.5837e+00, -1.1004e+00,  1.6273e+00,  ..., -1.9308e+00,\n",
       "            2.8426e+00, -1.0946e+00],\n",
       "          ...,\n",
       "          [ 3.3490e+00, -6.5785e-01,  2.5063e+00,  ..., -2.6624e+00,\n",
       "            1.4302e+00, -1.7146e+00],\n",
       "          [ 2.8573e+00, -9.1680e-01,  2.6225e+00,  ..., -1.9666e+00,\n",
       "            2.4433e+00, -2.7713e-01],\n",
       "          [ 3.7325e+00, -1.5563e+00,  1.3083e+00,  ..., -1.7931e+00,\n",
       "            2.2101e+00, -6.1541e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0641e+00,  1.4168e+00, -4.2139e-01,  ...,  1.2772e+00,\n",
       "            6.2524e-01, -7.6027e-01],\n",
       "          [ 7.3997e-01,  6.7653e-01, -7.1437e-01,  ...,  9.8413e-01,\n",
       "            3.1955e-02, -5.3506e-01],\n",
       "          [ 1.9825e-01, -1.5415e+00,  1.2838e-01,  ..., -7.8872e-01,\n",
       "            1.0461e+00, -3.7808e-01],\n",
       "          ...,\n",
       "          [ 6.5410e-01, -1.0005e+00,  2.2884e+00,  ..., -3.5865e+00,\n",
       "            3.3633e+00,  1.0130e+00],\n",
       "          [-1.1035e+00, -1.7259e+00, -8.4944e-01,  ..., -1.3281e+00,\n",
       "            1.0897e+00,  3.7408e-01],\n",
       "          [-1.9813e-01, -1.6796e+00, -1.5254e+00,  ..., -7.7323e-01,\n",
       "            2.0471e+00, -4.8256e-01]],\n",
       "\n",
       "         [[-8.6230e-01, -7.7725e-01, -1.8418e-01,  ...,  5.9071e-01,\n",
       "           -1.9782e-01, -1.1596e+00],\n",
       "          [ 9.7239e-01, -1.3049e+00,  8.2203e-02,  ..., -5.7466e-02,\n",
       "            5.7775e-01, -1.4772e+00],\n",
       "          [-5.9892e-01,  1.3631e+00,  2.5241e-01,  ...,  6.1509e-01,\n",
       "            3.2552e-01,  1.4701e-01],\n",
       "          ...,\n",
       "          [-4.9469e-01,  1.5197e+00, -2.2542e+00,  ...,  3.7388e-01,\n",
       "            7.8178e-01,  2.1495e-01],\n",
       "          [-9.1141e-02,  5.0767e-01, -4.2946e-01,  ...,  1.4352e+00,\n",
       "            2.7021e-01,  5.5985e-01],\n",
       "          [-5.8830e-01,  1.3545e+00, -3.8955e-01,  ...,  1.3738e+00,\n",
       "           -4.5686e-02, -3.0015e-01]],\n",
       "\n",
       "         [[ 2.0916e+00,  6.1475e-01,  1.4686e+00,  ...,  3.1442e-01,\n",
       "            6.7411e-02,  8.8991e-01],\n",
       "          [ 2.3914e+00,  3.0874e-01,  1.4437e+00,  ...,  3.9946e-01,\n",
       "            3.9012e-01,  1.0795e+00],\n",
       "          [-1.2166e+00, -3.3100e+00, -2.5532e+00,  ...,  2.0657e-02,\n",
       "            8.4847e-01,  2.2031e+00],\n",
       "          ...,\n",
       "          [-6.3080e-01, -4.4593e+00, -2.3466e+00,  ..., -9.2350e-01,\n",
       "            1.2574e-01,  7.7568e-01],\n",
       "          [-2.4077e-01, -3.3007e+00, -3.1487e+00,  ...,  1.2501e+00,\n",
       "           -9.0130e-01,  5.6764e-01],\n",
       "          [-9.2278e-01, -3.8849e+00, -5.2166e-01,  ...,  6.5798e-01,\n",
       "           -8.2379e-01,  2.1759e+00]]]], grad_fn=<CloneBackward0>), tensor([[[[ 5.4150e-02, -9.7312e-02,  1.3995e-01,  ...,  1.0247e-02,\n",
       "           -3.7183e-02,  9.6315e-03],\n",
       "          [ 1.3551e-01,  3.0895e-02,  1.1601e-01,  ...,  2.8721e-01,\n",
       "            5.3190e-01,  3.0806e-01],\n",
       "          [-1.2369e-01, -8.2585e-01,  7.4292e-01,  ...,  5.7884e-01,\n",
       "            4.1619e-01, -1.2032e+00],\n",
       "          ...,\n",
       "          [ 3.0454e-02, -2.6942e-01,  1.0248e-01,  ...,  2.0005e-01,\n",
       "            1.5425e-01, -3.0535e-01],\n",
       "          [-1.2626e+00, -1.1365e+00, -8.7454e-02,  ...,  7.6018e-01,\n",
       "           -1.2218e-01,  9.3466e-01],\n",
       "          [-1.9721e-01, -1.0733e+00, -2.2983e-02,  ...,  2.4912e-01,\n",
       "           -4.0256e-01, -6.6323e-01]],\n",
       "\n",
       "         [[-3.4080e-02, -2.5238e-02, -3.5360e-02,  ...,  4.6609e-02,\n",
       "           -1.1951e-01, -7.9513e-02],\n",
       "          [ 6.2739e-02,  3.9228e-01,  7.4602e-01,  ..., -2.1026e-01,\n",
       "           -1.0776e-01,  1.0514e+00],\n",
       "          [ 8.2195e-02, -1.2262e+00, -4.2889e-01,  ...,  6.6713e-02,\n",
       "            3.1172e-01,  3.8724e-01],\n",
       "          ...,\n",
       "          [ 2.4851e-01, -1.4000e+00, -3.3571e-01,  ...,  2.4287e-01,\n",
       "           -1.0499e-01, -1.4419e+00],\n",
       "          [-2.7839e-01, -1.0835e+00, -5.1990e-01,  ...,  2.3946e-01,\n",
       "            1.2972e-02, -1.6848e-01],\n",
       "          [ 6.6881e-02, -6.7774e-01, -2.1352e-02,  ...,  3.7798e-01,\n",
       "           -3.4336e-01, -9.1352e-04]],\n",
       "\n",
       "         [[ 4.6788e-04,  2.1488e-01, -1.8403e-02,  ..., -8.6511e-03,\n",
       "            1.6571e-01, -4.1519e-02],\n",
       "          [ 1.4988e-01,  9.8813e-01,  4.1882e-01,  ...,  1.4615e-01,\n",
       "            7.3095e-01,  1.9034e-01],\n",
       "          [ 1.0729e+00,  2.0011e-01,  1.3636e-01,  ...,  3.9857e-02,\n",
       "           -5.7239e-02,  1.4715e+00],\n",
       "          ...,\n",
       "          [-2.0068e-01, -1.6128e-01, -2.6690e-02,  ...,  4.2546e-04,\n",
       "           -1.1641e-01, -1.4326e-01],\n",
       "          [ 3.8363e-01, -6.0753e-01,  3.9357e-01,  ..., -8.5918e-02,\n",
       "           -6.9601e-02,  4.6670e-01],\n",
       "          [ 1.3536e-01, -3.8714e-01, -6.3290e-02,  ...,  6.1991e-04,\n",
       "            1.6504e-02,  3.9036e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.1128e-02, -7.1479e-03,  9.5777e-03,  ..., -1.2802e-02,\n",
       "            6.2778e-01, -1.9070e-01],\n",
       "          [-1.9421e-01,  2.1613e-02, -3.7960e-02,  ...,  8.6816e-02,\n",
       "            4.2690e-01, -4.0828e-01],\n",
       "          [ 3.7613e-01,  2.4348e-02,  1.2266e-01,  ...,  1.6125e-01,\n",
       "            1.1142e+00,  7.4084e-01],\n",
       "          ...,\n",
       "          [ 2.8129e-01,  3.9356e-01,  2.3594e-01,  ...,  1.0502e+00,\n",
       "            3.4667e-01, -2.5751e-01],\n",
       "          [-3.0989e-01, -7.8587e-01, -5.3466e-01,  ..., -5.0624e-02,\n",
       "            2.0474e-01,  2.5909e-01],\n",
       "          [-1.4491e-01,  2.3644e-01,  1.0823e-01,  ...,  3.0967e-02,\n",
       "            6.1635e-01,  3.2542e-01]],\n",
       "\n",
       "         [[-4.2000e-02, -2.8060e-02, -1.2972e-01,  ...,  9.9624e-02,\n",
       "            1.8289e-01,  8.0309e-02],\n",
       "          [-1.4867e-02,  1.2930e-01,  1.3803e-01,  ...,  5.9829e-02,\n",
       "           -1.6195e-01, -8.6079e-02],\n",
       "          [ 6.6670e-01,  7.7423e-01,  7.3011e-01,  ...,  6.1310e-01,\n",
       "           -1.0004e+00, -4.3125e-01],\n",
       "          ...,\n",
       "          [ 6.2061e-01,  4.9352e-01,  1.2224e+00,  ..., -1.5859e-01,\n",
       "           -9.3457e-01, -1.8489e+00],\n",
       "          [ 2.9587e-01, -2.8062e-01,  3.7808e-01,  ..., -1.0510e-01,\n",
       "           -9.5838e-01,  2.5839e-01],\n",
       "          [-5.0068e-01,  2.6578e-01,  7.5710e-01,  ..., -6.7691e-01,\n",
       "           -4.0808e-02, -6.4901e-02]],\n",
       "\n",
       "         [[ 2.7376e-02, -1.0726e-03, -3.1223e-02,  ..., -2.0672e-02,\n",
       "            1.3447e-01, -4.8904e-02],\n",
       "          [ 6.6824e-02,  1.9129e-02, -1.0892e-02,  ...,  3.5835e-02,\n",
       "            1.5367e-01, -5.4158e-02],\n",
       "          [-5.3506e-01, -9.2570e-01, -3.8377e-01,  ..., -5.1720e-02,\n",
       "           -4.9353e-01, -2.7410e-01],\n",
       "          ...,\n",
       "          [ 1.4951e+00, -7.1198e-01,  1.0359e+00,  ...,  3.1739e-01,\n",
       "            1.6116e+00, -1.4315e+00],\n",
       "          [-3.4119e-01, -5.8725e-01,  3.3618e-02,  ..., -4.5245e-01,\n",
       "           -7.9546e-01,  5.0925e-01],\n",
       "          [-3.6531e-01, -5.1618e-01,  4.3862e-03,  ...,  5.6086e-01,\n",
       "           -1.0925e-01, -3.3915e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 0.0744, -0.0925,  0.3162,  ..., -0.0814, -0.1758,  0.0596],\n",
       "          [ 0.0747, -0.0921,  0.3146,  ..., -0.0812, -0.1749,  0.0590],\n",
       "          [-0.4225, -0.9241, -0.8460,  ..., -0.0854,  0.0607,  0.1219],\n",
       "          ...,\n",
       "          [-0.2960,  0.9894, -0.4940,  ...,  0.2579, -0.2133,  0.7901],\n",
       "          [-0.1266,  0.0876, -1.0581,  ...,  0.2084, -0.1443, -0.3037],\n",
       "          [ 0.2312, -0.4917, -0.2303,  ..., -0.5293,  0.1192, -0.0823]],\n",
       "\n",
       "         [[ 0.2152, -0.4739,  0.1072,  ..., -0.4099, -0.5362, -0.3153],\n",
       "          [ 0.2139, -0.4713,  0.1057,  ..., -0.4070, -0.5353, -0.3146],\n",
       "          [ 0.1979, -0.9277, -0.1360,  ...,  0.2934,  0.1537, -0.3676],\n",
       "          ...,\n",
       "          [ 0.0933,  0.1666, -0.5593,  ...,  0.0024,  0.3210, -0.4367],\n",
       "          [ 0.8068, -0.0729,  0.2929,  ...,  0.8073,  0.2225,  0.0851],\n",
       "          [-0.6759,  0.8967, -0.4767,  ..., -0.2563,  0.0558,  0.3374]],\n",
       "\n",
       "         [[-0.1024,  0.3309,  0.3487,  ...,  0.0222,  0.1490, -0.1073],\n",
       "          [-0.1023,  0.3292,  0.3473,  ...,  0.0220,  0.1487, -0.1062],\n",
       "          [-0.1942, -1.0979,  0.0206,  ...,  0.1423,  0.2219, -0.6799],\n",
       "          ...,\n",
       "          [ 0.5950,  0.0515, -0.8291,  ...,  0.1581, -0.5467, -0.7866],\n",
       "          [ 0.2241, -0.5213,  0.0970,  ..., -0.0858,  0.3034, -0.8203],\n",
       "          [ 0.0850, -0.4523, -0.0952,  ..., -1.2173,  0.0989, -0.8409]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4796, -0.0147,  0.5506,  ..., -0.2982,  0.4614,  0.0021],\n",
       "          [ 0.4777, -0.0144,  0.5492,  ..., -0.2969,  0.4604,  0.0022],\n",
       "          [-0.5231, -0.1987,  0.3905,  ..., -0.2917,  0.0018, -0.2585],\n",
       "          ...,\n",
       "          [-0.5798, -0.5452, -0.5516,  ..., -0.2682, -0.4850,  0.3804],\n",
       "          [-0.3032, -0.5831, -0.5164,  ...,  0.1147, -0.0588,  0.2528],\n",
       "          [ 0.3435,  1.7460, -0.3714,  ..., -0.3009, -0.4855,  0.4365]],\n",
       "\n",
       "         [[-0.1339,  0.2132, -0.2780,  ..., -0.3616, -0.1237, -0.0267],\n",
       "          [-0.1333,  0.2129, -0.2770,  ..., -0.3595, -0.1229, -0.0267],\n",
       "          [ 0.9734,  0.3732,  0.9352,  ..., -1.2002, -0.0493,  0.7451],\n",
       "          ...,\n",
       "          [ 0.1382,  0.0446, -0.2426,  ..., -0.2603, -0.6059,  1.6770],\n",
       "          [ 0.7110,  0.4276,  0.7062,  ..., -1.5663, -0.2958,  0.8010],\n",
       "          [ 0.2715, -0.5648,  0.4935,  ...,  0.0831, -0.5289, -0.1583]],\n",
       "\n",
       "         [[-0.4611,  0.4814,  0.2348,  ...,  0.0036,  0.3687, -0.0512],\n",
       "          [-0.4580,  0.4788,  0.2318,  ...,  0.0047,  0.3691, -0.0515],\n",
       "          [-1.1080, -0.2764, -0.1849,  ..., -0.7894, -0.2800, -0.2531],\n",
       "          ...,\n",
       "          [ 0.5669, -0.5909, -0.5701,  ..., -0.5236,  0.1576,  0.3790],\n",
       "          [ 0.1216,  0.0762,  0.0464,  ..., -0.2247, -0.9967, -1.4327],\n",
       "          [ 0.2144, -0.3424,  0.7057,  ..., -0.5408, -0.0114, -0.6327]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[-0.2800, -0.2085,  0.1052,  ...,  0.2207, -0.1821,  0.4705],\n",
       "          [-0.2805, -0.2083,  0.1052,  ...,  0.2211, -0.1819,  0.4712],\n",
       "          [ 0.2572, -0.5360,  0.2549,  ...,  0.2251, -0.0610, -0.4546],\n",
       "          ...,\n",
       "          [ 0.2791,  0.3782,  1.0591,  ..., -0.4660,  0.7490, -0.8777],\n",
       "          [ 0.6566, -0.2568,  0.6606,  ..., -0.6166,  0.5098, -0.2181],\n",
       "          [-0.3998,  0.2186,  0.2127,  ...,  0.2068,  0.3388, -0.0974]],\n",
       "\n",
       "         [[-0.0842,  0.2343,  0.1775,  ...,  0.1408, -0.0805,  0.0530],\n",
       "          [-0.0837,  0.2339,  0.1775,  ...,  0.1405, -0.0794,  0.0532],\n",
       "          [-0.4551,  0.4029, -0.5345,  ..., -0.2357, -0.0909,  0.3460],\n",
       "          ...,\n",
       "          [ 0.3264,  0.3860,  0.4568,  ..., -0.3583,  0.5122, -0.0903],\n",
       "          [ 0.3563,  0.5893, -0.1175,  ..., -1.2121,  0.0939,  0.0753],\n",
       "          [ 0.3825,  1.1847, -0.2484,  ...,  0.2580,  0.0872, -0.3373]],\n",
       "\n",
       "         [[ 0.4884,  0.0517,  0.2019,  ..., -0.3160, -0.0326, -0.1362],\n",
       "          [ 0.4880,  0.0517,  0.2023,  ..., -0.3159, -0.0324, -0.1357],\n",
       "          [-0.7239,  0.0254,  0.7082,  ..., -0.4547,  0.5983,  1.1024],\n",
       "          ...,\n",
       "          [ 0.0735,  1.1935, -0.4278,  ..., -0.0992,  0.3774, -0.1991],\n",
       "          [ 0.0561,  0.6131, -0.2759,  ...,  0.1864,  0.7460, -0.0596],\n",
       "          [-0.3467,  0.6068,  0.4307,  ...,  0.2505,  0.1173,  0.0521]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1562,  0.0025,  0.1309,  ...,  0.0591,  0.4375, -0.1410],\n",
       "          [-0.1565,  0.0021,  0.1305,  ...,  0.0594,  0.4380, -0.1392],\n",
       "          [ 0.9588, -1.1224,  0.3395,  ..., -0.6418, -0.4149,  0.2403],\n",
       "          ...,\n",
       "          [ 0.4976, -1.3509,  0.5632,  ...,  0.3509,  0.0981, -0.7444],\n",
       "          [ 0.9158, -0.5084,  0.0571,  ...,  0.3198,  0.2868,  0.5825],\n",
       "          [ 0.6320, -0.6837,  1.2539,  ..., -0.5487,  0.0675, -1.0569]],\n",
       "\n",
       "         [[-0.1522,  0.2358,  0.1396,  ..., -0.1867,  0.2292,  0.2221],\n",
       "          [-0.1524,  0.2357,  0.1400,  ..., -0.1864,  0.2295,  0.2221],\n",
       "          [ 0.9497,  1.1066,  0.6059,  ...,  0.7389, -0.0550, -1.0423],\n",
       "          ...,\n",
       "          [ 0.1465,  0.2456,  0.6898,  ...,  0.4773,  0.2976, -1.0038],\n",
       "          [ 0.3453,  0.3710,  0.5094,  ...,  0.2993, -0.0665, -0.5379],\n",
       "          [-0.7714,  0.0637, -0.3767,  ..., -0.4841, -0.0627,  0.1279]],\n",
       "\n",
       "         [[ 0.1489, -0.1030,  0.2664,  ...,  0.0952, -0.0447,  0.2672],\n",
       "          [ 0.1493, -0.1029,  0.2664,  ...,  0.0948, -0.0451,  0.2689],\n",
       "          [-0.7965, -0.7352,  1.1963,  ...,  0.3347, -0.6903, -0.8504],\n",
       "          ...,\n",
       "          [-0.2171,  0.6114, -0.1500,  ..., -0.6894, -0.7530,  0.0698],\n",
       "          [-0.4767, -0.6433,  0.5584,  ...,  0.2654, -1.3669,  0.2819],\n",
       "          [ 0.9231, -0.5112, -0.1685,  ..., -0.2089,  0.7276, -0.5568]]]],\n",
       "       grad_fn=<CloneBackward0>)), (tensor([[[[ 0.1706, -0.0842, -0.2216,  ..., -0.5840,  0.5176, -0.9015],\n",
       "          [ 0.3011, -0.2254,  0.2122,  ...,  0.8698,  0.6817, -0.4352],\n",
       "          [-2.9340, -0.4926, -0.4704,  ...,  2.6051,  0.5972,  1.2265],\n",
       "          ...,\n",
       "          [-0.9057,  1.4585,  0.8226,  ...,  1.0243,  0.3957, -0.2410],\n",
       "          [ 0.0855,  0.1538, -0.1242,  ...,  0.7481, -0.4529, -0.4797],\n",
       "          [-0.1515,  1.2079,  0.5867,  ...,  1.8052,  1.7517, -0.3296]],\n",
       "\n",
       "         [[-0.9657,  0.3288, -0.4290,  ...,  0.7370, -0.2689, -2.5466],\n",
       "          [-1.2650,  0.4214,  1.8106,  ...,  0.3244, -1.1613,  0.0925],\n",
       "          [-1.0883,  0.2188, -1.1927,  ...,  1.3635,  0.4912,  0.7509],\n",
       "          ...,\n",
       "          [-0.5751, -0.2213,  1.0723,  ..., -0.0089, -0.8842,  1.1552],\n",
       "          [-0.9892, -0.2560,  0.8750,  ...,  0.6582, -1.1846,  1.3527],\n",
       "          [-1.0721, -0.8834,  0.0868,  ..., -0.5009,  0.5341,  1.5112]],\n",
       "\n",
       "         [[-0.7566, -1.4540,  0.0117,  ...,  3.2635,  0.1162,  1.8935],\n",
       "          [-0.2772,  0.3560, -1.7313,  ...,  0.0615,  0.8060, -1.6662],\n",
       "          [-0.3197, -0.7264, -0.2024,  ..., -0.1135,  0.1719, -1.5310],\n",
       "          ...,\n",
       "          [-0.3589, -1.8825, -1.6795,  ..., -2.2173,  0.0724, -2.7909],\n",
       "          [-0.1824, -1.7494, -1.2525,  ..., -2.3475,  0.7331, -2.3658],\n",
       "          [-0.1814, -2.6321, -1.2753,  ..., -2.4262,  0.5297, -3.2153]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5159,  2.5897,  1.3438,  ..., -0.3976, -0.0986,  0.8526],\n",
       "          [-0.0178,  1.2574,  2.2859,  ...,  0.1939,  0.8960,  0.1703],\n",
       "          [-1.3526, -1.3600, -2.7494,  ...,  1.4172, -0.6025,  0.8619],\n",
       "          ...,\n",
       "          [-1.0088, -0.1428, -2.0337,  ...,  1.0396,  0.9682, -0.8346],\n",
       "          [ 0.0735, -1.0987, -2.2081,  ...,  1.4071, -0.5271, -1.0920],\n",
       "          [-0.9115, -1.1300, -3.5495,  ...,  0.4947, -0.0700, -0.8714]],\n",
       "\n",
       "         [[ 0.4867, -0.2648,  0.5457,  ...,  0.7945, -0.7437, -0.1264],\n",
       "          [ 0.3847, -0.5333,  0.3743,  ...,  1.0302,  0.1160,  0.4304],\n",
       "          [ 2.2584,  1.3647,  2.3771,  ...,  0.9569,  0.1430,  0.9564],\n",
       "          ...,\n",
       "          [-0.2217, -0.3029,  0.0612,  ...,  1.1733, -0.2192,  1.2601],\n",
       "          [ 1.2475,  1.4389,  1.8622,  ..., -0.1179, -0.3956,  0.3233],\n",
       "          [ 1.6991, -1.2834, -0.4439,  ...,  0.3946,  1.1232,  1.8977]],\n",
       "\n",
       "         [[ 1.4677, -2.3186,  0.0991,  ...,  1.1077,  1.2694, -2.1336],\n",
       "          [ 0.5948, -0.7853,  0.1927,  ..., -2.3487,  1.1658, -0.7562],\n",
       "          [ 0.6664, -1.7848, -0.6061,  ..., -0.9633, -0.1931,  1.1356],\n",
       "          ...,\n",
       "          [ 1.0763,  2.0814, -0.0383,  ...,  1.3076, -0.7419, -0.3346],\n",
       "          [ 0.4816,  1.5991, -1.2741,  ...,  0.1327, -0.5813,  1.2535],\n",
       "          [ 2.5352,  1.9207,  0.4049,  ..., -1.1054, -1.8554, -0.0939]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[-7.5859e-03, -4.2330e-02, -1.9707e-02,  ..., -9.4928e-03,\n",
       "           -4.0006e-02,  3.1712e-02],\n",
       "          [-7.8696e-02, -3.3136e-01, -1.9815e-01,  ..., -3.0399e-01,\n",
       "           -1.5932e-01,  1.9639e-01],\n",
       "          [ 6.3792e-02,  1.0661e+00, -1.8753e-01,  ...,  6.6046e-01,\n",
       "           -1.9537e-01,  2.0869e-01],\n",
       "          ...,\n",
       "          [ 2.6515e-01,  8.2824e-01,  4.2353e-01,  ..., -1.9152e-01,\n",
       "            9.1817e-02,  4.3405e-02],\n",
       "          [-3.9265e-01, -1.1382e+00,  1.0841e+00,  ...,  1.1948e+00,\n",
       "            3.7307e-01,  7.3990e-01],\n",
       "          [-1.7529e-01,  1.1761e+00, -5.3183e-01,  ..., -2.6774e-01,\n",
       "            2.1172e-02,  3.4512e-02]],\n",
       "\n",
       "         [[ 1.5649e-02,  5.7078e-02, -7.5085e-02,  ...,  4.0557e-02,\n",
       "           -2.8402e-02, -6.6283e-02],\n",
       "          [-3.7623e-01, -6.5417e-02,  3.4925e-01,  ..., -8.7201e-02,\n",
       "            1.6119e-01,  3.3995e-03],\n",
       "          [-3.5546e-01, -4.8888e-01,  5.6503e-01,  ...,  2.5291e-02,\n",
       "           -4.0485e-01, -1.8320e-01],\n",
       "          ...,\n",
       "          [-5.2999e-02,  1.9233e-01, -9.2843e-01,  ...,  2.0732e-01,\n",
       "           -9.2338e-02, -1.9384e-02],\n",
       "          [-6.8679e-01,  1.9912e-01,  6.1141e-01,  ..., -7.8620e-01,\n",
       "            3.5233e-01, -1.2765e-01],\n",
       "          [-1.2342e+00, -3.6550e-01, -9.2970e-01,  ...,  7.2896e-01,\n",
       "           -1.6802e+00, -4.3672e-01]],\n",
       "\n",
       "         [[ 4.8702e-02,  2.3164e-02, -1.6216e-01,  ...,  8.0837e-03,\n",
       "            1.8166e-01,  3.2825e-04],\n",
       "          [ 3.4813e-02, -3.2394e-01,  2.3532e-01,  ...,  1.4794e-01,\n",
       "           -1.4688e-01, -2.0247e-01],\n",
       "          [ 6.2356e-01, -8.5463e-02, -9.0764e-01,  ...,  1.0310e+00,\n",
       "            7.9269e-01, -1.4312e-01],\n",
       "          ...,\n",
       "          [-3.6775e-02, -2.1612e-01, -2.8959e-01,  ...,  1.2397e-01,\n",
       "           -5.1846e-01,  3.0153e-01],\n",
       "          [ 4.0399e-01, -3.4628e-01, -1.0801e+00,  ...,  8.0539e-01,\n",
       "           -6.7684e-01,  8.3815e-01],\n",
       "          [-1.3873e-01, -8.0061e-02, -4.3669e-01,  ..., -2.6885e-01,\n",
       "           -5.4250e-01,  5.3846e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.4790e-02,  1.3640e-01,  9.0567e-02,  ...,  8.1223e-02,\n",
       "            1.0462e-02,  5.4930e-02],\n",
       "          [ 1.2007e-01,  4.3486e-01,  3.9903e-02,  ..., -2.5874e-01,\n",
       "           -2.7246e-02,  8.1819e-02],\n",
       "          [-6.0707e-01,  1.6293e+00, -6.6033e-01,  ..., -4.7078e-01,\n",
       "            1.9123e-01, -8.2885e-01],\n",
       "          ...,\n",
       "          [-3.7725e-01,  5.0111e-01,  8.8349e-02,  ..., -1.6364e+00,\n",
       "           -9.6614e-01, -1.6023e-02],\n",
       "          [-8.2405e-01, -6.6878e-01,  2.3742e-01,  ..., -5.9091e-01,\n",
       "           -5.5806e-02, -7.7049e-01],\n",
       "          [-1.7784e-01,  1.1053e+00, -3.5525e-01,  ..., -1.0161e+00,\n",
       "            5.4913e-01, -6.1408e-01]],\n",
       "\n",
       "         [[ 7.8665e-01, -4.8256e-02, -1.3332e-03,  ..., -3.8990e-02,\n",
       "           -1.8257e-03, -3.8803e-02],\n",
       "          [ 3.7258e+00,  2.1320e-01, -1.3865e-01,  ...,  2.2185e-01,\n",
       "            1.7275e-01,  1.5431e-02],\n",
       "          [-5.5748e-01,  9.2752e-02, -1.8088e-01,  ..., -8.0734e-01,\n",
       "            1.2425e-01,  6.9982e-02],\n",
       "          ...,\n",
       "          [ 3.2219e+00,  2.3129e-01, -1.1024e-01,  ...,  3.0539e-01,\n",
       "            1.1070e-01, -2.2508e-02],\n",
       "          [ 3.0728e-01, -3.2699e-01, -1.7444e+00,  ..., -1.6044e+00,\n",
       "           -6.6524e-01, -4.3547e-01],\n",
       "          [ 4.6190e-01, -4.5604e-02, -3.2562e-01,  ...,  2.4289e+00,\n",
       "           -8.2428e-01, -2.2648e-01]],\n",
       "\n",
       "         [[-9.7233e-02, -1.1260e-01, -1.0331e-01,  ...,  1.4436e-02,\n",
       "            8.6854e-02, -2.6416e-02],\n",
       "          [ 1.5540e-01,  6.6867e-02,  1.1027e-01,  ..., -1.4367e-02,\n",
       "            1.5229e-01, -8.6228e-02],\n",
       "          [-7.8064e-01, -3.6800e-01,  3.3877e-01,  ...,  9.9954e-02,\n",
       "           -2.2068e-01, -9.7795e-01],\n",
       "          ...,\n",
       "          [ 2.9383e-01,  4.0300e-01,  3.9313e-01,  ..., -3.6607e-01,\n",
       "           -5.4124e-01,  5.7005e-01],\n",
       "          [ 1.1003e-01, -5.1015e-01, -9.6841e-02,  ..., -3.1639e-01,\n",
       "           -4.8802e-01, -2.4346e-02],\n",
       "          [ 8.4545e-01, -2.5446e-01,  3.8196e-01,  ..., -1.3164e+00,\n",
       "           -2.3405e-02, -3.6932e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-3.7139e-01,  4.6404e-02,  2.9394e-01,  ...,  1.3946e-01,\n",
       "           -2.9712e-01,  9.1696e-02],\n",
       "          [-3.7041e-01,  4.5575e-02,  2.9359e-01,  ...,  1.3930e-01,\n",
       "           -2.9640e-01,  9.1831e-02],\n",
       "          [ 1.5429e-01,  1.1393e-01,  5.4492e-02,  ..., -1.6231e-01,\n",
       "            9.5773e-01,  1.4343e-01],\n",
       "          ...,\n",
       "          [ 8.8855e-02, -2.8216e-01, -1.6685e-02,  ..., -5.2644e-01,\n",
       "            4.7583e-01, -3.9115e-01],\n",
       "          [ 5.0184e-01, -2.7669e-01,  4.5341e-01,  ..., -1.2088e-01,\n",
       "            1.5807e-01, -7.8949e-02],\n",
       "          [ 1.8409e-01,  5.4119e-01,  2.7562e-02,  ..., -4.6237e-01,\n",
       "            7.5511e-02, -5.3355e-01]],\n",
       "\n",
       "         [[ 1.8148e-01, -5.4923e-01, -1.8164e-01,  ..., -4.8062e-01,\n",
       "            3.1820e-01, -1.8335e-01],\n",
       "          [ 1.8308e-01, -5.4913e-01, -1.8284e-01,  ..., -4.8071e-01,\n",
       "            3.1711e-01, -1.8601e-01],\n",
       "          [-8.5029e-01,  6.5580e-01,  1.2967e+00,  ..., -5.5601e-04,\n",
       "            1.5184e-01,  1.2769e-01],\n",
       "          ...,\n",
       "          [-9.0283e-02,  4.1453e-01,  9.3608e-02,  ...,  5.9865e-01,\n",
       "           -3.4813e-01,  1.4073e-01],\n",
       "          [ 6.1447e-02,  3.3279e-01,  1.5609e-01,  ..., -1.6084e-01,\n",
       "            1.1627e-02,  2.8921e-01],\n",
       "          [ 1.4392e-01,  5.7828e-01, -6.9403e-02,  ...,  7.6775e-01,\n",
       "           -5.1073e-01,  7.1293e-01]],\n",
       "\n",
       "         [[-1.3404e-01, -2.9717e-01,  2.1939e-01,  ..., -7.3485e-02,\n",
       "            3.2469e-02,  2.7247e-01],\n",
       "          [-1.3387e-01, -2.9533e-01,  2.1899e-01,  ..., -7.1908e-02,\n",
       "            3.2720e-02,  2.7266e-01],\n",
       "          [ 3.5431e-01,  3.3224e-01,  1.0608e+00,  ...,  2.0341e-01,\n",
       "            5.6471e-02,  6.6384e-03],\n",
       "          ...,\n",
       "          [ 1.2538e-01,  2.4368e-01,  5.1361e-02,  ..., -5.9078e-01,\n",
       "           -3.2695e-01,  1.8245e-02],\n",
       "          [ 2.8981e-02,  6.6668e-01, -3.4804e-01,  ..., -5.6409e-01,\n",
       "           -1.2191e-01,  6.6343e-02],\n",
       "          [-4.1948e-01, -2.0240e-01,  3.6610e-01,  ...,  2.7006e-01,\n",
       "            1.0564e+00, -2.9867e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.0305e-01, -1.6732e-01,  4.9041e-01,  ..., -1.9583e-01,\n",
       "           -7.7527e-01, -5.5972e-02],\n",
       "          [-2.0236e-01, -1.6601e-01,  4.8877e-01,  ..., -1.9425e-01,\n",
       "           -7.7345e-01, -5.5236e-02],\n",
       "          [-2.3825e-01,  7.1147e-01,  6.1988e-01,  ...,  4.9831e-02,\n",
       "            9.0644e-01,  4.8763e-01],\n",
       "          ...,\n",
       "          [ 7.3231e-01, -1.6179e-01,  1.0535e+00,  ..., -3.0005e-01,\n",
       "            3.7555e-01,  2.1022e-01],\n",
       "          [ 8.6840e-01, -8.5712e-01,  8.3785e-01,  ..., -1.6259e-01,\n",
       "            3.8122e-01,  1.5854e-01],\n",
       "          [ 2.4274e-01, -1.7032e+00,  7.8590e-01,  ..., -1.0192e+00,\n",
       "            4.9840e-01, -2.9060e-01]],\n",
       "\n",
       "         [[ 4.7136e-01,  2.6132e-01,  2.0826e-01,  ..., -9.6173e-02,\n",
       "            2.4283e-01, -5.4905e-02],\n",
       "          [ 4.6866e-01,  2.6019e-01,  2.0791e-01,  ..., -9.6411e-02,\n",
       "            2.4151e-01, -5.4182e-02],\n",
       "          [-8.4832e-01, -1.0646e-02,  4.2605e-01,  ...,  1.4905e+00,\n",
       "            3.2484e-01,  1.6843e-01],\n",
       "          ...,\n",
       "          [-5.2700e-01, -4.6107e-01,  6.2675e-01,  ..., -3.1750e-01,\n",
       "           -2.4926e-01,  7.1534e-01],\n",
       "          [-2.5071e-01, -1.6626e-02,  5.5027e-01,  ...,  6.8285e-01,\n",
       "           -3.2632e-02,  9.9587e-01],\n",
       "          [-1.9764e-01,  6.1703e-02,  4.2844e-01,  ...,  7.2174e-01,\n",
       "            5.6008e-01, -7.8521e-02]],\n",
       "\n",
       "         [[ 2.5656e-01,  1.3437e-01,  1.8281e-01,  ...,  4.3988e-01,\n",
       "            1.3389e-01,  1.1677e-01],\n",
       "          [ 2.5584e-01,  1.3314e-01,  1.8124e-01,  ...,  4.3786e-01,\n",
       "            1.3320e-01,  1.1728e-01],\n",
       "          [ 3.1678e-01,  5.7047e-01,  5.0091e-01,  ...,  7.9025e-01,\n",
       "            7.4141e-01,  1.0548e+00],\n",
       "          ...,\n",
       "          [ 9.6781e-02,  4.8267e-01,  6.2666e-01,  ..., -7.0949e-02,\n",
       "            3.1960e-01, -4.6013e-01],\n",
       "          [ 1.7283e-01, -2.1521e-02,  6.9637e-01,  ..., -6.4758e-02,\n",
       "            1.1841e+00, -1.9567e-01],\n",
       "          [-3.4529e-01, -3.8799e-01, -2.2082e-01,  ..., -3.0932e-01,\n",
       "            6.6129e-01, -8.0773e-03]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.5689e-01, -3.6679e-03, -2.2172e-01,  ..., -3.1739e-04,\n",
       "           -2.3296e-02, -3.7211e-03],\n",
       "          [ 1.5653e-01, -3.1891e-03, -2.2113e-01,  ..., -1.5805e-04,\n",
       "           -2.3754e-02, -4.1488e-03],\n",
       "          [-3.4397e-01, -4.2739e-01, -1.4782e+00,  ..., -5.7026e-01,\n",
       "           -1.0228e+00,  2.6506e-01],\n",
       "          ...,\n",
       "          [ 3.1412e-02, -1.2549e-01, -1.4089e-01,  ...,  6.0293e-01,\n",
       "            4.3548e-01, -3.4189e-01],\n",
       "          [-8.0012e-01, -8.8650e-01, -5.2577e-01,  ..., -2.4955e-01,\n",
       "            9.0010e-02,  4.1589e-01],\n",
       "          [ 5.5521e-01,  5.5278e-01, -2.4531e-01,  ...,  5.9554e-03,\n",
       "           -3.3103e-01,  5.3876e-01]],\n",
       "\n",
       "         [[-3.5075e-01,  8.5160e-02, -1.4436e-02,  ..., -2.4114e-01,\n",
       "            2.7232e-01, -3.3292e-02],\n",
       "          [-3.5054e-01,  8.4694e-02, -1.4025e-02,  ..., -2.4213e-01,\n",
       "            2.7164e-01, -3.3129e-02],\n",
       "          [-4.8710e-01, -1.4734e-01,  4.3196e-01,  ..., -3.9164e-01,\n",
       "           -9.5225e-02, -1.6069e-01],\n",
       "          ...,\n",
       "          [ 7.0050e-01, -5.3214e-01,  9.2251e-02,  ...,  4.9954e-01,\n",
       "            4.5387e-01,  7.2918e-01],\n",
       "          [ 3.1062e-01, -4.3619e-01,  1.0937e+00,  ...,  9.9797e-02,\n",
       "            8.9819e-02,  3.4369e-02],\n",
       "          [-8.6391e-02,  1.4858e-01,  9.0185e-01,  ...,  4.9078e-01,\n",
       "           -1.1233e-02,  5.6650e-01]],\n",
       "\n",
       "         [[ 5.1056e-01,  1.2724e-01,  3.0223e-02,  ..., -2.9425e-01,\n",
       "            2.3195e-01,  3.7884e-01],\n",
       "          [ 5.1046e-01,  1.2737e-01,  3.0289e-02,  ..., -2.9426e-01,\n",
       "            2.3130e-01,  3.7845e-01],\n",
       "          [-4.3855e-01, -2.6701e-01, -2.1339e-01,  ..., -1.1901e+00,\n",
       "            3.2141e-01, -6.5743e-01],\n",
       "          ...,\n",
       "          [ 5.9012e-01,  2.9517e-01,  2.3569e-01,  ..., -1.1059e-01,\n",
       "            2.0712e-01,  1.1679e-01],\n",
       "          [-3.1913e-01,  2.9128e-01, -2.9627e-01,  ..., -1.7978e-01,\n",
       "            8.4498e-02,  6.5094e-01],\n",
       "          [-1.3750e-01, -7.8767e-01,  5.9964e-01,  ...,  5.8658e-01,\n",
       "            5.7397e-01, -4.6764e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.4282e-01, -3.2130e-01, -2.7755e-01,  ..., -6.3430e-01,\n",
       "            5.3037e-01, -6.4889e-01],\n",
       "          [ 1.4307e-01, -3.2155e-01, -2.7756e-01,  ..., -6.3507e-01,\n",
       "            5.3087e-01, -6.4864e-01],\n",
       "          [ 5.7931e-01,  2.6146e-01, -1.3572e-01,  ...,  8.1312e-01,\n",
       "            5.9235e-01,  3.4114e-01],\n",
       "          ...,\n",
       "          [ 2.0211e-01,  2.2519e-01, -2.4704e-01,  ..., -1.3146e-01,\n",
       "           -2.6618e-01, -1.7938e-01],\n",
       "          [ 5.4217e-01,  4.0750e-01, -3.8950e-01,  ...,  1.1318e-01,\n",
       "            8.0643e-02,  2.5820e-01],\n",
       "          [-5.7802e-02, -5.7410e-03,  1.6487e-01,  ...,  6.8952e-01,\n",
       "            3.1754e-01, -2.7058e-01]],\n",
       "\n",
       "         [[ 4.6015e-02, -2.5036e-02,  6.5357e-04,  ..., -1.0355e-02,\n",
       "           -1.0319e-01, -3.9622e-02],\n",
       "          [ 4.6236e-02, -2.5327e-02,  5.3945e-04,  ..., -1.0086e-02,\n",
       "           -1.0273e-01, -4.0279e-02],\n",
       "          [-6.3823e-01,  1.5469e-01, -3.8053e-02,  ..., -7.0934e-01,\n",
       "           -5.8465e-01, -7.1966e-02],\n",
       "          ...,\n",
       "          [-2.4537e-01, -6.3536e-01, -2.6939e-01,  ..., -1.9615e-01,\n",
       "            1.0513e+00, -1.8058e-01],\n",
       "          [-6.4093e-01, -6.5784e-01, -3.9979e-01,  ..., -6.0438e-01,\n",
       "            1.0336e+00,  2.5740e-02],\n",
       "          [ 6.4629e-01,  3.5675e-01, -4.5874e-01,  ...,  1.5061e-01,\n",
       "           -2.5327e-01, -3.4217e-02]],\n",
       "\n",
       "         [[-8.7577e-03,  2.6799e-01,  4.0985e-02,  ..., -2.1778e-01,\n",
       "           -1.4144e-01,  2.8776e-01],\n",
       "          [-9.7253e-03,  2.6857e-01,  4.1386e-02,  ..., -2.1795e-01,\n",
       "           -1.4153e-01,  2.8783e-01],\n",
       "          [-6.4014e-01,  2.2011e-02,  1.0136e+00,  ...,  1.4626e-02,\n",
       "            8.9721e-02,  3.6218e-01],\n",
       "          ...,\n",
       "          [-9.7308e-01, -5.6719e-01,  1.9943e-01,  ..., -4.3324e-01,\n",
       "            6.9877e-01,  3.6352e-01],\n",
       "          [-2.7001e-01, -6.9083e-01,  4.9989e-01,  ..., -8.7450e-01,\n",
       "           -1.8142e-01,  1.4553e-01],\n",
       "          [ 4.1309e-01, -2.3064e-01,  4.4219e-01,  ...,  4.1089e-02,\n",
       "           -5.7518e-01,  4.0663e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-0.0419,  0.0723, -0.1479,  ...,  0.7560, -0.3927,  0.7077],\n",
       "          [-0.4556, -0.1667,  0.2910,  ...,  0.8750, -0.7243, -0.8527],\n",
       "          [ 0.7290, -0.6998, -0.3879,  ...,  1.1523, -0.1453, -1.7164],\n",
       "          ...,\n",
       "          [ 0.1614, -0.9323,  0.7126,  ...,  0.6774, -1.9410, -1.1618],\n",
       "          [-0.1389,  0.2788,  0.7462,  ...,  1.4694, -1.6253, -0.5464],\n",
       "          [ 1.0371, -0.2511, -0.4636,  ...,  2.2121, -1.6895, -2.2957]],\n",
       "\n",
       "         [[-0.2819, -0.4791,  0.0798,  ...,  0.2014, -0.2059,  0.5118],\n",
       "          [ 0.9905,  0.4295, -0.9668,  ..., -0.4548,  0.7771,  0.5349],\n",
       "          [ 1.3083, -1.5446, -2.3477,  ..., -1.1248, -0.2791,  0.7961],\n",
       "          ...,\n",
       "          [ 0.3330, -0.2241, -0.2136,  ...,  1.1829,  1.3355,  0.2375],\n",
       "          [ 0.5296, -0.2123,  0.5571,  ..., -0.0296, -0.0957,  0.1296],\n",
       "          [ 0.7614,  0.4757, -0.0987,  ..., -0.6556,  0.5177,  0.1090]],\n",
       "\n",
       "         [[ 0.1304,  0.1528, -0.9031,  ..., -0.0519, -0.0070, -0.0964],\n",
       "          [-0.8003, -0.6985,  0.5288,  ...,  0.8994,  0.4759,  0.6720],\n",
       "          [ 0.3534, -0.4402, -0.7786,  ...,  0.1746,  1.0476, -0.7525],\n",
       "          ...,\n",
       "          [-0.3633, -0.9005,  0.6482,  ...,  0.6425,  1.4668,  0.9381],\n",
       "          [ 0.8593, -0.5964, -0.5065,  ...,  0.1342,  1.0825,  0.6824],\n",
       "          [ 0.6386, -0.0054,  0.4999,  ..., -0.1271,  1.7634,  0.2676]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.6817,  0.3881, -0.2341,  ..., -0.5828,  0.0865,  0.2691],\n",
       "          [ 0.6121, -0.2246, -0.3442,  ...,  0.0078,  0.4531,  0.4434],\n",
       "          [ 0.1208, -1.4163, -0.2750,  ...,  0.8405,  0.8620, -0.2275],\n",
       "          ...,\n",
       "          [ 0.0786, -0.3642, -1.2012,  ...,  0.1449,  0.1012,  0.6186],\n",
       "          [ 0.1098, -1.1163, -1.4540,  ..., -0.1890, -0.7061,  0.5494],\n",
       "          [-0.8059, -0.3387, -1.4183,  ...,  0.1244, -0.8799,  0.2225]],\n",
       "\n",
       "         [[ 0.2134, -0.7062,  0.1500,  ..., -0.0821,  0.0322,  0.0574],\n",
       "          [ 0.0791,  0.2082, -0.2984,  ..., -0.0123, -0.4237, -0.0949],\n",
       "          [-0.3898, -0.1760, -0.4317,  ...,  0.4229,  0.4786,  0.4520],\n",
       "          ...,\n",
       "          [ 0.2977,  0.6158,  0.2346,  ...,  0.6881,  1.6148,  1.4733],\n",
       "          [ 0.9291,  1.1843,  1.2364,  ..., -0.3678,  2.7634,  1.2957],\n",
       "          [ 0.1293,  1.4838,  0.0807,  ..., -0.8410,  0.6502,  1.0386]],\n",
       "\n",
       "         [[ 1.2369,  0.4922,  0.0813,  ...,  0.3153, -0.3160,  0.2226],\n",
       "          [-0.3239, -0.4603, -0.1961,  ...,  0.0880, -0.3241, -0.2005],\n",
       "          [-0.1046,  0.1606,  1.0442,  ..., -0.8012, -1.0493, -0.9818],\n",
       "          ...,\n",
       "          [-0.1333, -1.8179,  0.0542,  ...,  0.8821,  0.1963, -0.4930],\n",
       "          [ 0.0511, -2.4437,  2.0612,  ..., -2.7023,  0.1009,  0.0886],\n",
       "          [-0.4742, -0.8134,  1.6349,  ..., -0.0640,  2.5733, -0.6735]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[-1.8925e-03,  5.5406e-02,  1.7797e-02,  ..., -5.5040e-04,\n",
       "           -6.7511e-02,  9.2173e-03],\n",
       "          [ 6.9678e-01,  5.9462e-01,  3.8170e-02,  ..., -5.2578e-01,\n",
       "           -3.7947e-01,  1.3278e-01],\n",
       "          [-8.3787e-01, -7.8526e-01,  2.4318e-01,  ..., -4.1080e-01,\n",
       "            1.2752e+00, -5.1700e-01],\n",
       "          ...,\n",
       "          [ 7.4954e-01, -9.8845e-01,  2.7306e-01,  ...,  1.0396e-01,\n",
       "           -1.1399e-01,  1.1500e+00],\n",
       "          [ 3.6952e-01, -1.7554e+00,  5.9494e-01,  ...,  1.8256e-01,\n",
       "            9.8707e-01, -6.1566e-01],\n",
       "          [-1.3770e-01,  4.2177e-01, -3.5290e-01,  ..., -7.0010e-01,\n",
       "           -1.0138e-02,  2.4975e-02]],\n",
       "\n",
       "         [[-1.2051e-02, -2.3298e-02,  3.9517e-02,  ..., -2.0366e-02,\n",
       "            2.1768e-02, -7.9758e-03],\n",
       "          [-1.9321e-01, -4.3426e-01,  5.2227e-01,  ...,  3.9942e-02,\n",
       "           -1.6638e-01, -6.9762e-02],\n",
       "          [-3.9104e-01,  1.0469e-01, -1.1743e+00,  ..., -9.6768e-01,\n",
       "           -8.8242e-01,  8.2737e-01],\n",
       "          ...,\n",
       "          [-3.2806e-01, -8.0447e-01, -8.7815e-01,  ..., -2.9502e-01,\n",
       "            1.5105e+00, -1.2316e-02],\n",
       "          [-2.0095e-01, -7.5596e-01, -5.8282e-01,  ..., -9.4740e-01,\n",
       "            9.0751e-02, -6.5745e-02],\n",
       "          [ 3.6251e-01, -9.5085e-02, -1.5481e-01,  ..., -1.2033e+00,\n",
       "            4.5072e-01,  9.4538e-01]],\n",
       "\n",
       "         [[ 8.8520e-03, -1.0211e-02,  1.1596e-02,  ..., -3.3988e-03,\n",
       "            2.2021e-02, -3.7007e-03],\n",
       "          [-2.3825e-01, -1.1815e+00,  1.6558e-02,  ...,  5.9660e-02,\n",
       "           -2.1123e-01, -7.0672e-02],\n",
       "          [-8.6504e-02, -2.3332e+00,  9.6968e-01,  ...,  2.8407e-01,\n",
       "            5.1412e-01,  2.3354e+00],\n",
       "          ...,\n",
       "          [ 1.0083e+00, -9.9416e-01,  1.0866e+00,  ...,  1.9909e+00,\n",
       "           -4.1045e-01, -1.5341e-01],\n",
       "          [ 1.6085e+00, -8.9179e-01, -2.1809e-02,  ...,  2.0117e+00,\n",
       "           -6.5165e-01, -4.9071e-01],\n",
       "          [ 1.8444e+00, -7.1049e-01,  4.2168e-01,  ...,  4.2223e-02,\n",
       "           -6.1700e-01,  6.5129e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.9574e-03, -2.7161e-02, -6.8543e-03,  ...,  1.5876e-03,\n",
       "            4.2589e-02, -1.6309e-03],\n",
       "          [-2.8879e-01, -6.4703e-01, -3.7909e-01,  ...,  3.5061e-01,\n",
       "            1.0078e+00,  1.4995e-01],\n",
       "          [ 5.9372e-01, -6.2720e-01,  3.0460e-01,  ...,  5.2848e-03,\n",
       "           -1.0176e+00, -4.9169e-01],\n",
       "          ...,\n",
       "          [ 5.3873e-01, -4.3782e-01, -3.0273e-01,  ..., -8.5320e-01,\n",
       "            2.8666e-02, -3.7513e-01],\n",
       "          [ 9.1099e-01, -7.4715e-02,  7.5839e-02,  ..., -1.0684e+00,\n",
       "           -7.0579e-02, -8.5177e-01],\n",
       "          [ 2.0802e+00, -6.4602e-01, -1.1064e+00,  ..., -1.8454e+00,\n",
       "            9.3510e-01, -1.0599e+00]],\n",
       "\n",
       "         [[ 1.8709e-02, -6.7593e-03, -3.7202e-02,  ...,  2.4307e-02,\n",
       "           -1.7281e-02, -3.3706e-02],\n",
       "          [ 4.5184e-01, -1.7930e-01, -1.2673e-01,  ...,  5.3344e-01,\n",
       "           -2.5413e-01, -5.5600e-01],\n",
       "          [ 2.1481e-01, -4.8378e-01,  6.6710e-01,  ...,  5.5683e-01,\n",
       "            1.9795e+00,  1.0377e+00],\n",
       "          ...,\n",
       "          [-9.6798e-02,  4.2318e-01,  6.2573e-02,  ...,  3.1414e-01,\n",
       "           -9.2762e-01,  6.5710e-01],\n",
       "          [ 3.7303e-01,  8.1863e-01, -5.2498e-01,  ..., -3.9461e-01,\n",
       "            1.1363e+00,  4.3147e-01],\n",
       "          [-9.0014e-01, -2.4140e-01, -2.6016e-01,  ...,  7.5299e-03,\n",
       "            8.0461e-01,  1.2275e+00]],\n",
       "\n",
       "         [[-6.3038e-02,  2.9304e-02,  1.2237e-02,  ..., -1.4231e-02,\n",
       "           -5.9910e-02,  5.6646e-02],\n",
       "          [-1.2203e+00,  5.4361e-01, -1.3589e-01,  ..., -1.4301e-01,\n",
       "           -5.5756e-01,  4.7802e-01],\n",
       "          [-2.3988e-01,  3.5169e-01, -4.5552e-01,  ...,  4.4832e-01,\n",
       "            1.6021e-01,  9.6785e-02],\n",
       "          ...,\n",
       "          [-6.6995e-01, -8.9559e-02,  1.3087e-02,  ...,  8.1202e-01,\n",
       "            3.3768e-01,  3.3237e-01],\n",
       "          [-8.5660e-01, -5.6727e-01,  2.4055e-01,  ...,  7.2669e-01,\n",
       "            1.2177e-01, -9.5728e-02],\n",
       "          [ 1.9441e-01,  2.0550e-01, -1.6810e-01,  ...,  1.6087e+00,\n",
       "            3.0125e-01, -1.8767e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.2750e-01,  1.1505e-01,  9.1206e-01,  ..., -4.9398e-01,\n",
       "            2.4915e-02, -1.4498e-01],\n",
       "          [ 1.2589e-01,  1.1534e-01,  9.0972e-01,  ..., -4.9350e-01,\n",
       "            2.6190e-02, -1.4553e-01],\n",
       "          [-2.1754e-01, -9.3552e-02, -3.0885e-01,  ..., -9.6412e-02,\n",
       "            7.5364e-01,  5.4495e-01],\n",
       "          ...,\n",
       "          [ 2.1822e-03,  3.9376e-01,  2.1349e-01,  ...,  5.1877e-01,\n",
       "           -7.4174e-02, -1.0420e+00],\n",
       "          [-1.4395e-02,  8.9944e-01, -3.8543e-01,  ...,  8.8165e-01,\n",
       "           -7.3138e-02, -1.0497e+00],\n",
       "          [-3.0290e-03,  3.9399e-01, -7.5584e-01,  ..., -1.6537e-02,\n",
       "           -1.6489e-01, -1.3794e-01]],\n",
       "\n",
       "         [[ 1.4680e+00,  5.2587e-01,  4.1743e-01,  ...,  5.8513e-01,\n",
       "           -3.4421e-01, -2.5141e-01],\n",
       "          [ 1.4641e+00,  5.2553e-01,  4.1679e-01,  ...,  5.8372e-01,\n",
       "           -3.4482e-01, -2.5178e-01],\n",
       "          [ 2.4716e-03,  8.8336e-02, -1.0642e+00,  ..., -1.9563e-01,\n",
       "           -2.3986e+00,  4.4315e-01],\n",
       "          ...,\n",
       "          [-1.2706e+00,  3.4172e-01, -1.5483e+00,  ...,  1.6296e+00,\n",
       "           -1.9368e+00, -1.7425e-01],\n",
       "          [-8.3143e-01,  5.3123e-01, -9.9898e-01,  ...,  1.1607e+00,\n",
       "           -7.1371e-01,  1.6525e-01],\n",
       "          [ 3.1428e-01, -4.9676e-01,  4.4203e-01,  ...,  3.6170e-01,\n",
       "           -9.0811e-01,  4.9896e-02]],\n",
       "\n",
       "         [[-3.8731e-01, -4.1288e-02,  8.4428e-02,  ..., -1.6934e-02,\n",
       "            9.0756e-02,  4.1205e-01],\n",
       "          [-3.8466e-01, -4.1979e-02,  8.3628e-02,  ..., -1.6836e-02,\n",
       "            9.0368e-02,  4.1019e-01],\n",
       "          [-1.0823e+00,  4.5208e-01,  4.7380e-01,  ..., -2.6316e-01,\n",
       "           -1.0936e+00, -1.2290e-01],\n",
       "          ...,\n",
       "          [-1.6033e+00, -1.7352e-01, -1.4263e-01,  ...,  1.5819e+00,\n",
       "           -1.4112e+00,  1.9839e-01],\n",
       "          [-1.9766e+00, -6.8105e-01,  6.9674e-01,  ...,  1.0411e+00,\n",
       "           -1.5450e+00, -3.8807e-01],\n",
       "          [-6.1630e-01,  2.0186e-01,  4.4647e-02,  ...,  7.2009e-02,\n",
       "            1.5841e-01, -7.0940e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3031e-01,  1.9791e-01,  6.9965e-02,  ..., -1.4897e-01,\n",
       "           -5.4489e-01,  5.6126e-01],\n",
       "          [ 1.2875e-01,  1.9792e-01,  7.0645e-02,  ..., -1.4843e-01,\n",
       "           -5.4320e-01,  5.5875e-01],\n",
       "          [ 6.2469e-01, -1.0950e-01, -3.8701e-01,  ..., -8.0296e-01,\n",
       "           -3.1924e-01, -2.7709e-01],\n",
       "          ...,\n",
       "          [ 6.0108e-01, -3.9221e-01,  3.9585e-02,  ..., -4.7693e-01,\n",
       "            5.5479e-01, -1.8052e-01],\n",
       "          [ 9.0676e-01, -3.5705e-01, -2.2978e-01,  ..., -8.5154e-01,\n",
       "            4.2550e-01,  2.3599e-01],\n",
       "          [-3.8666e-01,  5.2006e-01,  9.8736e-02,  ...,  1.0440e+00,\n",
       "           -7.7724e-01,  2.9531e-01]],\n",
       "\n",
       "         [[-2.0346e-01,  3.1631e-01, -1.6357e-01,  ...,  4.6731e-01,\n",
       "           -4.1300e-01, -3.3760e-01],\n",
       "          [-2.0191e-01,  3.1641e-01, -1.6311e-01,  ...,  4.6540e-01,\n",
       "           -4.1197e-01, -3.3638e-01],\n",
       "          [-8.7108e-01,  1.1183e+00, -8.4402e-01,  ...,  1.4643e+00,\n",
       "           -1.8046e-01,  1.2780e+00],\n",
       "          ...,\n",
       "          [-6.1555e-01,  3.2572e-01, -4.7530e-01,  ..., -2.0222e-01,\n",
       "            4.7991e-01, -2.3935e-02],\n",
       "          [-2.3424e-01,  2.2194e-01, -4.9310e-01,  ...,  5.4732e-01,\n",
       "            5.3495e-01,  7.4591e-01],\n",
       "          [ 1.7923e-01,  3.1977e-01,  5.0447e-01,  ..., -1.8676e-01,\n",
       "            8.0779e-01,  4.8459e-01]],\n",
       "\n",
       "         [[-6.8281e-01, -7.6418e-01, -4.8611e-01,  ..., -2.2340e-01,\n",
       "           -9.8590e-02,  2.1292e-01],\n",
       "          [-6.7930e-01, -7.6105e-01, -4.8401e-01,  ..., -2.2300e-01,\n",
       "           -9.8501e-02,  2.1097e-01],\n",
       "          [-1.1906e-02,  2.3881e-01, -4.5494e-01,  ...,  2.1880e-02,\n",
       "            5.6719e-01, -7.3090e-01],\n",
       "          ...,\n",
       "          [ 9.4375e-02,  4.8027e-02,  9.3489e-01,  ...,  1.0856e+00,\n",
       "           -1.1947e+00,  6.6646e-01],\n",
       "          [ 1.9694e-02,  1.9630e-02,  2.6232e-01,  ..., -2.2864e-01,\n",
       "            2.4648e-01,  1.4754e-01],\n",
       "          [-3.6234e-01, -1.8760e-02,  2.7810e-01,  ...,  4.9336e-01,\n",
       "           -6.2209e-01,  1.6051e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-2.6151e-02, -6.1075e-03, -3.2304e-02,  ...,  2.0953e-01,\n",
       "           -1.0291e-01,  2.1815e-01],\n",
       "          [-2.5754e-02, -6.4830e-03, -3.2345e-02,  ...,  2.0924e-01,\n",
       "           -1.0373e-01,  2.1865e-01],\n",
       "          [-6.6776e-01, -3.8316e-01, -1.0264e-01,  ..., -6.6052e-01,\n",
       "           -3.4797e-01, -6.3511e-01],\n",
       "          ...,\n",
       "          [-4.6087e-01, -5.5531e-01,  5.3566e-01,  ..., -1.6817e-01,\n",
       "            9.0396e-01, -2.8374e-01],\n",
       "          [-3.7743e-01, -1.1035e+00,  2.7496e-01,  ..., -9.0014e-02,\n",
       "            6.0907e-01, -4.2778e-01],\n",
       "          [-7.3473e-01, -1.7038e-01,  5.4323e-01,  ..., -2.4891e-01,\n",
       "            1.2199e-01, -3.2627e-01]],\n",
       "\n",
       "         [[-2.2938e-02,  5.4020e-02,  1.1000e-01,  ...,  6.8529e-02,\n",
       "            9.3745e-02, -8.2382e-02],\n",
       "          [-2.3407e-02,  5.3648e-02,  1.1001e-01,  ...,  6.7718e-02,\n",
       "            9.4011e-02, -8.2200e-02],\n",
       "          [-4.6563e-01, -1.7799e-01,  8.4225e-01,  ..., -6.9515e-01,\n",
       "           -2.8690e-01,  4.6245e-01],\n",
       "          ...,\n",
       "          [-2.7681e-01,  4.6705e-01,  1.5033e-01,  ..., -5.4398e-01,\n",
       "           -5.1326e-01,  2.3914e-01],\n",
       "          [-2.9042e-02,  4.6905e-01,  8.2368e-01,  ..., -5.4472e-01,\n",
       "           -1.9356e-01,  1.5372e-01],\n",
       "          [-5.5846e-01, -1.2888e-01,  1.1469e-02,  ..., -8.6506e-02,\n",
       "            3.1334e-01,  1.9401e-01]],\n",
       "\n",
       "         [[-2.5222e-01, -6.1707e-01, -2.5143e-01,  ..., -2.6475e-01,\n",
       "           -2.8019e-01,  5.0892e-01],\n",
       "          [-2.5224e-01, -6.1789e-01, -2.5106e-01,  ..., -2.6502e-01,\n",
       "           -2.7945e-01,  5.0881e-01],\n",
       "          [ 2.3612e-01, -8.6413e-02, -7.1883e-01,  ...,  9.8266e-01,\n",
       "            3.8640e-02, -4.9611e-01],\n",
       "          ...,\n",
       "          [-2.1375e-01,  7.7852e-02,  9.3240e-01,  ..., -4.4035e-01,\n",
       "            1.6333e-02, -7.2681e-01],\n",
       "          [ 1.3332e-01, -3.8342e-01,  2.5842e-01,  ...,  9.9454e-01,\n",
       "           -3.5442e-01, -7.1833e-01],\n",
       "          [-2.1042e-02,  3.4525e-01,  6.8069e-01,  ...,  2.7666e-01,\n",
       "           -8.1674e-02,  5.1953e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.2175e-03, -1.4386e-02, -3.0719e-02,  ...,  2.1309e-02,\n",
       "           -8.5299e-03,  1.9035e-01],\n",
       "          [-2.0959e-03, -1.4094e-02, -3.0247e-02,  ...,  2.1052e-02,\n",
       "           -8.2638e-03,  1.9054e-01],\n",
       "          [-1.9488e-01, -3.4152e-01,  8.8671e-01,  ...,  9.8142e-01,\n",
       "            3.1511e-01, -1.3712e-01],\n",
       "          ...,\n",
       "          [-1.1454e-01, -4.2195e-01, -3.6985e-01,  ...,  2.0399e-01,\n",
       "            4.0809e-01, -8.0437e-01],\n",
       "          [-2.1536e-01, -3.1918e-01, -1.6868e-01,  ...,  7.1430e-01,\n",
       "            6.0458e-01, -1.0687e+00],\n",
       "          [-3.2782e-01, -4.4325e-02,  3.6632e-01,  ...,  1.7727e-01,\n",
       "            3.6788e-01, -7.3066e-01]],\n",
       "\n",
       "         [[ 4.1865e-02, -5.7144e-02, -7.9030e-02,  ..., -5.3242e-03,\n",
       "            5.6393e-02, -1.4542e-04],\n",
       "          [ 4.1576e-02, -5.7201e-02, -7.8643e-02,  ..., -4.8969e-03,\n",
       "            5.6393e-02, -1.3060e-04],\n",
       "          [-2.0024e-01,  2.9299e-01,  3.3081e-01,  ...,  1.3290e+00,\n",
       "           -3.7525e-03, -1.3830e+00],\n",
       "          ...,\n",
       "          [-3.6198e-01, -1.5592e-01,  3.8774e-01,  ...,  1.1393e-01,\n",
       "           -3.3745e-02, -1.0641e+00],\n",
       "          [ 7.1761e-02,  1.0477e+00,  1.0944e+00,  ...,  2.9074e-02,\n",
       "           -1.3880e-01, -8.9309e-01],\n",
       "          [-2.6071e-01,  1.0678e+00, -3.0878e-01,  ...,  2.0431e-01,\n",
       "            6.0759e-01, -6.2739e-01]],\n",
       "\n",
       "         [[ 3.1137e-02,  8.8767e-02,  2.3135e-01,  ..., -1.2343e-01,\n",
       "           -2.3984e-02,  3.4001e-02],\n",
       "          [ 3.1809e-02,  8.8630e-02,  2.3104e-01,  ..., -1.2323e-01,\n",
       "           -2.4550e-02,  3.4283e-02],\n",
       "          [-7.0449e-01,  4.7741e-01, -4.2150e-01,  ...,  4.9273e-03,\n",
       "           -9.4914e-01,  7.7459e-01],\n",
       "          ...,\n",
       "          [-6.1941e-01,  7.3364e-01,  5.5397e-01,  ...,  3.9199e-01,\n",
       "           -7.0355e-01,  3.4159e-01],\n",
       "          [-5.6725e-01,  6.0575e-01, -3.7788e-01,  ...,  2.9385e-01,\n",
       "           -7.5826e-01, -2.3068e-01],\n",
       "          [ 7.9794e-01, -8.3525e-02,  2.0845e-01,  ..., -1.3109e+00,\n",
       "           -1.3476e-02,  4.1752e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-1.3850, -0.9411,  0.0364,  ...,  0.8104,  0.1382,  0.7219],\n",
       "          [-0.6462, -1.8570,  0.6218,  ...,  1.6329, -0.5865, -2.6608],\n",
       "          [-1.7077, -1.3057, -0.9073,  ...,  1.8687, -0.8730,  0.3020],\n",
       "          ...,\n",
       "          [-1.6451,  0.3988, -2.1824,  ..., -1.6662,  1.4488, -3.0612],\n",
       "          [ 0.1941, -1.6120, -0.9159,  ...,  2.2464,  0.9461, -1.0527],\n",
       "          [-2.5047, -2.0635,  0.0588,  ...,  0.3702,  0.2728, -1.8481]],\n",
       "\n",
       "         [[ 0.6219,  0.3866, -0.0246,  ..., -0.1683, -0.1619, -1.1680],\n",
       "          [-0.3518,  0.1051, -0.3174,  ...,  0.8983,  0.1771, -0.1042],\n",
       "          [-1.0711,  0.2774, -0.4224,  ...,  1.6209, -0.0538,  1.7543],\n",
       "          ...,\n",
       "          [-0.3234,  0.2751,  0.5540,  ...,  0.2308, -1.2435,  1.0164],\n",
       "          [ 0.5720, -0.0154, -0.3177,  ...,  0.8811, -0.3290,  2.3710],\n",
       "          [ 1.0254,  0.0554, -0.3655,  ...,  0.7276, -0.0911,  1.0040]],\n",
       "\n",
       "         [[-0.7339, -0.9575,  0.5632,  ...,  0.2614,  0.6772,  0.3249],\n",
       "          [-0.6038,  0.3782,  2.9254,  ...,  0.8431,  0.6369,  1.0892],\n",
       "          [ 3.2363, -0.9165, -0.7158,  ..., -0.0435, -0.3299,  3.9816],\n",
       "          ...,\n",
       "          [ 2.5130, -1.5881,  2.0757,  ...,  2.2253,  2.7615, -1.0512],\n",
       "          [ 1.5895,  1.7147,  1.4773,  ...,  1.8092, -0.4503,  2.6367],\n",
       "          [-0.8169,  0.6600, -0.1163,  ...,  0.2792, -0.6150,  1.4299]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3003, -0.0546, -0.4223,  ...,  0.0727,  0.0932, -0.0984],\n",
       "          [-0.0777, -0.3152, -0.3523,  ..., -0.7045, -0.3050,  0.0747],\n",
       "          [ 0.3121, -1.2733,  0.1654,  ...,  0.2491, -1.4202, -0.0983],\n",
       "          ...,\n",
       "          [-0.9923,  0.1842,  0.1046,  ...,  0.3278,  0.7861, -0.3663],\n",
       "          [-0.4190, -0.6146, -0.1781,  ..., -0.1839,  0.2976,  0.3690],\n",
       "          [-1.3981, -0.2421, -0.0937,  ..., -0.1703, -0.3492, -0.4888]],\n",
       "\n",
       "         [[ 0.7916, -0.6601,  0.3260,  ..., -0.4669, -0.3729,  0.2804],\n",
       "          [ 0.8442,  0.0605,  0.7858,  ...,  0.7744,  0.1076,  0.0121],\n",
       "          [ 0.2442,  0.5048,  1.5327,  ...,  0.1378,  0.1689, -1.5003],\n",
       "          ...,\n",
       "          [ 0.8592,  0.0965,  1.4327,  ..., -0.2318, -1.1306,  0.7206],\n",
       "          [ 0.4499, -0.3046,  2.4978,  ...,  0.1875,  0.9221, -1.7678],\n",
       "          [ 1.5256, -0.0260,  0.2334,  ...,  0.9229,  0.6175, -1.5145]],\n",
       "\n",
       "         [[ 0.8239,  0.1289, -0.5861,  ..., -0.3385, -0.8988, -0.4040],\n",
       "          [-0.8035, -1.5202,  0.1778,  ..., -0.5417, -0.0732,  0.8142],\n",
       "          [-1.2318, -1.4134,  0.9836,  ..., -0.4445,  0.8456,  1.2565],\n",
       "          ...,\n",
       "          [-1.1184, -0.6389, -0.2131,  ...,  1.2157,  0.5248,  0.6160],\n",
       "          [-0.4256, -1.5106,  1.3990,  ...,  0.3268,  0.4512,  1.3228],\n",
       "          [-0.4024, -1.4307,  1.7694,  ..., -0.3829,  0.2706,  1.5999]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 3.0822e-03,  5.5436e-03, -2.0220e-02,  ..., -1.1882e-02,\n",
       "           -1.4995e-02, -2.2706e-02],\n",
       "          [ 3.1705e-01,  8.8547e-02, -3.1526e-03,  ..., -1.8032e-01,\n",
       "           -3.3085e-01, -1.1805e-01],\n",
       "          [ 3.2606e-01, -2.6184e-01,  7.3234e-02,  ...,  2.9810e-01,\n",
       "            1.0236e-01,  8.6336e-01],\n",
       "          ...,\n",
       "          [ 6.2486e-01,  1.0013e+00, -1.0151e+00,  ..., -6.3847e-01,\n",
       "           -6.6653e-01, -6.9030e-02],\n",
       "          [-4.1087e-02,  3.5511e-01, -6.0780e-01,  ...,  4.8987e-01,\n",
       "           -6.8939e-01, -2.9142e-01],\n",
       "          [-1.3577e-01,  8.0551e-01, -5.0122e-01,  ..., -9.3186e-01,\n",
       "           -4.0724e-01, -3.4183e-01]],\n",
       "\n",
       "         [[ 1.1250e-02,  4.5996e-03, -1.6136e-02,  ..., -1.0089e-03,\n",
       "           -2.1896e-03,  2.0469e-03],\n",
       "          [ 2.4703e-01, -1.8225e-01, -7.4142e-02,  ...,  2.9969e-01,\n",
       "           -1.8011e-01,  1.2769e-01],\n",
       "          [ 2.3616e-01,  5.1636e-01,  1.5513e+00,  ..., -4.3834e-02,\n",
       "            1.7465e-01, -1.0545e+00],\n",
       "          ...,\n",
       "          [ 3.9832e-01, -2.5561e-01, -2.1422e-01,  ..., -2.4541e-01,\n",
       "           -3.3600e-01,  5.9877e-01],\n",
       "          [ 1.2620e+00, -5.6099e-01,  7.5292e-01,  ..., -3.9911e-01,\n",
       "           -2.9506e-01, -1.1959e+00],\n",
       "          [ 1.3642e+00,  7.5259e-01,  6.9075e-01,  ...,  2.4778e-01,\n",
       "           -2.8389e-01, -6.4626e-01]],\n",
       "\n",
       "         [[-4.0863e-02,  4.4717e-03,  5.0744e-03,  ...,  1.3471e-02,\n",
       "           -1.0159e-03,  2.5795e-02],\n",
       "          [ 1.3942e-02, -2.6560e-02,  9.5317e-02,  ..., -1.6168e-01,\n",
       "            2.2381e-01, -8.6296e-02],\n",
       "          [-1.0759e-01, -5.4381e-01, -6.2927e-01,  ..., -5.9248e-01,\n",
       "            1.8798e-01,  2.3335e-01],\n",
       "          ...,\n",
       "          [ 2.5152e-01,  1.2657e-01,  5.9974e-01,  ...,  2.8767e-01,\n",
       "            1.8956e-01,  1.5093e-01],\n",
       "          [-1.7778e-01, -1.0276e-01,  3.2651e-02,  ..., -1.7949e-01,\n",
       "            6.4317e-01,  4.0281e-01],\n",
       "          [ 5.7318e-03,  6.9447e-01, -1.6417e-01,  ...,  1.8869e-01,\n",
       "           -5.0303e-01, -5.6928e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3607e-02,  3.0256e-02, -3.2340e-03,  ..., -1.9893e-02,\n",
       "            3.6989e-04, -7.3890e-03],\n",
       "          [ 3.8874e-01,  8.6979e-01, -8.5619e-02,  ..., -2.1758e-01,\n",
       "           -1.5031e-02, -1.2933e-01],\n",
       "          [-1.5157e+00,  4.3396e-01, -6.2070e-01,  ..., -1.0330e-01,\n",
       "            1.0298e+00,  1.3174e+00],\n",
       "          ...,\n",
       "          [-1.9717e+00, -6.0974e-02, -1.4836e+00,  ...,  2.7185e-01,\n",
       "           -7.8646e-01, -5.1177e-01],\n",
       "          [-1.5759e+00,  7.7147e-01, -1.8580e+00,  ...,  5.7048e-01,\n",
       "           -3.7481e-01, -3.7445e-01],\n",
       "          [-1.7586e+00,  1.2988e+00, -8.8755e-01,  ...,  7.2901e-01,\n",
       "           -7.7219e-01, -6.0583e-01]],\n",
       "\n",
       "         [[-5.1543e-02, -1.8294e-02,  1.3500e-03,  ..., -6.7983e-03,\n",
       "            7.6155e-03,  1.0473e-03],\n",
       "          [-7.4917e-01, -4.2751e-01,  4.6281e-03,  ..., -9.0863e-02,\n",
       "            2.1731e-01,  2.6704e-01],\n",
       "          [-3.2525e-01,  5.5525e-01,  1.6938e-01,  ...,  6.3160e-01,\n",
       "           -1.7783e-01, -1.0075e+00],\n",
       "          ...,\n",
       "          [-1.0321e-01, -7.6339e-01,  5.7542e-01,  ...,  2.7570e-01,\n",
       "            7.3884e-01,  3.0341e-01],\n",
       "          [ 1.2996e+00,  3.1304e-01, -8.5937e-02,  ..., -2.1208e-01,\n",
       "            6.4246e-01,  6.3986e-01],\n",
       "          [ 1.2019e+00,  6.0468e-01, -8.7011e-01,  ...,  1.0414e+00,\n",
       "           -3.2206e-02,  1.0622e+00]],\n",
       "\n",
       "         [[ 3.5914e-02,  7.8674e-02,  1.1195e-01,  ...,  1.7087e-01,\n",
       "            6.9050e-03, -2.0422e-01],\n",
       "          [ 8.0391e-02, -3.8284e-02,  2.0578e-01,  ..., -2.9639e-01,\n",
       "            6.0835e-02, -1.0225e-01],\n",
       "          [-4.3094e-01,  5.5496e-01,  3.2745e-01,  ...,  5.4087e-01,\n",
       "           -5.1904e-01,  1.2556e-01],\n",
       "          ...,\n",
       "          [ 4.7155e-01, -1.1563e-01, -6.5061e-01,  ..., -1.0355e+00,\n",
       "           -3.0322e-01, -1.7609e-01],\n",
       "          [ 4.8147e-01, -9.9719e-01,  1.7759e-01,  ..., -1.1303e+00,\n",
       "           -4.8821e-01,  1.7094e-01],\n",
       "          [-1.1472e-01,  3.4962e-01,  1.7965e-01,  ..., -5.2149e-01,\n",
       "           -2.3225e-01,  7.9673e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 0.0200, -0.5600,  0.2173,  ..., -0.3016, -0.5157,  0.2071],\n",
       "          [ 0.0192, -0.5589,  0.2159,  ..., -0.3001, -0.5148,  0.2058],\n",
       "          [-1.6904, -0.3167, -0.6081,  ...,  1.0115, -0.4892, -0.1012],\n",
       "          ...,\n",
       "          [-0.2799, -0.1318, -0.5151,  ..., -1.0882,  0.9022,  0.7933],\n",
       "          [-0.4544,  0.2386, -0.1944,  ..., -0.6267,  0.4549,  0.3659],\n",
       "          [ 0.5489, -0.3753, -1.0339,  ..., -0.1030, -1.0285,  0.3284]],\n",
       "\n",
       "         [[-0.2426, -0.0576, -0.1729,  ..., -0.7178,  0.1675,  0.0620],\n",
       "          [-0.2409, -0.0576, -0.1714,  ..., -0.7156,  0.1673,  0.0621],\n",
       "          [ 0.6509, -0.4350, -0.6617,  ..., -0.2007, -0.8698,  0.2648],\n",
       "          ...,\n",
       "          [ 0.5054,  0.1048, -0.3570,  ...,  0.0828, -0.7957,  0.6246],\n",
       "          [ 0.0963, -0.1841, -0.1819,  ...,  0.3319, -1.1101,  0.0526],\n",
       "          [-0.9183, -0.0192, -0.4608,  ..., -0.2937, -0.0708,  0.4161]],\n",
       "\n",
       "         [[-0.3946, -0.2686, -0.2044,  ..., -0.8170,  0.2967, -0.5106],\n",
       "          [-0.3928, -0.2680, -0.2071,  ..., -0.8170,  0.2964, -0.5107],\n",
       "          [-0.5895,  0.0447,  0.1115,  ...,  1.0345, -0.9110,  0.3345],\n",
       "          ...,\n",
       "          [-0.4230,  0.1752, -0.2198,  ...,  0.7937, -0.0464,  0.2566],\n",
       "          [-0.4726,  0.3880, -0.5379,  ...,  0.5909, -0.3724,  0.8607],\n",
       "          [ 0.8232,  0.3286,  0.5898,  ...,  1.2339,  0.2890,  0.2251]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0473,  0.3975, -0.3052,  ...,  0.0583, -0.5431, -0.0738],\n",
       "          [ 0.0475,  0.3955, -0.3037,  ...,  0.0579, -0.5405, -0.0736],\n",
       "          [ 0.7246,  0.2948, -0.0915,  ..., -0.5253, -0.4632, -0.1476],\n",
       "          ...,\n",
       "          [ 0.0927,  0.9509, -0.0892,  ..., -0.3675,  0.5060, -0.3510],\n",
       "          [ 0.9525,  0.3366, -0.3010,  ..., -1.2475,  0.2417,  0.2712],\n",
       "          [-0.0620,  0.3108,  0.1845,  ..., -0.5237, -0.4859, -0.4288]],\n",
       "\n",
       "         [[-0.1005, -0.4269,  0.0823,  ...,  0.1187,  0.0493, -0.4968],\n",
       "          [-0.1003, -0.4255,  0.0821,  ...,  0.1175,  0.0489, -0.4954],\n",
       "          [ 0.4552,  0.8613,  1.9565,  ..., -1.2304,  0.3381,  0.4058],\n",
       "          ...,\n",
       "          [-0.0822,  0.8629,  0.3495,  ..., -0.5883, -0.0348,  0.1389],\n",
       "          [ 0.4293,  0.6309,  1.0398,  ..., -1.5401,  0.5154,  0.1072],\n",
       "          [ 0.1951,  0.3012, -0.0219,  ..., -0.5020,  0.9702,  1.3853]],\n",
       "\n",
       "         [[ 0.2422, -0.0503,  0.2136,  ..., -0.5088,  0.3274,  0.3208],\n",
       "          [ 0.2418, -0.0500,  0.2129,  ..., -0.5086,  0.3267,  0.3200],\n",
       "          [ 0.6470, -0.2968, -0.8117,  ...,  0.1646, -0.5608, -0.6384],\n",
       "          ...,\n",
       "          [-0.7091,  0.4937, -0.4606,  ...,  0.1470, -1.2241, -0.6617],\n",
       "          [ 0.3900, -0.1595, -1.1233,  ...,  0.6067, -0.7026, -0.9479],\n",
       "          [-1.2612,  0.5341,  0.0045,  ...,  0.7412, -0.6184, -0.3075]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 2.9688e-02,  6.6189e-04,  2.7995e-02,  ...,  7.9262e-03,\n",
       "            2.8694e-02,  1.4060e-01],\n",
       "          [ 2.9369e-02,  9.8170e-04,  2.7947e-02,  ...,  8.2743e-03,\n",
       "            2.8391e-02,  1.4065e-01],\n",
       "          [ 8.9116e-02, -4.4314e-01, -3.4652e-01,  ...,  1.1495e-01,\n",
       "            1.3237e-01, -9.2125e-01],\n",
       "          ...,\n",
       "          [ 1.1790e+00, -5.2962e-01, -7.4298e-01,  ...,  9.0879e-01,\n",
       "            6.7016e-01,  4.5115e-01],\n",
       "          [ 8.2357e-01, -4.8070e-01, -2.6014e-01,  ...,  7.1838e-01,\n",
       "           -5.7054e-01, -3.5037e-01],\n",
       "          [ 1.8609e-01,  6.0261e-01,  1.6831e-01,  ..., -8.7204e-02,\n",
       "            4.2213e-01, -3.8357e-01]],\n",
       "\n",
       "         [[-9.3350e-03, -1.1247e-02, -2.3257e-02,  ...,  1.6930e-02,\n",
       "           -2.0789e-02, -1.1751e-02],\n",
       "          [-9.9456e-03, -1.1071e-02, -2.3170e-02,  ...,  1.7840e-02,\n",
       "           -2.0735e-02, -1.1710e-02],\n",
       "          [-2.6890e-01, -4.1013e-01, -1.1488e+00,  ..., -4.9914e-01,\n",
       "            7.6445e-01, -7.9929e-01],\n",
       "          ...,\n",
       "          [ 4.0520e-01,  2.3554e-01, -3.8810e-01,  ...,  2.1219e-01,\n",
       "           -2.3979e-02, -1.2434e+00],\n",
       "          [-6.3701e-01,  2.3992e-01, -9.2366e-01,  ..., -3.9368e-01,\n",
       "            6.5788e-01, -1.4873e+00],\n",
       "          [ 1.1690e-01,  6.8360e-01,  6.0311e-02,  ..., -1.1764e+00,\n",
       "            1.0156e+00,  1.0736e-01]],\n",
       "\n",
       "         [[-1.2475e-01, -1.8461e-02,  4.5513e-02,  ..., -2.8527e-02,\n",
       "            3.2080e-02, -6.3557e-02],\n",
       "          [-1.2463e-01, -1.8580e-02,  4.5265e-02,  ..., -2.8052e-02,\n",
       "            3.2205e-02, -6.3423e-02],\n",
       "          [-7.6853e-02,  9.6108e-01, -6.8023e-02,  ..., -2.6732e-01,\n",
       "            1.0837e-01, -1.2602e-01],\n",
       "          ...,\n",
       "          [ 3.7114e-01, -8.0672e-02, -5.2466e-01,  ..., -4.9899e-01,\n",
       "            2.2686e-01,  5.5983e-01],\n",
       "          [ 2.5459e-01,  4.3106e-01, -1.0408e+00,  ..., -2.1586e-01,\n",
       "            1.0549e-01,  2.4544e-01],\n",
       "          [-2.2906e-01, -6.5259e-02,  4.7644e-02,  ...,  5.5303e-01,\n",
       "           -1.4197e-01, -6.3132e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.5594e-02,  5.9152e-02, -2.5293e-02,  ...,  2.2770e-02,\n",
       "           -1.6146e-02, -6.1311e-02],\n",
       "          [-3.5667e-02,  5.9523e-02, -2.5459e-02,  ...,  2.2267e-02,\n",
       "           -1.6153e-02, -6.0824e-02],\n",
       "          [ 1.1037e+00,  5.4020e-01,  7.7892e-01,  ..., -4.2563e-01,\n",
       "            6.1950e-02,  8.9015e-02],\n",
       "          ...,\n",
       "          [ 2.0236e-01,  3.8588e-01,  1.4159e+00,  ..., -2.7404e-01,\n",
       "            1.6274e-01,  1.6613e-01],\n",
       "          [ 1.4077e+00,  4.3917e-01,  1.0949e+00,  ..., -8.8500e-01,\n",
       "            5.4237e-01,  1.3514e-01],\n",
       "          [ 5.7357e-02, -3.5351e-01,  2.2247e-01,  ...,  6.3048e-01,\n",
       "           -2.8506e-01, -7.1349e-01]],\n",
       "\n",
       "         [[-8.7232e-02,  6.0305e-03, -8.7616e-02,  ..., -3.1746e-02,\n",
       "            1.9457e-02,  6.7099e-03],\n",
       "          [-8.7355e-02,  6.0636e-03, -8.7473e-02,  ..., -3.1507e-02,\n",
       "            1.9813e-02,  6.7398e-03],\n",
       "          [-4.1251e-01,  3.4385e-01,  1.0089e+00,  ...,  3.5399e-01,\n",
       "            5.5636e-02, -4.2839e-01],\n",
       "          ...,\n",
       "          [-1.0763e-02, -1.2059e-01,  5.0304e-01,  ...,  3.1666e-01,\n",
       "            5.6505e-01, -2.5389e-01],\n",
       "          [ 1.4009e-01,  1.3640e-01,  8.0081e-01,  ...,  3.6186e-02,\n",
       "            4.3921e-01, -4.8826e-01],\n",
       "          [-3.1259e-01,  2.9291e-02,  4.9998e-01,  ..., -3.0165e-01,\n",
       "            6.4677e-01, -1.7428e-01]],\n",
       "\n",
       "         [[ 3.5094e-02,  6.5482e-02, -3.7533e-02,  ...,  6.7327e-03,\n",
       "           -2.8840e-02,  1.1955e-02],\n",
       "          [ 3.4948e-02,  6.5378e-02, -3.7109e-02,  ...,  6.9852e-03,\n",
       "           -2.8162e-02,  1.1796e-02],\n",
       "          [-4.3549e-01, -4.2704e-01, -9.7707e-01,  ...,  2.2277e-02,\n",
       "            7.6834e-01,  1.0953e+00],\n",
       "          ...,\n",
       "          [-3.9486e-01,  5.3067e-01, -3.1189e-01,  ...,  6.6256e-01,\n",
       "           -5.3469e-01,  3.2040e-01],\n",
       "          [-2.7212e-01,  1.4606e-03, -8.8202e-01,  ...,  6.2206e-01,\n",
       "            1.0875e-01,  5.2116e-02],\n",
       "          [ 7.1248e-02, -9.1416e-01, -3.9319e-01,  ..., -6.3089e-02,\n",
       "           -4.7165e-01,  3.6100e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-1.4588e-01, -4.3287e-02,  4.9292e-01,  ...,  1.5627e-03,\n",
       "            7.9148e-01,  2.9859e-01],\n",
       "          [ 7.4112e-01, -5.3555e-01, -9.8543e-01,  ...,  4.9110e-01,\n",
       "            2.0426e-01, -1.3100e-01],\n",
       "          [ 6.4784e-03,  4.2591e-01,  1.3648e+00,  ..., -7.0124e-01,\n",
       "           -3.2159e-01,  6.7093e-01],\n",
       "          ...,\n",
       "          [-2.7487e-01,  7.5109e-01, -1.4617e-01,  ...,  3.5258e-01,\n",
       "           -2.2640e-01,  4.8524e-02],\n",
       "          [-6.5160e-01,  1.1078e+00,  3.6912e-01,  ...,  1.0279e+00,\n",
       "           -5.8729e-01,  1.4208e+00],\n",
       "          [-9.4490e-01,  2.1515e-01,  5.6432e-01,  ...,  7.2938e-01,\n",
       "           -2.0825e-01,  4.3190e-01]],\n",
       "\n",
       "         [[ 2.1872e-01,  1.0304e+00, -1.1891e+00,  ..., -2.1724e-01,\n",
       "           -2.5487e-02, -5.3527e-01],\n",
       "          [-8.6248e-02, -2.3382e-01, -1.6310e-01,  ...,  8.2111e-01,\n",
       "            7.5842e-03, -9.3811e-01],\n",
       "          [-5.4598e-01,  4.8273e-01,  3.6545e+00,  ...,  1.0026e+00,\n",
       "            2.4799e+00,  5.4173e-01],\n",
       "          ...,\n",
       "          [-9.2795e-01, -7.3710e-01,  6.5096e-01,  ..., -3.1101e-02,\n",
       "           -6.3178e-01, -1.1334e+00],\n",
       "          [-4.4716e-01, -1.4758e+00,  3.7371e+00,  ...,  2.2857e+00,\n",
       "            1.3894e+00,  1.4194e+00],\n",
       "          [-8.5721e-01, -3.4706e-01,  1.1730e-01,  ...,  5.4135e-01,\n",
       "            1.2047e-01, -4.7003e-01]],\n",
       "\n",
       "         [[-6.8742e-01,  1.7175e-02, -5.7551e-01,  ..., -6.2997e-02,\n",
       "           -2.5522e-01,  3.1696e-01],\n",
       "          [ 1.4489e+00, -1.9565e+00,  5.2348e-01,  ...,  8.9950e-01,\n",
       "           -1.6698e+00, -1.8725e+00],\n",
       "          [ 1.2145e-03, -8.7972e-01, -3.8976e-01,  ...,  1.0540e+00,\n",
       "            9.2437e-01, -1.2584e+00],\n",
       "          ...,\n",
       "          [-2.0359e-01, -2.9819e-01,  7.7419e-01,  ..., -9.7315e-01,\n",
       "            5.1899e-01,  1.0352e+00],\n",
       "          [-6.4926e-01, -7.6584e-01,  5.5965e-01,  ...,  3.9094e-01,\n",
       "            8.8887e-01,  8.9437e-02],\n",
       "          [ 4.0763e-02, -5.9699e-01,  5.5614e-01,  ..., -2.7798e-01,\n",
       "            4.7349e-01,  8.7777e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.7379e-02, -2.8711e-01, -1.2833e-01,  ...,  1.2181e-01,\n",
       "            2.0266e-01,  1.0362e-01],\n",
       "          [ 3.5671e-01,  5.8521e-01,  1.6009e-01,  ..., -1.2059e+00,\n",
       "           -1.0100e+00, -1.1923e+00],\n",
       "          [-9.1270e-01,  6.2827e-01, -2.5720e-01,  ..., -8.4960e-01,\n",
       "           -1.2824e+00,  5.3316e-02],\n",
       "          ...,\n",
       "          [ 3.6837e-01, -2.7171e-01,  1.2000e+00,  ..., -1.1299e+00,\n",
       "           -9.6932e-01, -9.5106e-02],\n",
       "          [-1.7870e+00, -1.0816e+00, -2.8976e-01,  ..., -9.8666e-01,\n",
       "           -1.5535e+00, -6.3165e-02],\n",
       "          [-6.3793e-02, -4.1573e-01, -1.2990e-02,  ..., -1.2140e+00,\n",
       "           -9.4759e-01, -6.5144e-01]],\n",
       "\n",
       "         [[ 9.2826e-03, -3.2129e-01,  5.9222e-01,  ...,  4.2907e-01,\n",
       "            3.7811e-01,  4.7495e-01],\n",
       "          [-7.8365e-01, -3.3528e-01, -4.7662e-01,  ..., -6.3547e-01,\n",
       "           -2.0441e-01, -2.8823e-01],\n",
       "          [-1.3372e+00, -1.1932e+00, -1.4251e+00,  ..., -9.3939e-01,\n",
       "            9.3976e-01, -1.3279e+00],\n",
       "          ...,\n",
       "          [-1.1768e+00,  6.5233e-01, -2.6520e-01,  ..., -7.3245e-01,\n",
       "           -3.1778e-01, -8.0760e-01],\n",
       "          [-9.2874e-01,  6.4232e-01,  1.0959e+00,  ..., -2.4831e-01,\n",
       "            1.8226e+00, -7.1162e-01],\n",
       "          [-7.8305e-01, -1.2243e+00, -9.3753e-01,  ..., -2.8009e-01,\n",
       "            1.0950e+00, -2.0980e-01]],\n",
       "\n",
       "         [[ 4.2390e-01, -1.9884e-01, -4.7602e-01,  ...,  4.5418e-01,\n",
       "            7.8476e-01,  3.6566e-01],\n",
       "          [ 2.2937e-01,  7.5048e-01,  1.2594e+00,  ...,  9.9084e-01,\n",
       "            1.5078e-01,  4.9048e-01],\n",
       "          [ 1.1349e+00, -9.3953e-01,  4.9388e-02,  ..., -1.6556e+00,\n",
       "            3.4072e-01,  1.3574e+00],\n",
       "          ...,\n",
       "          [-1.7172e+00,  1.6050e+00, -4.2713e-01,  ...,  5.9740e-01,\n",
       "           -8.4771e-01,  3.9803e-01],\n",
       "          [ 1.1136e+00,  1.0794e+00,  4.9023e-01,  ..., -2.3481e+00,\n",
       "            6.8532e-01,  1.4516e+00],\n",
       "          [ 1.8089e-01,  1.0645e+00,  1.1740e+00,  ..., -3.6482e-01,\n",
       "            7.1710e-01,  5.6801e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-1.8231e-03, -2.7317e-03,  1.4071e-04,  ..., -5.1749e-03,\n",
       "            8.1150e-03, -1.5750e-03],\n",
       "          [-6.7700e-01,  3.8656e-01,  6.2552e-01,  ..., -1.7278e-01,\n",
       "           -4.4100e-01,  8.0757e-02],\n",
       "          [ 4.3436e-01, -2.2410e-01, -9.8928e-01,  ..., -1.0741e+00,\n",
       "           -1.1704e+00, -1.6675e+00],\n",
       "          ...,\n",
       "          [-2.6298e-01, -4.2845e-01,  8.8565e-01,  ...,  2.3541e-01,\n",
       "           -7.1889e-01,  7.2689e-01],\n",
       "          [ 4.3264e-01,  2.1460e-01,  1.0776e+00,  ...,  5.7148e-01,\n",
       "           -1.0876e+00,  5.1120e-01],\n",
       "          [ 4.8204e-01, -4.9977e-02,  1.3583e-01,  ..., -1.4045e-01,\n",
       "           -1.2025e+00, -2.9668e-01]],\n",
       "\n",
       "         [[-2.0485e-02, -3.5419e-03,  2.2095e-02,  ..., -9.4822e-03,\n",
       "            1.1576e-02,  2.1664e-02],\n",
       "          [-1.4683e-01, -1.0835e-01, -2.8230e-01,  ..., -1.6653e-01,\n",
       "            8.4368e-02, -9.7541e-02],\n",
       "          [-3.9626e-01, -4.4471e-01, -3.0697e-01,  ..., -5.1132e-01,\n",
       "            2.5002e-01,  2.3613e-01],\n",
       "          ...,\n",
       "          [ 2.3929e-01,  9.0592e-01,  7.3955e-01,  ..., -1.6222e-01,\n",
       "            1.3389e-01, -8.6889e-02],\n",
       "          [ 2.2123e-01,  7.7939e-02, -5.3145e-01,  ..., -2.6917e-01,\n",
       "            6.7747e-02, -4.5932e-01],\n",
       "          [ 4.6952e-01, -7.8040e-01, -3.4254e-01,  ..., -3.5103e-02,\n",
       "           -2.8305e-02, -1.6382e-02]],\n",
       "\n",
       "         [[-1.7796e-02,  4.2735e-03,  1.3468e-02,  ...,  5.7415e-03,\n",
       "           -8.4602e-03, -7.5562e-03],\n",
       "          [ 1.9453e-01, -3.8219e-01,  3.0645e-01,  ...,  1.9109e-01,\n",
       "            2.4112e-01,  1.2715e-01],\n",
       "          [ 6.6046e-02, -6.6597e-01,  2.1391e-01,  ...,  4.0021e-01,\n",
       "            1.0099e+00, -5.2891e-01],\n",
       "          ...,\n",
       "          [ 8.2707e-01, -7.2738e-01, -8.9572e-01,  ..., -1.1137e-01,\n",
       "           -1.1094e+00,  1.2373e+00],\n",
       "          [ 1.9137e-01, -9.6345e-01,  5.5336e-01,  ...,  2.9734e-01,\n",
       "           -1.7989e+00,  7.6976e-01],\n",
       "          [ 5.5226e-01, -3.2269e-01,  8.0426e-03,  ..., -5.9916e-01,\n",
       "           -7.7591e-01,  6.4763e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.7840e-03, -6.5547e-03,  3.4530e-03,  ...,  7.4820e-03,\n",
       "            1.6482e-02,  7.2362e-03],\n",
       "          [-1.3296e-01,  5.6374e-02, -1.2368e-01,  ..., -1.4292e-01,\n",
       "            1.0199e-01, -4.1515e-01],\n",
       "          [-6.6156e-01, -3.0402e-01, -7.7206e-01,  ..., -8.7393e-01,\n",
       "            7.9860e-01,  4.2650e-01],\n",
       "          ...,\n",
       "          [-6.7193e-01,  6.6859e-01,  1.9456e+00,  ..., -9.1724e-02,\n",
       "           -9.3365e-02,  7.5842e-01],\n",
       "          [ 2.7207e-01,  3.2311e-01, -1.8224e+00,  ...,  9.4558e-01,\n",
       "            6.4661e-01,  6.1173e-02],\n",
       "          [-5.8129e-01,  9.8084e-02, -2.8595e-01,  ...,  3.1839e-02,\n",
       "            1.8229e-01, -5.6956e-01]],\n",
       "\n",
       "         [[-1.6651e-03, -6.3864e-03,  2.0184e-03,  ..., -8.8597e-03,\n",
       "           -3.0533e-02,  1.1300e-02],\n",
       "          [-3.1266e-01, -3.4350e-01,  1.0570e-01,  ..., -4.9598e-01,\n",
       "           -7.4653e-01,  5.7392e-01],\n",
       "          [-1.5375e+00, -1.1610e+00, -4.2295e-01,  ...,  1.1942e+00,\n",
       "            3.5764e-01, -1.1442e-01],\n",
       "          ...,\n",
       "          [ 4.9090e-02, -1.4506e-02,  3.7775e-01,  ..., -6.7942e-01,\n",
       "            2.3757e-01, -7.9237e-01],\n",
       "          [ 1.1637e-02, -6.6127e-01,  1.0175e+00,  ...,  1.7322e+00,\n",
       "           -1.2576e-02, -6.8914e-01],\n",
       "          [ 3.9282e-01, -9.6431e-01,  3.7623e-01,  ...,  2.1710e-01,\n",
       "           -4.4144e-02,  3.8554e-01]],\n",
       "\n",
       "         [[-5.1702e-03, -8.2387e-03,  7.3514e-03,  ...,  3.2556e-03,\n",
       "            2.4702e-03,  5.1581e-03],\n",
       "          [-1.7511e-01, -2.1118e-02, -2.8507e-01,  ..., -3.5936e-01,\n",
       "            1.0262e-01, -4.9558e-02],\n",
       "          [-5.4502e-01,  7.0720e-01,  4.5445e-02,  ..., -2.5616e-01,\n",
       "            1.0201e-01,  5.6111e-01],\n",
       "          ...,\n",
       "          [-4.3358e-01,  6.1936e-01, -8.9832e-01,  ..., -9.6341e-01,\n",
       "            2.1530e-02,  3.8771e-01],\n",
       "          [ 2.5315e-01,  1.6708e+00,  8.0823e-01,  ...,  5.8844e-01,\n",
       "            2.5195e-01,  5.9775e-01],\n",
       "          [-1.2363e-01, -5.7406e-01,  3.5278e-01,  ...,  1.6870e-01,\n",
       "           -4.1481e-01,  1.2024e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-0.2155,  0.2429, -0.0240,  ...,  0.1696, -0.0313, -0.3118],\n",
       "          [-0.2141,  0.2421, -0.0237,  ...,  0.1697, -0.0315, -0.3108],\n",
       "          [ 0.6202, -0.3850,  0.0348,  ...,  0.5668,  0.1806,  0.5317],\n",
       "          ...,\n",
       "          [ 0.7450, -0.7543,  0.1941,  ...,  0.4076,  0.1674, -0.0555],\n",
       "          [ 0.6975, -1.0381,  0.5847,  ...,  0.5607,  0.9847,  0.5341],\n",
       "          [-0.5983,  0.1373, -0.5435,  ..., -0.7617, -0.1227,  0.8583]],\n",
       "\n",
       "         [[-0.6911, -0.2754, -0.8573,  ...,  0.2045,  0.7499,  0.0167],\n",
       "          [-0.6893, -0.2747, -0.8550,  ...,  0.2042,  0.7474,  0.0164],\n",
       "          [-0.0876,  0.4278, -0.0888,  ...,  0.3706, -0.4906, -0.6876],\n",
       "          ...,\n",
       "          [-0.2347, -0.2554, -0.5789,  ...,  0.3002,  0.2179, -0.4192],\n",
       "          [ 0.1684,  0.5850, -0.1345,  ...,  1.0873,  0.1017, -0.2291],\n",
       "          [-0.4046, -0.0378, -0.5544,  ..., -0.4070,  0.8160, -0.6909]],\n",
       "\n",
       "         [[ 0.0138, -0.2583, -0.0520,  ...,  0.0059,  1.3176, -0.0337],\n",
       "          [ 0.0135, -0.2575, -0.0517,  ...,  0.0052,  1.3126, -0.0331],\n",
       "          [-0.3081,  0.9767,  0.0956,  ..., -0.5635, -0.3065, -0.3079],\n",
       "          ...,\n",
       "          [-0.4804,  0.1244, -1.0424,  ..., -0.1846, -0.4754, -0.1041],\n",
       "          [ 0.6781,  0.4745, -0.6389,  ..., -0.3853, -0.6468,  0.7484],\n",
       "          [ 0.6887,  0.1438,  0.0709,  ..., -0.1670,  0.5994,  0.2389]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0390, -0.8676,  0.3170,  ...,  0.1487, -0.2095,  0.0944],\n",
       "          [ 0.0383, -0.8642,  0.3159,  ...,  0.1477, -0.2092,  0.0946],\n",
       "          [ 0.2869,  0.0901,  0.7250,  ...,  0.7067,  0.2652, -0.5204],\n",
       "          ...,\n",
       "          [ 0.5753,  0.2678,  0.5919,  ..., -0.4417,  0.3581,  0.5675],\n",
       "          [ 0.0304,  0.0366,  0.6931,  ..., -0.0621,  0.3126,  0.5014],\n",
       "          [ 0.1670,  0.4118, -0.7815,  ..., -0.5875,  1.1794,  0.4279]],\n",
       "\n",
       "         [[-0.3289, -0.3325, -0.1874,  ...,  0.7743,  0.0256,  0.0744],\n",
       "          [-0.3297, -0.3332, -0.1875,  ...,  0.7747,  0.0274,  0.0744],\n",
       "          [ 0.6571,  0.6907,  0.0504,  ..., -0.7124, -0.4001, -0.3640],\n",
       "          ...,\n",
       "          [-0.1680, -0.3630,  0.1439,  ..., -0.7592, -0.0776,  0.0222],\n",
       "          [ 0.2712, -0.1934,  0.5410,  ..., -0.5533, -0.1157, -0.9127],\n",
       "          [ 0.1269,  0.6481,  0.7949,  ..., -1.3283, -0.4923, -0.0733]],\n",
       "\n",
       "         [[-0.1930,  0.3010,  0.0714,  ..., -0.4798,  0.7340,  0.3108],\n",
       "          [-0.1932,  0.3001,  0.0719,  ..., -0.4789,  0.7319,  0.3103],\n",
       "          [ 1.4293, -0.1060,  0.5939,  ...,  0.0991,  1.7318, -0.7346],\n",
       "          ...,\n",
       "          [ 0.0644, -0.3376, -1.0331,  ..., -0.4677,  0.2625, -0.6961],\n",
       "          [ 1.4755, -0.2868, -0.6793,  ...,  0.1492,  0.7923, -0.2270],\n",
       "          [-0.3499,  0.8649, -0.3157,  ..., -0.0258,  0.0758, -0.2397]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[-0.0329,  0.0131,  0.0955,  ..., -0.0259, -0.0338, -0.0492],\n",
       "          [-0.0329,  0.0133,  0.0953,  ..., -0.0255, -0.0342, -0.0492],\n",
       "          [ 0.5547,  0.3241, -0.6176,  ...,  0.0343,  0.3827,  0.9263],\n",
       "          ...,\n",
       "          [-0.6998,  0.3713, -1.0355,  ..., -0.2361,  0.5534, -0.3571],\n",
       "          [-0.4360,  0.1707, -1.1169,  ..., -0.9136,  0.2681,  0.2685],\n",
       "          [-0.5884, -0.3455,  0.1491,  ..., -0.1909, -0.0295, -0.1725]],\n",
       "\n",
       "         [[-0.0109,  0.0878,  0.0223,  ..., -0.0741,  0.1241, -0.0122],\n",
       "          [-0.0107,  0.0873,  0.0227,  ..., -0.0738,  0.1246, -0.0124],\n",
       "          [ 0.3400, -0.2914, -0.0313,  ...,  1.1865, -0.2301, -2.0049],\n",
       "          ...,\n",
       "          [-0.1182,  0.2848,  0.4859,  ..., -0.3221, -0.5488,  0.6150],\n",
       "          [ 0.1765, -0.0253, -0.4276,  ...,  1.1431, -0.1809,  0.1195],\n",
       "          [ 0.3480, -0.7177,  0.1648,  ...,  0.4135,  0.5430, -0.0812]],\n",
       "\n",
       "         [[-0.0273,  0.0307, -0.0789,  ..., -0.0092, -0.0248,  0.0522],\n",
       "          [-0.0267,  0.0305, -0.0787,  ..., -0.0095, -0.0250,  0.0526],\n",
       "          [ 0.6077,  0.1560,  0.0359,  ..., -0.3190,  0.3640, -0.2637],\n",
       "          ...,\n",
       "          [-0.0326,  0.2729, -0.3659,  ..., -0.3313, -0.2056, -0.8542],\n",
       "          [ 0.2565, -0.7359,  0.3025,  ..., -0.6346, -0.3226, -1.4320],\n",
       "          [-0.1580,  0.1059,  0.6284,  ...,  0.0202, -0.3954, -0.0189]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0125,  0.0174, -0.0109,  ..., -0.0250,  0.0052, -0.0572],\n",
       "          [ 0.0124,  0.0172, -0.0108,  ..., -0.0249,  0.0048, -0.0569],\n",
       "          [ 0.1938,  0.3010, -0.1200,  ..., -0.2149, -0.0193, -0.3014],\n",
       "          ...,\n",
       "          [-0.0523,  0.1466, -0.2824,  ...,  0.3389, -0.6205, -0.3021],\n",
       "          [-0.2434,  0.5452, -0.0369,  ..., -0.0453, -0.2510, -0.1426],\n",
       "          [ 1.0097, -0.5158,  0.1001,  ..., -0.4132,  0.0366, -0.3009]],\n",
       "\n",
       "         [[-0.1541,  0.0744, -0.4514,  ..., -0.0299,  0.0753, -0.5099],\n",
       "          [-0.1540,  0.0741, -0.4516,  ..., -0.0288,  0.0753, -0.5099],\n",
       "          [ 0.2295,  0.2144,  0.0568,  ..., -0.2749,  0.0145,  0.1962],\n",
       "          ...,\n",
       "          [ 0.0496, -0.3196, -0.5152,  ..., -0.4022,  0.2750,  0.4050],\n",
       "          [-0.2593, -0.3249, -0.2358,  ...,  0.5864,  0.7580, -0.0327],\n",
       "          [ 0.6305,  0.0979, -0.2458,  ...,  0.0776,  0.0131,  0.0615]],\n",
       "\n",
       "         [[ 0.0096,  0.2019, -0.1255,  ...,  0.0847,  0.0233,  0.0635],\n",
       "          [ 0.0096,  0.2023, -0.1251,  ...,  0.0846,  0.0233,  0.0636],\n",
       "          [-0.1707, -0.1735, -0.7102,  ..., -0.4759,  0.7684, -0.3509],\n",
       "          ...,\n",
       "          [-0.8402, -0.1293,  0.7156,  ..., -0.6293,  1.0491, -0.0270],\n",
       "          [-1.8915, -0.4885,  0.1909,  ..., -0.3167,  0.4573, -0.5098],\n",
       "          [-0.0928,  0.4460,  0.3759,  ...,  0.5368, -0.0637,  0.0172]]]],\n",
       "       grad_fn=<CloneBackward0>)), (tensor([[[[ 2.0748e-01,  3.1731e-01,  8.4868e-01,  ..., -8.6423e-02,\n",
       "           -4.5393e-02,  2.1808e-01],\n",
       "          [-3.2785e-01, -9.2520e-01,  4.8536e-02,  ...,  6.4595e-01,\n",
       "           -1.8516e-01,  4.8973e-02],\n",
       "          [-1.1121e+00, -1.6227e+00, -6.1661e-01,  ...,  1.2874e+00,\n",
       "            7.5585e-01,  3.1351e-01],\n",
       "          ...,\n",
       "          [ 1.1397e+00, -1.1208e+00,  2.9764e-01,  ...,  2.3575e-01,\n",
       "            9.2447e-01, -5.6816e-01],\n",
       "          [-7.5895e-01, -5.2831e-01, -1.5269e+00,  ..., -3.9529e-01,\n",
       "            6.9109e-01,  9.8020e-01],\n",
       "          [-1.2996e+00, -1.6288e-01, -6.8783e-01,  ...,  1.8062e-03,\n",
       "            3.7635e-01, -5.1741e-01]],\n",
       "\n",
       "         [[-6.1031e-02, -3.7694e-01, -8.9050e-02,  ...,  1.1124e+00,\n",
       "           -7.0183e-01,  7.8530e-01],\n",
       "          [ 6.8638e-01, -5.5687e-02,  1.2760e+00,  ..., -3.0661e-01,\n",
       "           -1.6296e-02,  1.6586e-01],\n",
       "          [-1.2878e+00,  1.3493e+00,  2.1806e-02,  ..., -1.2488e+00,\n",
       "            7.1979e-01, -2.4634e-01],\n",
       "          ...,\n",
       "          [-9.3392e-01,  1.7653e+00, -1.6631e+00,  ..., -9.1839e-01,\n",
       "           -4.3602e-01, -2.0544e+00],\n",
       "          [ 1.8410e+00,  1.7536e+00, -1.5592e+00,  ..., -1.6537e-02,\n",
       "            4.4033e-01, -2.0355e+00],\n",
       "          [ 5.5468e-01,  2.1449e+00, -6.0646e-01,  ...,  7.5934e-01,\n",
       "            1.3594e+00, -2.0163e+00]],\n",
       "\n",
       "         [[ 4.2593e-01,  4.5002e-01, -3.3097e-01,  ...,  5.2046e-01,\n",
       "           -1.2664e+00, -1.1763e-01],\n",
       "          [-1.1143e+00, -3.3518e-01,  5.2163e-01,  ..., -6.5337e-01,\n",
       "            3.2531e-01,  1.9511e-01],\n",
       "          [ 1.5503e+00,  1.3672e+00, -9.4792e-01,  ..., -4.1868e-01,\n",
       "           -1.3279e+00, -2.4712e+00],\n",
       "          ...,\n",
       "          [-1.0798e+00,  1.7732e-01, -8.7809e-02,  ...,  1.8706e+00,\n",
       "           -5.8768e-01,  4.2620e-01],\n",
       "          [ 3.6994e-01,  1.7189e+00, -3.9670e-01,  ...,  1.0138e+00,\n",
       "            9.6053e-01, -5.9405e-01],\n",
       "          [-1.3377e+00,  3.2048e-01,  1.9540e-01,  ...,  4.8125e-01,\n",
       "           -1.8806e-01, -6.1178e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.3108e-01,  5.8131e-01, -3.5704e-02,  ...,  1.6588e-01,\n",
       "            3.7000e-01,  1.4539e+00],\n",
       "          [ 6.6895e-01,  1.0890e+00,  6.3122e-01,  ...,  1.8337e-01,\n",
       "           -5.5025e-01,  1.1113e-01],\n",
       "          [-2.7589e-01,  1.8879e-01,  5.0381e-01,  ...,  1.0118e+00,\n",
       "           -1.1430e-01, -3.6720e-01],\n",
       "          ...,\n",
       "          [-1.8322e+00,  1.7085e-01,  1.6339e+00,  ...,  4.6729e-01,\n",
       "           -1.5937e+00, -5.5141e-01],\n",
       "          [-1.1280e+00, -9.5017e-01,  4.1286e-02,  ...,  1.9700e+00,\n",
       "           -1.8162e+00,  6.5187e-01],\n",
       "          [-1.1889e+00,  2.9499e-01,  7.8681e-01,  ...,  2.1230e+00,\n",
       "           -2.2363e+00, -1.6988e+00]],\n",
       "\n",
       "         [[ 1.8868e-01,  3.3221e-02,  5.4113e-01,  ...,  4.5761e-01,\n",
       "           -4.0378e-01, -3.1131e-01],\n",
       "          [ 2.9068e-01, -6.5338e-01, -5.8117e-01,  ..., -1.1295e-01,\n",
       "           -9.1184e-01,  5.3491e-02],\n",
       "          [ 1.5390e+00, -3.7366e-01, -2.7496e-01,  ..., -5.6610e-01,\n",
       "           -5.9903e-01, -1.8994e+00],\n",
       "          ...,\n",
       "          [-7.6115e-03,  6.2502e-02,  3.4380e-01,  ..., -1.0834e+00,\n",
       "           -1.4874e+00,  1.8148e+00],\n",
       "          [-1.1033e+00,  1.0274e-01, -8.8705e-01,  ...,  5.0330e-01,\n",
       "           -1.0892e+00,  1.2842e+00],\n",
       "          [-3.7309e-01, -6.5457e-01, -5.3715e-01,  ..., -1.2671e+00,\n",
       "           -8.2259e-01, -1.9797e-01]],\n",
       "\n",
       "         [[ 1.8991e-01, -3.3051e-01,  2.3190e-01,  ..., -2.7071e-02,\n",
       "           -2.8336e-02,  8.5212e-02],\n",
       "          [ 2.3097e-01,  7.1652e-01,  3.0094e-01,  ...,  2.6568e-01,\n",
       "           -8.1436e-01,  1.3847e+00],\n",
       "          [ 1.7802e-01,  1.4239e+00,  1.6348e-01,  ...,  5.3335e-01,\n",
       "            1.9636e+00, -2.9946e-01],\n",
       "          ...,\n",
       "          [ 2.4668e-01,  9.0130e-01,  1.5297e-01,  ..., -5.0114e-01,\n",
       "            7.3730e-02, -4.7371e-02],\n",
       "          [-1.4879e-01,  1.9323e+00,  3.9775e-01,  ...,  4.2179e-01,\n",
       "            1.0472e+00, -2.7722e-01],\n",
       "          [-8.3809e-02,  2.1514e+00,  6.8674e-02,  ..., -5.1543e-01,\n",
       "            4.6735e-01,  4.3076e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-8.0586e-04, -9.1004e-03,  5.4960e-03,  ..., -8.2264e-03,\n",
       "            1.2844e-02, -2.1961e-02],\n",
       "          [-2.3438e-01, -1.0140e-01, -4.9848e-01,  ..., -1.9967e-01,\n",
       "            5.0268e-01, -4.8227e-02],\n",
       "          [-8.1528e-01, -4.1574e-01, -4.2566e-01,  ...,  1.9509e-01,\n",
       "           -1.3151e+00,  3.3108e-02],\n",
       "          ...,\n",
       "          [-5.5295e-01,  7.1515e-01,  1.9011e-01,  ...,  1.2250e+00,\n",
       "           -1.6685e-02,  2.7417e-01],\n",
       "          [-1.4803e+00, -3.6466e-01, -3.4617e-01,  ...,  7.6783e-01,\n",
       "           -6.8932e-01, -5.2266e-03],\n",
       "          [-1.2503e+00,  7.5472e-02, -3.7798e-02,  ...,  2.3863e-01,\n",
       "           -6.5377e-01,  3.8616e-01]],\n",
       "\n",
       "         [[-5.1835e-02, -1.6828e-02, -9.9982e-02,  ...,  2.6208e-02,\n",
       "           -5.9944e-02, -4.9202e-02],\n",
       "          [ 7.4328e-02,  2.8744e-01,  5.4301e-01,  ..., -2.9901e-01,\n",
       "           -2.5677e-01, -8.9817e-02],\n",
       "          [ 6.0099e-02,  1.2152e-01,  3.8189e-01,  ..., -3.2055e-01,\n",
       "            2.6103e-01, -1.3121e-01],\n",
       "          ...,\n",
       "          [-5.5817e-01,  1.0497e+00,  2.2509e-01,  ..., -6.9649e-01,\n",
       "            2.6136e-01, -5.3485e-01],\n",
       "          [-9.5564e-04, -4.4376e-02,  2.4125e-01,  ..., -6.7236e-01,\n",
       "           -6.7969e-01, -5.4388e-01],\n",
       "          [ 1.5433e-01,  3.5734e-01,  5.7303e-01,  ..., -2.5884e-01,\n",
       "           -3.1318e-02, -6.9936e-01]],\n",
       "\n",
       "         [[-8.6539e-04,  4.4211e-03, -4.3843e-03,  ..., -1.0141e-02,\n",
       "           -2.6205e-03, -2.6414e-03],\n",
       "          [-2.1480e-01,  4.8993e-01, -7.0026e-02,  ..., -4.6441e-01,\n",
       "            1.9292e-02,  1.0098e-02],\n",
       "          [-7.6155e-01,  4.1403e-01,  5.3388e-01,  ...,  4.5494e-01,\n",
       "            6.5824e-02,  4.9269e-01],\n",
       "          ...,\n",
       "          [-5.0106e-01, -8.8444e-02,  2.0668e-01,  ...,  1.7033e-02,\n",
       "            1.0927e+00, -1.2565e+00],\n",
       "          [ 1.9401e-01,  4.1423e-01, -9.9797e-02,  ...,  1.1457e+00,\n",
       "            9.8625e-01, -5.4807e-01],\n",
       "          [ 8.2561e-01, -4.4749e-01, -3.4078e-01,  ...,  4.6050e-01,\n",
       "           -2.4576e-02, -2.1768e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.5466e-03, -5.7515e-03,  5.8140e-03,  ..., -5.2644e-03,\n",
       "           -6.3891e-03,  8.7282e-03],\n",
       "          [ 9.5226e-01, -4.7896e-01,  3.9060e-01,  ..., -2.5911e-01,\n",
       "           -3.8669e-01,  1.1164e+00],\n",
       "          [ 3.8114e-01,  6.8669e-01,  8.2626e-01,  ...,  7.5912e-01,\n",
       "           -2.3210e-01, -3.1074e-01],\n",
       "          ...,\n",
       "          [ 6.7785e-01,  5.8260e-01, -1.0338e-01,  ...,  3.7526e-01,\n",
       "           -1.0932e+00, -8.7968e-02],\n",
       "          [ 4.7962e-01, -4.8438e-01,  1.6721e+00,  ...,  3.4719e-01,\n",
       "           -5.6619e-01, -1.1258e+00],\n",
       "          [ 9.5314e-01,  3.3843e-01,  1.4503e+00,  ...,  7.7840e-01,\n",
       "           -4.2852e-01, -9.7200e-01]],\n",
       "\n",
       "         [[ 1.5830e-02, -8.1555e-03,  1.3413e-03,  ...,  2.5719e-02,\n",
       "            1.2264e-02, -6.3531e-03],\n",
       "          [ 2.7071e-01,  1.5412e-01,  1.4399e-01,  ...,  1.1327e-01,\n",
       "            2.2706e-01, -4.9861e-02],\n",
       "          [-8.0030e-01, -3.6650e-01,  2.1245e-01,  ..., -3.0632e-01,\n",
       "            6.2399e-01, -1.1351e-01],\n",
       "          ...,\n",
       "          [ 5.6319e-01,  1.1391e-01,  3.0771e-01,  ...,  2.1757e-01,\n",
       "           -1.8384e-02, -2.3628e-01],\n",
       "          [ 3.2923e-02, -5.8188e-01,  2.7501e-01,  ...,  1.8877e-02,\n",
       "           -5.0880e-01, -1.0877e-01],\n",
       "          [-1.5840e-01, -2.5919e-01, -2.8684e-01,  ...,  5.6982e-01,\n",
       "           -7.4270e-01, -8.8070e-02]],\n",
       "\n",
       "         [[-1.4338e-02,  1.0799e-02,  2.1338e-03,  ..., -9.9770e-04,\n",
       "            6.7818e-03,  3.7383e-03],\n",
       "          [ 1.1003e-01,  6.3814e-02, -2.8152e-01,  ...,  9.8984e-02,\n",
       "           -2.3568e-01,  3.8538e-01],\n",
       "          [ 1.2045e+00, -7.4291e-01, -1.4322e-01,  ..., -1.1674e+00,\n",
       "            1.6635e-01, -7.5323e-01],\n",
       "          ...,\n",
       "          [ 2.6939e-02,  7.4283e-01,  2.9215e-01,  ...,  6.8747e-01,\n",
       "            1.3263e-01, -2.9831e-02],\n",
       "          [-1.4662e-01,  2.3704e-02, -1.5173e+00,  ..., -4.1225e-01,\n",
       "            4.7575e-01,  4.2477e-01],\n",
       "          [ 1.7882e-01,  6.6985e-01, -1.1437e+00,  ..., -8.0125e-02,\n",
       "           -1.4750e-01,  4.8225e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-0.0477, -0.3300,  0.4666,  ...,  0.1797, -0.0407, -0.5009],\n",
       "          [-0.0474, -0.3299,  0.4649,  ...,  0.1804, -0.0415, -0.4986],\n",
       "          [ 1.4918,  0.0177,  0.4802,  ...,  1.6984, -0.1791, -0.9839],\n",
       "          ...,\n",
       "          [-0.4105, -0.9425, -0.2783,  ...,  0.8362, -0.4122, -1.4996],\n",
       "          [-0.0724, -0.6681,  0.0954,  ...,  0.9147, -0.6940, -1.0577],\n",
       "          [-0.9601, -0.3415, -0.2596,  ...,  0.8897, -1.3160, -0.3720]],\n",
       "\n",
       "         [[ 0.3485, -0.1207,  0.3783,  ...,  1.0908, -0.2239, -0.1132],\n",
       "          [ 0.3460, -0.1180,  0.3771,  ...,  1.0865, -0.2247, -0.1116],\n",
       "          [ 0.0625, -0.0926, -0.1584,  ...,  0.0397,  1.1495, -0.1153],\n",
       "          ...,\n",
       "          [ 0.6071, -0.0592, -0.1842,  ...,  0.0310,  0.2011,  0.6989],\n",
       "          [ 0.4760, -0.2919, -0.5879,  ...,  0.2278,  0.0889,  0.0537],\n",
       "          [-0.1945, -0.5934,  0.7921,  ...,  0.9752,  0.4554,  1.6794]],\n",
       "\n",
       "         [[ 0.1561,  0.1334,  0.0701,  ...,  0.1317, -0.0766,  0.2046],\n",
       "          [ 0.1562,  0.1331,  0.0695,  ...,  0.1325, -0.0770,  0.2046],\n",
       "          [ 0.2314,  0.9222, -0.2539,  ..., -0.2995, -0.0267,  0.6124],\n",
       "          ...,\n",
       "          [ 0.5386,  0.1503, -0.7239,  ..., -0.9276, -0.0831, -0.9885],\n",
       "          [ 0.8270,  0.3260,  0.3354,  ..., -1.0497, -0.3435, -0.7301],\n",
       "          [ 0.5498, -0.7266, -0.2107,  ..., -0.8229, -1.0559,  0.3066]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1948,  0.1743,  0.3788,  ...,  0.0027,  0.2864,  0.1659],\n",
       "          [-0.1958,  0.1735,  0.3770,  ...,  0.0018,  0.2856,  0.1652],\n",
       "          [-0.2145,  0.3638, -0.9326,  ...,  1.1852, -0.6461,  0.5987],\n",
       "          ...,\n",
       "          [-0.9137,  0.5762, -0.9269,  ..., -0.1038, -0.5417,  0.9167],\n",
       "          [-0.0859, -0.2183, -0.5422,  ...,  0.6719, -0.2260,  0.6283],\n",
       "          [ 0.2203,  0.9177,  0.4988,  ...,  0.1778, -0.4390,  0.7072]],\n",
       "\n",
       "         [[ 0.2711, -0.3777, -0.1671,  ...,  0.2387, -1.0399, -0.5312],\n",
       "          [ 0.2709, -0.3766, -0.1662,  ...,  0.2378, -1.0365, -0.5298],\n",
       "          [-0.0093,  0.3757, -0.6381,  ..., -0.2660, -0.0356,  0.1120],\n",
       "          ...,\n",
       "          [-0.2193,  0.5333, -0.3415,  ..., -0.1052,  0.0993,  0.1217],\n",
       "          [-0.0818,  0.4066, -0.7218,  ...,  0.1053,  0.7820, -0.3232],\n",
       "          [ 1.1121,  0.8829,  1.1138,  ..., -0.8271, -0.2861, -0.2541]],\n",
       "\n",
       "         [[ 0.2035,  0.2360, -0.0954,  ...,  0.4947,  0.0130, -0.0936],\n",
       "          [ 0.2029,  0.2352, -0.0948,  ...,  0.4929,  0.0132, -0.0935],\n",
       "          [-0.3620, -0.3514, -0.5643,  ..., -0.6364, -0.1968,  0.2759],\n",
       "          ...,\n",
       "          [-0.5190,  0.0038,  0.0847,  ...,  0.4083, -1.5166,  0.1937],\n",
       "          [-0.6937, -0.5407, -0.3066,  ..., -0.2147, -0.8897,  0.0917],\n",
       "          [-0.3703,  0.3099, -0.7780,  ...,  0.0780,  0.1829,  0.2797]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[-6.1211e-02,  2.3759e-01,  2.1901e-01,  ...,  2.4743e-01,\n",
       "            2.1834e-01, -2.8256e-01],\n",
       "          [-6.1432e-02,  2.3755e-01,  2.1928e-01,  ...,  2.4762e-01,\n",
       "            2.1823e-01, -2.8299e-01],\n",
       "          [-1.4163e-01,  2.9731e-01,  5.0350e-01,  ...,  4.2763e-01,\n",
       "            1.4378e-01,  2.7686e-04],\n",
       "          ...,\n",
       "          [-2.4534e-01, -1.6255e-01, -9.4114e-02,  ...,  3.8654e-01,\n",
       "           -8.0549e-02, -3.9397e-02],\n",
       "          [-4.0430e-01,  3.6554e-01,  7.3185e-02,  ...,  3.6562e-01,\n",
       "           -3.1569e-01,  7.3120e-01],\n",
       "          [-3.7050e-02, -5.7718e-01,  2.5851e-01,  ...,  3.2815e-01,\n",
       "           -3.7043e-01,  1.8290e-01]],\n",
       "\n",
       "         [[ 4.0296e-02,  3.1473e-03, -2.3701e-02,  ..., -3.3420e-02,\n",
       "            9.4946e-02, -8.9432e-02],\n",
       "          [ 4.0436e-02,  3.3563e-03, -2.4001e-02,  ..., -3.3439e-02,\n",
       "            9.5041e-02, -8.9198e-02],\n",
       "          [-1.0693e+00,  9.9525e-02,  6.8744e-01,  ..., -3.3188e-02,\n",
       "            2.1465e-01,  9.5743e-02],\n",
       "          ...,\n",
       "          [-2.4054e-01,  1.0934e-01,  8.5966e-01,  ...,  4.7961e-01,\n",
       "            4.1979e-01,  3.8764e-02],\n",
       "          [-7.2493e-01, -3.7214e-01,  3.6732e-01,  ..., -7.6385e-02,\n",
       "           -3.4191e-01,  3.7084e-01],\n",
       "          [ 1.1219e-01,  5.5082e-01,  9.6979e-02,  ..., -3.6796e-01,\n",
       "           -1.5937e-01,  7.3087e-01]],\n",
       "\n",
       "         [[ 1.3838e-01, -7.5857e-02,  1.8179e-01,  ...,  3.0175e-02,\n",
       "           -3.6920e-02, -1.7733e-02],\n",
       "          [ 1.3808e-01, -7.5881e-02,  1.8202e-01,  ...,  3.0528e-02,\n",
       "           -3.6285e-02, -1.7915e-02],\n",
       "          [-4.5714e-01,  1.8842e-01, -7.7914e-01,  ..., -8.1026e-01,\n",
       "           -7.8589e-01,  4.8818e-01],\n",
       "          ...,\n",
       "          [ 1.0861e-01,  2.0293e-01,  3.2418e-01,  ...,  4.0940e-01,\n",
       "           -3.2122e-01, -3.7893e-01],\n",
       "          [ 1.0237e-01,  5.0427e-01, -1.7685e-01,  ...,  3.3743e-01,\n",
       "           -1.1641e-01,  1.6652e-01],\n",
       "          [ 1.6492e-01,  2.0030e-01,  1.7161e-01,  ..., -2.2045e-01,\n",
       "            3.4274e-01,  2.2061e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.9889e-02,  3.3042e-02, -5.1011e-02,  ...,  3.3633e-03,\n",
       "           -1.1558e-01,  2.4781e-02],\n",
       "          [ 7.9796e-02,  3.3115e-02, -5.0677e-02,  ...,  3.2281e-03,\n",
       "           -1.1532e-01,  2.4558e-02],\n",
       "          [ 2.7970e-02, -1.6661e-02, -1.7570e-01,  ..., -1.5818e-01,\n",
       "            5.9033e-02,  1.2867e+00],\n",
       "          ...,\n",
       "          [-2.3393e-01, -1.1784e-01,  5.6917e-01,  ..., -1.3061e-01,\n",
       "            4.5222e-01,  4.1605e-02],\n",
       "          [-6.6367e-02,  4.8057e-01,  4.6127e-01,  ..., -1.0397e-01,\n",
       "            7.9764e-01,  1.0540e+00],\n",
       "          [-5.9083e-02, -2.9203e-01,  3.2598e-01,  ...,  7.4026e-02,\n",
       "            4.1882e-01, -1.2556e-02]],\n",
       "\n",
       "         [[ 2.4968e-03, -2.7832e-02,  3.4855e-02,  ..., -2.3710e-03,\n",
       "           -1.5339e-03,  3.9428e-03],\n",
       "          [ 2.4744e-03, -2.7754e-02,  3.4874e-02,  ..., -2.3714e-03,\n",
       "           -1.4449e-03,  3.9084e-03],\n",
       "          [ 3.9191e-02,  4.8554e-01,  1.1007e+00,  ...,  8.0442e-01,\n",
       "            7.5640e-01, -5.1940e-01],\n",
       "          ...,\n",
       "          [-4.4625e-01,  5.8506e-01,  2.2236e-01,  ..., -1.2070e-01,\n",
       "            7.7842e-01, -8.6742e-01],\n",
       "          [-3.1428e-01,  9.8783e-01,  9.0395e-01,  ...,  2.6065e-01,\n",
       "            6.6256e-01, -1.4640e+00],\n",
       "          [-1.0063e+00,  3.4073e-01,  7.3523e-01,  ...,  1.2482e+00,\n",
       "            3.7139e-01, -6.9290e-01]],\n",
       "\n",
       "         [[-3.4731e-02, -7.8977e-02, -1.9828e-02,  ...,  9.0891e-03,\n",
       "           -1.2225e-02, -1.0823e-02],\n",
       "          [-3.3994e-02, -7.8577e-02, -2.0037e-02,  ...,  8.7604e-03,\n",
       "           -1.1926e-02, -1.0791e-02],\n",
       "          [ 3.9638e-01,  3.7229e-01,  2.6320e-01,  ...,  3.7594e-01,\n",
       "            6.3372e-02,  1.5875e-01],\n",
       "          ...,\n",
       "          [-2.0261e-01,  2.2631e-01,  3.2191e-01,  ...,  3.3857e-01,\n",
       "            4.4029e-01, -7.7870e-02],\n",
       "          [-4.4960e-01, -2.9011e-01,  2.7836e-01,  ...,  3.3018e-01,\n",
       "           -4.7337e-02,  4.4361e-02],\n",
       "          [ 2.7028e-01,  2.1585e-01,  2.8427e-01,  ..., -6.7531e-02,\n",
       "            1.8280e-01, -5.2216e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[ 1.3722, -0.1360,  1.2855,  ...,  0.6428,  1.1533, -0.1590],\n",
       "          [-0.3608, -0.7314, -0.0689,  ..., -0.4813, -0.5131,  0.0466],\n",
       "          [-2.4046,  0.7385, -0.2303,  ..., -0.0073, -1.8726,  0.3064],\n",
       "          ...,\n",
       "          [-1.6838, -1.5568,  0.7476,  ..., -0.5014, -0.9992, -1.3613],\n",
       "          [-2.0849, -0.3042,  0.7236,  ..., -2.0734, -2.1134, -0.8453],\n",
       "          [-1.7925, -0.0969,  0.0046,  ..., -1.2817, -1.9155, -0.8517]],\n",
       "\n",
       "         [[ 0.0597, -0.0349,  1.3520,  ...,  0.5151, -0.0120,  0.5730],\n",
       "          [ 0.4407,  2.2057, -1.6599,  ..., -0.4220, -1.1189, -0.1397],\n",
       "          [ 2.9091,  0.9577, -3.4191,  ..., -0.3504, -1.5439, -1.4999],\n",
       "          ...,\n",
       "          [ 1.5902,  2.6057, -4.3623,  ..., -0.5268,  1.5593, -1.5722],\n",
       "          [ 0.1877,  1.9561, -3.1491,  ...,  0.9026, -0.0366, -1.3898],\n",
       "          [ 2.1044,  4.2438, -2.8894,  ...,  1.4198, -0.4994, -1.0772]],\n",
       "\n",
       "         [[-1.6652,  0.4149,  0.0460,  ..., -0.4968,  0.9597,  0.0085],\n",
       "          [-0.4698, -0.4629,  0.6344,  ..., -0.3696,  0.4182, -0.9281],\n",
       "          [ 2.3885, -1.7341,  1.0925,  ...,  0.1021, -1.2192,  1.6591],\n",
       "          ...,\n",
       "          [ 0.6496, -0.9434,  0.3037,  ...,  0.3781, -0.7674, -1.5417],\n",
       "          [ 2.5825,  1.3952,  1.8551,  ...,  0.6708, -2.8357,  0.7549],\n",
       "          [ 0.8688, -0.5657,  0.6168,  ...,  0.6715, -0.9575, -0.2442]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3243, -0.1938, -0.4979,  ...,  1.0804, -0.0491, -0.0596],\n",
       "          [ 1.1790,  1.1680,  1.6374,  ..., -0.1711,  0.9862,  0.1343],\n",
       "          [ 1.1567,  0.8374,  1.2719,  ..., -2.1472, -1.1912, -0.9936],\n",
       "          ...,\n",
       "          [ 0.4580, -0.9890,  0.6113,  ..., -0.5170,  1.0610,  1.3789],\n",
       "          [ 1.8306, -0.1251,  0.7517,  ..., -1.0653,  0.9219, -0.3495],\n",
       "          [ 2.0175, -0.0507,  1.8266,  ..., -0.9882,  1.5621,  0.1119]],\n",
       "\n",
       "         [[ 0.0263,  0.2366, -0.6576,  ...,  0.7007,  0.0984,  0.7285],\n",
       "          [-0.4679,  0.1914,  0.0584,  ...,  0.3080,  0.3469, -0.8669],\n",
       "          [ 0.2663, -1.0873,  0.8497,  ...,  0.4687,  1.4425, -0.5303],\n",
       "          ...,\n",
       "          [-0.2704, -0.7947, -0.3245,  ..., -0.5422, -2.2509, -0.8126],\n",
       "          [-0.4099, -1.0773,  1.2839,  ..., -2.0386,  0.0236, -0.6875],\n",
       "          [-0.6842, -0.4280,  0.9214,  ..., -2.3046,  0.4187,  0.1606]],\n",
       "\n",
       "         [[ 0.5885,  0.0295, -1.8407,  ...,  0.3195, -0.1247,  0.1167],\n",
       "          [-0.6757, -0.0157,  0.5157,  ...,  0.5546,  0.2498, -1.0240],\n",
       "          [-1.1173, -0.5875,  1.3447,  ...,  1.2597,  0.2178,  0.7297],\n",
       "          ...,\n",
       "          [-1.0210,  0.7644,  2.3980,  ..., -0.1270,  0.1876,  1.1696],\n",
       "          [-1.0493,  0.3120,  2.7930,  ...,  1.6093, -0.2179, -0.6243],\n",
       "          [-0.9783, -0.5197,  2.1571,  ...,  2.7098, -0.1885, -0.0273]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 1.9027e-04,  2.1247e-03, -8.7043e-05,  ..., -4.4229e-03,\n",
       "            1.1869e-03, -2.5855e-03],\n",
       "          [ 1.3056e-01, -9.0435e-03, -8.4913e-02,  ...,  4.1479e-01,\n",
       "           -1.7949e-03, -7.1283e-02],\n",
       "          [ 2.0937e-01,  4.0742e-01, -2.8901e-01,  ...,  1.4842e+00,\n",
       "            1.6303e+00,  2.7607e-02],\n",
       "          ...,\n",
       "          [-5.6821e-01, -4.8627e-01, -1.1484e+00,  ...,  2.8467e-01,\n",
       "            8.5939e-02,  4.3079e-01],\n",
       "          [ 1.2021e+00,  2.0312e-01,  2.1438e-01,  ..., -5.4688e-01,\n",
       "           -3.2151e-01,  3.5475e-01],\n",
       "          [-7.2070e-03,  1.8544e-01, -1.2735e+00,  ...,  6.8115e-01,\n",
       "           -3.8852e-01, -8.0020e-01]],\n",
       "\n",
       "         [[ 2.1672e-03,  4.0386e-02,  1.7450e-03,  ...,  6.4803e-03,\n",
       "           -8.2528e-03,  2.5492e-04],\n",
       "          [ 1.4810e-01,  2.8458e-01, -2.8070e-01,  ...,  5.9185e-01,\n",
       "           -2.8568e-01,  1.6955e-01],\n",
       "          [ 5.8352e-02,  6.8843e-01, -6.8522e-01,  ...,  1.1565e+00,\n",
       "            2.2210e-01,  1.9490e-01],\n",
       "          ...,\n",
       "          [ 1.0562e-01,  6.1765e-01, -1.5489e-02,  ..., -1.0315e+00,\n",
       "            4.6140e-01, -2.1684e-01],\n",
       "          [ 1.4204e-01,  1.2352e+00, -1.1157e+00,  ..., -1.2459e-01,\n",
       "            6.1144e-02, -1.0476e+00],\n",
       "          [-3.5657e-01,  3.5064e-01,  5.0405e-02,  ...,  8.9146e-02,\n",
       "            2.3339e-01, -5.2024e-02]],\n",
       "\n",
       "         [[ 5.9416e-02, -4.6382e-03, -9.8495e-03,  ...,  2.1865e-03,\n",
       "           -9.8690e-03,  1.5520e-03],\n",
       "          [ 2.4442e+00, -7.9979e-02,  1.2167e-01,  ..., -2.5149e-01,\n",
       "           -1.1906e-01,  1.0666e-01],\n",
       "          [-3.1714e-01,  1.1351e-01, -4.2559e-01,  ..., -2.2280e-01,\n",
       "            1.1081e-01,  2.5451e-01],\n",
       "          ...,\n",
       "          [ 1.3585e+00,  7.6706e-02,  5.1423e-02,  ...,  1.3790e-01,\n",
       "            1.5109e-01,  8.2740e-02],\n",
       "          [-1.1520e-01,  1.4398e-01,  4.4490e-01,  ..., -4.7947e-01,\n",
       "           -2.8762e-01,  1.0242e-01],\n",
       "          [ 2.5892e+00, -3.2906e-01, -5.1770e-01,  ..., -5.8618e-02,\n",
       "           -4.9398e-01, -4.0515e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.0298e-03, -2.5108e-02, -2.4902e-02,  ..., -1.1612e-03,\n",
       "            7.0311e-03,  6.6672e-03],\n",
       "          [-4.9565e-01,  3.3408e-01, -1.1053e-01,  ..., -3.0998e-01,\n",
       "            6.4711e-02, -6.4242e-01],\n",
       "          [ 8.4919e-01,  5.6567e-01, -1.6240e-01,  ...,  3.8799e-01,\n",
       "           -2.3576e-01, -7.0758e-01],\n",
       "          ...,\n",
       "          [ 6.4066e-01, -1.1578e-01,  1.0823e+00,  ...,  1.4880e-01,\n",
       "           -1.9274e-01,  1.4891e-01],\n",
       "          [ 9.4248e-01,  5.4682e-01, -8.1600e-01,  ...,  6.1431e-01,\n",
       "           -7.8338e-01, -3.1949e-01],\n",
       "          [ 6.2190e-01, -8.1226e-02,  1.4363e-01,  ...,  6.0325e-02,\n",
       "           -5.6135e-01, -7.2253e-01]],\n",
       "\n",
       "         [[ 3.6600e-03,  1.3176e-02, -4.7591e-03,  ..., -7.6306e-03,\n",
       "           -1.4470e-02, -2.3708e-02],\n",
       "          [ 4.0373e-02, -3.7191e-01,  3.6215e-01,  ...,  4.1290e-02,\n",
       "            2.0602e-01, -7.0949e-02],\n",
       "          [ 4.6530e-03,  7.4880e-02,  3.2189e-01,  ...,  5.2548e-01,\n",
       "           -5.2171e-01,  6.3765e-01],\n",
       "          ...,\n",
       "          [ 5.1476e-01, -6.8246e-01, -2.4798e-01,  ...,  2.5834e-01,\n",
       "            5.8481e-01, -3.6870e-01],\n",
       "          [-9.0758e-01, -3.3312e-01,  3.3670e-01,  ...,  1.0406e+00,\n",
       "            2.9303e-01, -3.2289e-02],\n",
       "          [ 4.0811e-02,  7.5466e-02, -1.5653e-01,  ...,  1.2842e+00,\n",
       "           -2.4685e-01, -1.3180e-01]],\n",
       "\n",
       "         [[-1.1053e-02, -1.8317e-02, -1.0179e-02,  ..., -1.3334e-03,\n",
       "            3.1514e-02,  1.2467e-02],\n",
       "          [-3.3125e-01,  2.0240e-01,  2.7279e-01,  ..., -4.7650e-01,\n",
       "           -1.9204e-01, -4.4895e-01],\n",
       "          [ 5.0469e-01,  9.6079e-01,  4.9505e-01,  ...,  9.5665e-03,\n",
       "           -6.1726e-01, -7.2081e-01],\n",
       "          ...,\n",
       "          [-3.1174e-01,  1.5604e-01, -2.3800e-01,  ...,  1.9236e-01,\n",
       "            3.9125e-01,  1.4273e+00],\n",
       "          [-8.6931e-01,  3.1543e-01,  8.3990e-01,  ...,  1.4777e-01,\n",
       "           -9.4082e-01,  3.9164e-01],\n",
       "          [-1.0351e-01,  2.5454e-01,  3.5945e-02,  ...,  8.8141e-01,\n",
       "            4.3046e-02,  2.4226e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 2.4174e-03, -8.2511e-02, -4.3511e-02,  ..., -1.8696e-01,\n",
       "           -7.7052e-01,  5.3459e-01],\n",
       "          [ 2.3977e-03, -8.2483e-02, -4.4293e-02,  ..., -1.8719e-01,\n",
       "           -7.6830e-01,  5.3381e-01],\n",
       "          [ 6.8057e-01, -6.2855e-01, -8.6817e-01,  ..., -2.5585e-01,\n",
       "           -1.2031e-01, -8.1806e-01],\n",
       "          ...,\n",
       "          [-2.1529e-01, -1.0494e+00,  4.9512e-01,  ..., -8.7056e-01,\n",
       "            6.5902e-01, -4.6617e-01],\n",
       "          [ 6.7455e-01, -5.3796e-01,  4.5588e-01,  ...,  1.7818e-02,\n",
       "            2.4544e-02, -6.8032e-01],\n",
       "          [ 7.8751e-02,  8.5997e-04, -2.1930e-01,  ...,  1.1710e+00,\n",
       "           -1.7723e-01,  5.8787e-01]],\n",
       "\n",
       "         [[-3.3230e-01, -7.3815e-01,  3.5139e-01,  ...,  8.8969e-03,\n",
       "           -1.6375e-01,  2.2145e-01],\n",
       "          [-3.2991e-01, -7.3543e-01,  3.5001e-01,  ...,  9.8007e-03,\n",
       "           -1.6382e-01,  2.2106e-01],\n",
       "          [-4.5709e-01,  3.5261e-01,  3.8708e-01,  ...,  9.2605e-01,\n",
       "            4.2367e-01,  3.6003e-01],\n",
       "          ...,\n",
       "          [ 1.1784e-01,  4.4184e-01,  1.0746e+00,  ...,  3.3354e-02,\n",
       "           -3.0974e-01, -6.6988e-01],\n",
       "          [-3.7700e-03,  4.3509e-01,  1.1879e+00,  ...,  4.0649e-01,\n",
       "            3.7794e-01, -6.8166e-01],\n",
       "          [-4.9003e-01,  3.5862e-01,  1.4540e-01,  ..., -1.5900e-01,\n",
       "           -5.1224e-02,  3.5258e-01]],\n",
       "\n",
       "         [[ 1.7505e-02, -1.5207e-01,  1.1584e-01,  ..., -1.6930e-01,\n",
       "           -8.7182e-01,  2.2125e-01],\n",
       "          [ 1.7396e-02, -1.5234e-01,  1.1479e-01,  ..., -1.6960e-01,\n",
       "           -8.6902e-01,  2.2059e-01],\n",
       "          [-3.4312e-01, -5.9704e-01, -1.2304e-01,  ..., -3.8656e-01,\n",
       "            2.1926e-01,  1.2277e+00],\n",
       "          ...,\n",
       "          [ 3.1165e-01, -1.0912e+00,  1.5336e+00,  ...,  6.4469e-01,\n",
       "           -9.9205e-02,  6.7754e-01],\n",
       "          [ 1.6404e-01, -1.1207e+00,  7.1216e-01,  ..., -8.9191e-02,\n",
       "            2.6925e-01,  3.3010e-01],\n",
       "          [ 1.9689e-02,  8.5075e-01, -1.2445e-01,  ...,  2.5118e-01,\n",
       "            7.4469e-02, -1.7637e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.6844e-01, -4.1608e-01,  2.4513e-01,  ..., -2.2525e-01,\n",
       "            4.9072e-01, -7.8180e-04],\n",
       "          [ 1.6814e-01, -4.1404e-01,  2.4485e-01,  ..., -2.2527e-01,\n",
       "            4.8945e-01, -8.3230e-05],\n",
       "          [-4.7641e-02,  9.0664e-01, -5.7211e-01,  ...,  1.4391e-01,\n",
       "           -6.4174e-01, -2.7547e-01],\n",
       "          ...,\n",
       "          [ 4.4855e-01, -2.3723e-01, -3.3610e-01,  ...,  3.2386e-01,\n",
       "           -8.0357e-01,  6.4709e-01],\n",
       "          [-2.9242e-01,  7.2743e-01,  3.2356e-02,  ...,  2.9870e-01,\n",
       "           -5.6840e-01, -3.8319e-01],\n",
       "          [-5.9918e-01, -6.3143e-01, -2.1049e-01,  ..., -2.3573e-01,\n",
       "           -3.2228e-01, -3.8273e-01]],\n",
       "\n",
       "         [[-4.4022e-01,  8.0155e-01,  7.2584e-01,  ..., -4.1746e-02,\n",
       "            3.0749e-01, -6.3927e-02],\n",
       "          [-4.3969e-01,  8.0259e-01,  7.2570e-01,  ..., -3.9992e-02,\n",
       "            3.1123e-01, -6.3762e-02],\n",
       "          [-2.9021e-01,  7.1553e-02, -1.0443e+00,  ..., -3.7881e-01,\n",
       "           -1.2718e+00,  5.1947e-01],\n",
       "          ...,\n",
       "          [-1.1937e-01, -2.2197e-01, -5.6668e-01,  ..., -8.2761e-01,\n",
       "           -1.1085e+00,  2.7105e-03],\n",
       "          [-1.3523e-01, -5.2274e-01, -1.2438e+00,  ..., -3.0537e-01,\n",
       "           -8.6795e-01,  6.5525e-01],\n",
       "          [ 1.9876e-01, -1.2017e+00, -5.9646e-01,  ..., -5.5454e-02,\n",
       "           -1.4149e+00,  4.5088e-01]],\n",
       "\n",
       "         [[-2.5495e-01,  1.5827e-01, -1.9729e-02,  ...,  1.9713e-01,\n",
       "           -6.9129e-02, -3.6031e-02],\n",
       "          [-2.5504e-01,  1.5619e-01, -1.9397e-02,  ...,  1.9576e-01,\n",
       "           -7.1351e-02, -3.7487e-02],\n",
       "          [ 6.8644e-03,  7.9100e-01, -3.7141e-01,  ..., -1.3858e+00,\n",
       "            2.2109e-01, -4.7986e-01],\n",
       "          ...,\n",
       "          [ 9.5741e-01,  1.6286e-01, -1.3194e+00,  ..., -4.9339e-01,\n",
       "            5.4612e-01, -5.1451e-02],\n",
       "          [-1.1089e-01,  2.4996e-01, -1.4107e+00,  ..., -1.2976e+00,\n",
       "            9.0259e-01, -1.4442e-01],\n",
       "          [ 2.4924e-01,  3.3238e-01,  1.0685e+00,  ...,  2.5387e-01,\n",
       "            2.3054e-01, -4.3730e-02]]]], grad_fn=<CloneBackward0>), tensor([[[[-8.3322e-03, -5.5974e-02,  2.5530e-02,  ..., -2.8928e-02,\n",
       "           -3.6649e-02, -4.6120e-03],\n",
       "          [-8.4496e-03, -5.5667e-02,  2.5306e-02,  ..., -2.8678e-02,\n",
       "           -3.6663e-02, -4.6015e-03],\n",
       "          [-5.7135e-01,  4.5500e-01,  4.4212e-01,  ...,  8.9233e-01,\n",
       "           -4.0585e-02,  5.1944e-02],\n",
       "          ...,\n",
       "          [-4.0021e-01, -4.5071e-02, -3.4130e-01,  ..., -3.0830e-02,\n",
       "            1.0927e-01, -1.8948e-01],\n",
       "          [ 3.9336e-03,  1.2709e-01,  2.5892e-01,  ...,  4.5354e-01,\n",
       "            3.6914e-01,  1.7625e-02],\n",
       "          [ 1.2606e-01,  2.7144e-01,  2.6149e-01,  ..., -8.8231e-02,\n",
       "           -1.0085e-02,  3.5029e-03]],\n",
       "\n",
       "         [[-2.0729e-02, -4.5775e-03, -5.5056e-02,  ..., -1.9026e-02,\n",
       "           -4.8928e-02,  1.7843e-02],\n",
       "          [-2.0760e-02, -4.5882e-03, -5.5128e-02,  ..., -1.9273e-02,\n",
       "           -4.9144e-02,  1.7758e-02],\n",
       "          [-4.5696e-01,  2.9034e-01,  3.5680e-01,  ..., -1.5774e+00,\n",
       "            1.4762e+00, -2.6476e-01],\n",
       "          ...,\n",
       "          [ 2.8652e-01,  2.7788e-01,  1.4897e+00,  ..., -2.7033e-01,\n",
       "            1.0398e+00,  3.4820e-01],\n",
       "          [ 2.6226e-01,  1.0551e-01,  3.0269e-01,  ..., -6.9581e-02,\n",
       "            1.2785e+00,  1.7099e-01],\n",
       "          [-1.5993e-01,  4.3304e-01,  1.4817e+00,  ...,  4.7440e-02,\n",
       "            1.6542e-01, -1.9100e-01]],\n",
       "\n",
       "         [[-7.4302e-03, -3.3661e-03, -2.1841e-02,  ...,  1.0331e-02,\n",
       "           -2.0323e-02, -7.1281e-02],\n",
       "          [-7.4719e-03, -3.6493e-03, -2.1203e-02,  ...,  1.0268e-02,\n",
       "           -2.0363e-02, -7.1006e-02],\n",
       "          [-3.6543e-01,  2.6301e-01, -6.5443e-01,  ...,  1.2176e+00,\n",
       "           -1.7331e-01, -1.0469e+00],\n",
       "          ...,\n",
       "          [ 7.1697e-01, -7.7936e-01, -7.1116e-03,  ...,  2.5583e-01,\n",
       "           -5.6458e-01, -1.4701e-01],\n",
       "          [ 3.7393e-01,  1.2636e-01, -2.1607e-01,  ...,  7.9848e-01,\n",
       "           -4.4844e-01, -7.1034e-01],\n",
       "          [-6.3137e-01,  5.9086e-01,  8.2777e-01,  ...,  3.1108e-01,\n",
       "            2.6967e-01,  3.4901e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.8532e-03, -3.2418e-02,  5.3947e-04,  ...,  3.8540e-02,\n",
       "            9.3835e-04,  6.1057e-02],\n",
       "          [-7.2075e-03, -3.2439e-02,  8.3574e-04,  ...,  3.8767e-02,\n",
       "            8.7483e-04,  6.0751e-02],\n",
       "          [ 2.0761e-01, -1.9800e-01, -3.1497e-01,  ..., -1.6262e-01,\n",
       "           -5.0250e-01,  1.1645e-01],\n",
       "          ...,\n",
       "          [-8.7942e-01,  8.8077e-01, -6.6470e-01,  ..., -3.8719e-01,\n",
       "           -3.4852e-02, -1.6742e-02],\n",
       "          [-5.1992e-01,  1.4715e-01, -7.5732e-01,  ..., -5.6550e-01,\n",
       "            7.8311e-02, -1.0689e-01],\n",
       "          [-3.0500e-01, -1.2075e-01,  4.5162e-02,  ...,  2.2014e-01,\n",
       "            3.0832e-01,  1.9375e-01]],\n",
       "\n",
       "         [[-2.9271e-01, -7.0188e-01, -4.8695e-01,  ...,  7.5284e-02,\n",
       "            1.8582e-01, -2.2383e-01],\n",
       "          [-2.9250e-01, -7.0298e-01, -4.8757e-01,  ...,  7.4922e-02,\n",
       "            1.8624e-01, -2.2435e-01],\n",
       "          [ 5.7020e-01,  1.1258e-01,  2.7129e-01,  ..., -7.1748e-01,\n",
       "            4.6707e-02, -6.9499e-01],\n",
       "          ...,\n",
       "          [-3.1814e-01, -2.2738e-01,  5.2254e-02,  ..., -1.9272e-01,\n",
       "            8.3071e-01,  7.3913e-02],\n",
       "          [ 7.5970e-02, -2.1309e-01,  9.8675e-02,  ..., -1.1171e-01,\n",
       "           -3.8119e-02, -2.4957e-01],\n",
       "          [ 2.6938e-01,  4.3105e-02, -1.9796e-01,  ..., -5.6233e-02,\n",
       "            4.5425e-02, -2.0028e-01]],\n",
       "\n",
       "         [[ 2.3620e-02, -6.0072e-02, -4.5616e-02,  ...,  3.1947e-03,\n",
       "            1.8058e-03,  3.9058e-02],\n",
       "          [ 2.3844e-02, -6.0035e-02, -4.5595e-02,  ...,  2.9852e-03,\n",
       "            1.6778e-03,  3.9231e-02],\n",
       "          [ 1.3352e-03, -4.9197e-01, -6.6620e-01,  ...,  3.6081e-01,\n",
       "           -2.2249e-02, -3.3805e-01],\n",
       "          ...,\n",
       "          [ 1.7479e-01, -2.3994e-02,  4.8706e-01,  ...,  6.2657e-01,\n",
       "           -5.8765e-01,  1.4925e-01],\n",
       "          [-1.2469e-01,  1.9395e-02,  3.2576e-01,  ...,  4.6825e-01,\n",
       "            4.1539e-02,  4.9423e-02],\n",
       "          [-8.2102e-01,  1.6193e+00,  3.2001e-01,  ...,  4.9149e-01,\n",
       "           -2.4697e-01, -7.9310e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[ 0.1674,  0.0820, -0.1207,  ...,  0.5514, -0.0694,  0.0544],\n",
       "          [ 0.1143, -0.0115,  1.0866,  ...,  0.6444, -0.3853, -0.1870],\n",
       "          [-0.2852, -0.3292,  1.2058,  ..., -1.3066,  0.0225, -0.0789],\n",
       "          ...,\n",
       "          [-0.8304, -1.0641,  2.6560,  ..., -1.0261, -0.7822, -1.5225],\n",
       "          [-1.2976, -1.4122,  1.0758,  ..., -1.1285,  0.9019, -0.2014],\n",
       "          [-0.8921, -1.6561,  0.8835,  ..., -0.9483,  0.4571, -0.6595]],\n",
       "\n",
       "         [[-0.2177, -0.2469, -0.0852,  ...,  0.8369,  1.7350, -0.7731],\n",
       "          [-1.0536, -1.2564, -2.3747,  ..., -0.0151, -0.6027,  0.2250],\n",
       "          [-1.9700,  0.6294, -4.6227,  ..., -4.1897,  0.6762,  1.1418],\n",
       "          ...,\n",
       "          [-4.1395,  0.0789,  0.2016,  ..., -1.4912, -1.2279,  2.1694],\n",
       "          [-6.0923,  1.2478,  1.3147,  ..., -5.2947, -1.4274, -0.5448],\n",
       "          [-4.7855, -0.0956, -1.9210,  ...,  0.3883, -1.5241, -1.0927]],\n",
       "\n",
       "         [[ 0.1017,  0.1836, -0.4550,  ..., -0.3270,  0.1336, -0.3103],\n",
       "          [ 0.4422,  0.3236,  0.6234,  ...,  0.9607,  0.8692,  0.5463],\n",
       "          [-1.4962, -0.7204,  2.7447,  ...,  1.1711,  1.8739,  0.2453],\n",
       "          ...,\n",
       "          [ 1.3369, -0.9086,  0.3101,  ...,  0.6319,  1.8581,  1.3427],\n",
       "          [-0.4248, -0.1901,  0.6393,  ...,  1.4972,  1.0197,  1.3333],\n",
       "          [-0.0330,  1.0313,  0.5238,  ...,  0.3735,  0.9292,  0.1171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.1798,  0.1032,  0.1713,  ...,  0.2017,  0.2631,  0.0754],\n",
       "          [-1.5986, -0.6902,  0.2780,  ...,  0.2389, -0.9275,  0.7324],\n",
       "          [-1.8479, -1.2461,  1.1203,  ..., -0.3743, -0.3359,  0.5227],\n",
       "          ...,\n",
       "          [-2.4671, -1.7703,  0.5990,  ..., -0.3272,  0.3411,  1.4760],\n",
       "          [-3.0460,  0.2537,  1.4011,  ..., -0.2255, -1.7996,  0.3021],\n",
       "          [-3.5464, -0.8105,  1.1874,  ..., -0.0220, -1.0778,  0.8748]],\n",
       "\n",
       "         [[ 0.7519,  0.3095,  0.0931,  ...,  0.1121,  1.1889,  0.0713],\n",
       "          [-1.0692, -1.6622, -0.3881,  ..., -0.5173, -1.4207, -0.7002],\n",
       "          [-2.1293, -2.0009, -0.3129,  ..., -1.2853, -2.4446,  0.5600],\n",
       "          ...,\n",
       "          [ 0.4389, -1.2447, -0.3249,  ..., -1.3378, -1.7377, -0.1980],\n",
       "          [-1.3934, -2.4414, -1.5631,  ..., -0.1698, -2.8304, -0.0125],\n",
       "          [-1.3878, -2.8090, -1.0600,  ..., -1.9690, -2.8326, -0.4475]],\n",
       "\n",
       "         [[ 0.1933,  0.2351,  0.1024,  ...,  0.0200, -0.0489,  0.0562],\n",
       "          [-1.0234, -0.0771, -0.1962,  ...,  0.7027, -0.4881,  0.4051],\n",
       "          [-0.2390, -0.2937, -0.5674,  ..., -0.3674, -0.4153, -1.6870],\n",
       "          ...,\n",
       "          [-0.8623,  0.5395, -0.7729,  ...,  1.5540, -0.9349, -0.0186],\n",
       "          [-0.2450,  1.0240, -0.3516,  ...,  1.4116, -0.4521, -0.2363],\n",
       "          [ 0.2326,  0.1125, -0.3168,  ...,  0.8816, -0.3608,  0.4904]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 6.0951e-03,  2.9630e-02,  4.0890e-03,  ..., -2.1673e-02,\n",
       "            1.2913e-02,  2.9604e-02],\n",
       "          [ 5.8815e-01,  6.3218e-01, -1.2039e-01,  ...,  5.9348e-02,\n",
       "            1.2686e-02, -3.2031e-03],\n",
       "          [-3.1782e-01, -9.4526e-02,  5.4021e-01,  ..., -1.8285e-01,\n",
       "           -1.7927e-02, -3.0657e-02],\n",
       "          ...,\n",
       "          [-2.4242e-01, -1.1468e+00,  2.2393e-01,  ...,  1.1939e+00,\n",
       "            5.0023e-01,  8.7072e-01],\n",
       "          [ 1.2280e-01,  1.9717e-01,  7.3834e-01,  ...,  1.8697e-01,\n",
       "           -2.7869e-01, -3.0099e-01],\n",
       "          [ 3.9526e-01, -1.4175e-01, -8.1775e-01,  ..., -5.3254e-01,\n",
       "           -1.0902e+00, -7.9740e-01]],\n",
       "\n",
       "         [[ 9.8488e-02,  7.3083e-02,  1.1555e-02,  ..., -4.8680e-02,\n",
       "           -5.2769e-02, -1.6326e-02],\n",
       "          [ 5.1933e-02,  2.6499e-01, -1.2768e-01,  ...,  7.2261e-02,\n",
       "           -1.8574e-01, -2.4060e-01],\n",
       "          [ 4.3069e-01, -1.0263e-01, -6.3404e-02,  ...,  3.6196e-01,\n",
       "           -2.8391e-01,  2.8648e-01],\n",
       "          ...,\n",
       "          [ 3.5413e-01,  6.5003e-02,  2.9740e-01,  ..., -1.3967e-01,\n",
       "            2.6844e-01, -4.3882e-01],\n",
       "          [ 3.1893e-01,  6.2733e-02, -2.8911e-01,  ..., -1.9420e-01,\n",
       "           -5.5441e-01,  2.3835e-01],\n",
       "          [-5.1733e-02,  1.5542e-01,  6.9337e-02,  ..., -5.9908e-01,\n",
       "           -2.9331e-01,  5.6511e-02]],\n",
       "\n",
       "         [[-1.9492e-03,  6.3412e-03,  3.0474e-02,  ...,  1.4643e-02,\n",
       "           -8.0746e-03, -1.1779e-02],\n",
       "          [ 5.7563e-03,  5.2564e-01, -4.4400e-01,  ..., -2.0692e-01,\n",
       "           -7.2511e-01,  1.0936e+00],\n",
       "          [-5.6259e-01, -4.3561e-01, -1.1392e+00,  ..., -1.1824e+00,\n",
       "            1.0844e-01, -6.2286e-01],\n",
       "          ...,\n",
       "          [ 4.0730e-01, -1.0411e+00, -1.1024e-01,  ..., -9.6759e-02,\n",
       "            4.2928e-01,  4.9996e-01],\n",
       "          [-4.8409e-01,  4.2387e-02, -1.0883e+00,  ...,  1.5751e+00,\n",
       "           -4.5749e-01, -7.0014e-02],\n",
       "          [ 3.5962e-03,  3.5974e-01, -4.0800e-01,  ...,  1.2507e-01,\n",
       "           -1.1558e+00,  1.1773e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.1681e-03,  1.3670e-02,  1.9070e-03,  ...,  2.6148e-03,\n",
       "           -4.2542e-03, -1.7857e-02],\n",
       "          [-3.2417e-01,  3.1183e-01, -2.3083e-01,  ..., -1.6062e-01,\n",
       "           -1.4605e-01, -2.7694e-01],\n",
       "          [-7.9800e-01, -6.7625e-01,  1.3771e+00,  ...,  1.2079e+00,\n",
       "            1.2754e-01,  1.9232e+00],\n",
       "          ...,\n",
       "          [-5.3527e-01, -2.8503e-01, -7.3643e-01,  ..., -7.3316e-01,\n",
       "            3.4919e-01,  9.7402e-01],\n",
       "          [ 1.1453e-01, -4.7045e-01, -9.0524e-01,  ..., -9.5172e-01,\n",
       "           -1.9346e+00,  9.0846e-01],\n",
       "          [-4.3750e-01,  1.2768e-01,  9.3935e-01,  ...,  1.3277e-01,\n",
       "           -7.0662e-01, -1.4596e-01]],\n",
       "\n",
       "         [[-6.5394e-03,  6.9354e-03, -2.2051e-02,  ..., -4.5941e-03,\n",
       "           -2.4754e-03, -7.9595e-03],\n",
       "          [ 2.0903e-01, -5.4432e-01, -1.1833e-01,  ..., -1.7858e-02,\n",
       "           -5.4697e-01, -1.6687e-02],\n",
       "          [ 1.5583e+00, -3.6638e-01, -1.0821e+00,  ...,  1.9404e-01,\n",
       "           -1.0370e+00, -9.5880e-01],\n",
       "          ...,\n",
       "          [-5.7659e-01, -2.4690e-01, -1.7752e-01,  ...,  2.3909e-01,\n",
       "           -1.2936e-01, -3.4103e-01],\n",
       "          [-9.5073e-02, -3.8216e-02, -5.5777e-01,  ...,  2.2794e-01,\n",
       "            6.3355e-01,  6.0794e-01],\n",
       "          [ 3.1340e-01, -5.4964e-01, -8.8867e-01,  ..., -3.2956e-01,\n",
       "            6.3850e-01,  7.5656e-01]],\n",
       "\n",
       "         [[ 1.3984e-03, -9.3998e-03,  9.8554e-03,  ..., -8.4321e-03,\n",
       "           -1.1104e-02, -4.6976e-04],\n",
       "          [ 7.4881e-01,  6.0070e-01, -6.9632e-01,  ...,  6.1901e-01,\n",
       "            1.9091e-01,  3.5815e-02],\n",
       "          [ 1.3068e+00,  2.2125e-01, -5.4739e-01,  ...,  6.0132e-01,\n",
       "            4.0542e-01,  2.1974e-01],\n",
       "          ...,\n",
       "          [-4.0815e-01, -1.5174e+00, -1.0827e-01,  ..., -8.7138e-01,\n",
       "           -1.9234e+00, -1.7285e-01],\n",
       "          [-3.3795e-01, -9.7974e-01,  8.6758e-01,  ...,  1.2447e-01,\n",
       "            7.8055e-01, -1.0701e+00],\n",
       "          [ 5.8323e-01, -8.1084e-02,  1.3818e+00,  ..., -1.3283e-01,\n",
       "            9.3040e-01, -2.8443e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.7915e-01,  5.5491e-01, -1.3393e+00,  ...,  3.8223e-01,\n",
       "           -2.0815e-01, -1.2642e+00],\n",
       "          [ 1.7884e-01,  5.5308e-01, -1.3387e+00,  ...,  3.8127e-01,\n",
       "           -2.0772e-01, -1.2659e+00],\n",
       "          [ 5.7020e-01,  7.8985e-01,  1.2599e-01,  ..., -9.3912e-01,\n",
       "           -6.4803e-01,  9.4659e-01],\n",
       "          ...,\n",
       "          [ 2.7520e-01, -9.5049e-01,  1.1523e+00,  ...,  2.7790e-02,\n",
       "            1.8762e-01,  1.9470e+00],\n",
       "          [ 2.3742e-01, -7.3783e-01,  1.0102e+00,  ..., -8.2658e-01,\n",
       "           -6.7317e-01,  2.0084e+00],\n",
       "          [ 1.3885e+00,  2.9105e-01,  9.2682e-01,  ...,  4.0458e-01,\n",
       "            8.9027e-01,  5.0660e-01]],\n",
       "\n",
       "         [[ 1.9222e-01, -9.7244e-01, -3.5144e-01,  ...,  1.2797e-01,\n",
       "           -2.0617e-01, -2.4193e-02],\n",
       "          [ 1.9091e-01, -9.6845e-01, -3.5069e-01,  ...,  1.2806e-01,\n",
       "           -2.0594e-01, -2.4938e-02],\n",
       "          [ 1.2736e-01,  2.2295e-01, -5.1120e-01,  ..., -1.7996e-02,\n",
       "            1.9826e-01,  1.9566e-01],\n",
       "          ...,\n",
       "          [ 9.4388e-01,  6.7231e-01,  5.1409e-01,  ..., -7.9527e-01,\n",
       "            3.5796e-01, -2.5472e-01],\n",
       "          [ 8.5149e-01,  8.0521e-01, -2.2142e-01,  ...,  4.8931e-01,\n",
       "            8.3176e-02, -2.5986e-01],\n",
       "          [ 3.7960e-01, -5.3810e-01,  7.1983e-01,  ..., -1.1687e+00,\n",
       "           -8.4998e-01,  3.8319e-01]],\n",
       "\n",
       "         [[-1.0139e-01,  2.3140e-01,  8.0571e-03,  ...,  8.7668e-04,\n",
       "            4.0278e-01, -1.0573e-01],\n",
       "          [-1.0001e-01,  2.2995e-01,  7.9848e-03,  ...,  1.0512e-03,\n",
       "            4.0000e-01, -1.0444e-01],\n",
       "          [-2.3468e-01, -6.8420e-01, -1.1988e-01,  ..., -5.0581e-01,\n",
       "           -8.9472e-01, -3.1987e-01],\n",
       "          ...,\n",
       "          [-5.8544e-03, -6.9818e-01,  7.4422e-01,  ...,  5.1510e-01,\n",
       "           -6.7010e-01,  6.1672e-01],\n",
       "          [ 2.0659e-02, -7.4266e-01,  6.5936e-01,  ..., -5.6506e-02,\n",
       "           -6.2337e-01,  4.7418e-01],\n",
       "          [-4.9115e-01, -1.0805e+00, -4.1251e-01,  ..., -1.9574e-01,\n",
       "           -6.6207e-01, -9.7068e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.0101e-01,  1.2282e-01,  5.2267e-02,  ...,  2.9761e-01,\n",
       "           -7.2234e-01, -5.2253e-01],\n",
       "          [-2.9866e-01,  1.2250e-01,  5.0365e-02,  ...,  2.9668e-01,\n",
       "           -7.1897e-01, -5.2172e-01],\n",
       "          [-1.0166e-01, -1.4922e+00,  3.2516e-01,  ...,  6.5726e-02,\n",
       "           -1.4267e-01, -4.2036e-01],\n",
       "          ...,\n",
       "          [ 3.8985e-01, -7.5919e-01,  5.7621e-01,  ..., -9.0043e-01,\n",
       "            4.6685e-01,  6.0526e-02],\n",
       "          [ 7.5840e-01, -1.0291e+00,  1.4135e+00,  ..., -2.4027e-03,\n",
       "            2.5946e-01, -4.2544e-01],\n",
       "          [-4.4724e-02,  2.7441e-01, -2.5815e-01,  ..., -2.9097e-01,\n",
       "           -4.2978e-01,  5.1650e-03]],\n",
       "\n",
       "         [[-3.9435e-02, -2.2177e-01, -9.9726e-02,  ...,  5.5809e-02,\n",
       "           -6.1189e-01, -6.7378e-01],\n",
       "          [-3.9254e-02, -2.2027e-01, -9.9287e-02,  ...,  5.4872e-02,\n",
       "           -6.1012e-01, -6.7178e-01],\n",
       "          [-5.1824e-01,  1.7938e-01,  4.5990e-03,  ...,  5.1079e-02,\n",
       "           -7.7540e-01,  5.3047e-01],\n",
       "          ...,\n",
       "          [-4.2477e-01,  6.5016e-01,  2.6194e-01,  ...,  2.2982e-01,\n",
       "           -2.9368e-01,  4.0858e-01],\n",
       "          [ 6.8362e-01,  7.2300e-01, -2.0897e-01,  ...,  5.5108e-02,\n",
       "           -5.3887e-01, -7.2787e-02],\n",
       "          [ 5.0773e-01, -7.7314e-01,  1.0502e+00,  ..., -1.7671e-01,\n",
       "            3.0910e-02,  6.9565e-01]],\n",
       "\n",
       "         [[-6.3542e-01,  6.3438e-02,  2.8616e-01,  ..., -7.5419e-02,\n",
       "            8.0068e-02,  3.2588e-01],\n",
       "          [-6.3322e-01,  6.3418e-02,  2.8496e-01,  ..., -7.6725e-02,\n",
       "            8.1063e-02,  3.2510e-01],\n",
       "          [ 7.3387e-01,  1.0453e+00, -6.6658e-01,  ...,  1.0153e-01,\n",
       "           -4.3637e-01, -1.0105e+00],\n",
       "          ...,\n",
       "          [ 2.4046e-01, -5.2835e-01, -1.6999e-01,  ...,  3.5400e-01,\n",
       "           -8.1217e-02,  2.6875e-01],\n",
       "          [ 4.5323e-01,  7.9928e-01, -2.0011e-01,  ..., -7.9602e-02,\n",
       "            3.5385e-01, -3.2563e-02],\n",
       "          [-1.8409e-01,  5.1880e-01,  7.5416e-01,  ...,  1.3951e-01,\n",
       "           -7.7168e-01, -2.1485e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 4.9188e-03,  1.3422e-02, -1.8968e-02,  ...,  1.6781e-02,\n",
       "           -6.4267e-03, -6.8565e-02],\n",
       "          [ 4.5606e-03,  1.3433e-02, -1.9042e-02,  ...,  1.6471e-02,\n",
       "           -6.3625e-03, -6.8190e-02],\n",
       "          [ 8.6388e-02, -3.2406e-01, -8.3322e-01,  ..., -3.0267e-03,\n",
       "            4.2318e-02, -3.3159e-01],\n",
       "          ...,\n",
       "          [ 3.8739e-01,  8.7372e-01,  1.5039e-01,  ...,  8.5342e-02,\n",
       "            2.8138e-01,  2.7003e-01],\n",
       "          [ 1.4842e-01, -6.5325e-02, -9.5588e-02,  ...,  9.8102e-02,\n",
       "            6.4443e-01, -1.4367e-02],\n",
       "          [-1.1211e-02, -5.9104e-02, -2.2876e-01,  ...,  3.8571e-02,\n",
       "            2.7251e-01,  1.4498e+00]],\n",
       "\n",
       "         [[-6.3903e-03,  3.5080e-03, -1.9634e-02,  ..., -2.3243e-02,\n",
       "           -1.5821e-02, -1.6647e-02],\n",
       "          [-6.1860e-03,  3.7442e-03, -1.9801e-02,  ..., -2.3351e-02,\n",
       "           -1.6587e-02, -1.6736e-02],\n",
       "          [-7.4813e-01,  5.9453e-01, -8.0119e-01,  ..., -5.6570e-02,\n",
       "            1.9879e-01,  4.9484e-01],\n",
       "          ...,\n",
       "          [-4.7840e-01,  1.6124e+00,  1.1808e-01,  ...,  1.2424e+00,\n",
       "            9.9951e-01,  7.9196e-01],\n",
       "          [-3.9162e-01,  1.0348e+00, -1.1413e+00,  ..., -7.5940e-01,\n",
       "            2.7208e-01,  4.5679e-01],\n",
       "          [ 8.4862e-01,  5.4356e-02, -6.5761e-03,  ...,  8.8595e-01,\n",
       "           -4.1041e-01, -6.3872e-01]],\n",
       "\n",
       "         [[-5.5239e-02, -2.9024e-02, -1.6022e-02,  ...,  7.9021e-03,\n",
       "            1.9034e-02, -4.7284e-03],\n",
       "          [-5.5374e-02, -2.9083e-02, -1.6050e-02,  ...,  7.5075e-03,\n",
       "            1.9157e-02, -4.1893e-03],\n",
       "          [ 1.7120e-02,  3.8500e-01, -2.5184e-01,  ..., -1.1276e-01,\n",
       "            3.5934e-01,  1.1929e-01],\n",
       "          ...,\n",
       "          [ 3.9781e-01, -7.8731e-01, -1.0886e-01,  ..., -5.2791e-01,\n",
       "            7.8103e-03,  2.0872e-01],\n",
       "          [ 3.7223e-02, -7.1989e-01,  3.3033e-01,  ...,  2.6361e-01,\n",
       "            2.1297e-01,  4.9771e-02],\n",
       "          [-1.1337e-01,  5.9400e-01,  6.8390e-01,  ...,  2.1938e-01,\n",
       "           -1.8141e-01, -1.9390e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.4338e-02, -1.3656e-01,  1.2316e-01,  ...,  8.5839e-02,\n",
       "            4.8730e-03,  9.6606e-03],\n",
       "          [ 3.4629e-02, -1.3649e-01,  1.2359e-01,  ...,  8.6031e-02,\n",
       "            5.1839e-03,  9.5102e-03],\n",
       "          [-1.4118e+00, -2.7233e-01, -1.5154e+00,  ...,  5.0286e-01,\n",
       "            1.8311e+00, -2.1325e-02],\n",
       "          ...,\n",
       "          [ 4.9872e-01, -5.5486e-01, -5.5314e-01,  ..., -1.1048e+00,\n",
       "            6.6992e-01,  4.4761e-01],\n",
       "          [-3.2283e-01, -2.9303e-01, -1.1715e+00,  ...,  4.0665e-01,\n",
       "            1.0229e+00,  2.5726e-01],\n",
       "          [ 2.3801e-01, -3.0738e-01, -7.7766e-02,  ..., -1.0706e-01,\n",
       "           -2.3210e-01,  3.5498e-01]],\n",
       "\n",
       "         [[ 4.0621e-03,  2.5528e-02,  2.8463e-02,  ..., -1.8042e-03,\n",
       "           -1.0964e-02,  1.5392e-02],\n",
       "          [ 4.1737e-03,  2.5431e-02,  2.8589e-02,  ..., -1.5974e-03,\n",
       "           -1.0935e-02,  1.5601e-02],\n",
       "          [ 2.1599e-01,  4.3886e-02,  7.3067e-01,  ...,  7.1122e-01,\n",
       "           -1.7111e-01,  1.1477e-01],\n",
       "          ...,\n",
       "          [-3.8749e-01, -3.6670e-01,  1.5571e-01,  ...,  2.4881e-01,\n",
       "           -1.1899e-01,  4.1211e-01],\n",
       "          [ 2.1916e-01,  2.1543e-01, -1.0949e-01,  ...,  2.7099e-02,\n",
       "            1.5508e-01,  2.7088e-01],\n",
       "          [ 3.5367e-01, -5.9978e-02,  5.1036e-01,  ..., -2.3092e-01,\n",
       "           -1.7921e-01,  2.6667e-01]],\n",
       "\n",
       "         [[ 2.0384e-01,  1.3524e-01,  3.6783e-01,  ..., -3.9251e-01,\n",
       "           -3.0047e-01, -1.4634e-01],\n",
       "          [ 2.0419e-01,  1.3544e-01,  3.6823e-01,  ..., -3.9250e-01,\n",
       "           -3.0051e-01, -1.4635e-01],\n",
       "          [ 3.9009e-01,  2.8418e-01,  2.0640e-01,  ..., -1.6890e-01,\n",
       "           -1.8715e-01, -7.6044e-03],\n",
       "          ...,\n",
       "          [ 5.1363e-01, -1.8215e-01,  8.6481e-01,  ...,  1.1813e+00,\n",
       "            2.6673e-01,  1.0486e-01],\n",
       "          [ 2.6127e-01,  2.4172e-01,  5.5447e-01,  ..., -5.4790e-01,\n",
       "            3.2992e-01,  1.1192e-01],\n",
       "          [ 6.3123e-01, -5.6998e-02, -1.5084e-01,  ..., -4.8001e-02,\n",
       "            1.5740e-01,  3.0448e-02]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-1.6727e+00,  4.0996e-01, -1.0950e-01,  ..., -3.3482e-01,\n",
       "           -2.7616e-01,  9.7110e-02],\n",
       "          [ 3.2699e+00,  4.0072e-01,  3.6622e-01,  ..., -1.0959e+00,\n",
       "            7.3393e-02, -3.7545e-01],\n",
       "          [ 3.7165e+00, -8.8124e-01, -1.6029e+00,  ...,  5.9520e-01,\n",
       "            6.0110e-01, -4.6920e-01],\n",
       "          ...,\n",
       "          [ 4.1660e+00, -4.6754e-01,  1.1023e+00,  ..., -1.2228e+00,\n",
       "            3.7134e-01, -1.1061e+00],\n",
       "          [ 3.2750e+00, -3.2566e-01,  1.3515e+00,  ..., -2.9575e-01,\n",
       "           -7.6589e-02, -1.3532e+00],\n",
       "          [ 4.4228e+00, -1.2119e+00,  1.6488e+00,  ..., -6.0235e-01,\n",
       "            4.4022e-02,  4.4458e-01]],\n",
       "\n",
       "         [[-5.3168e-01, -2.3825e-01, -6.4357e-01,  ..., -9.7282e-02,\n",
       "           -7.4568e-01, -4.5047e-01],\n",
       "          [ 3.6470e-01, -1.1846e+00,  8.6525e-01,  ..., -5.3065e-01,\n",
       "           -9.1779e-01,  1.0067e+00],\n",
       "          [-4.5351e-01,  2.1792e-01,  1.7113e+00,  ..., -2.3643e-01,\n",
       "            2.9951e-02,  1.2215e+00],\n",
       "          ...,\n",
       "          [-1.6734e-01,  1.9622e-01,  1.3384e+00,  ...,  5.6961e-01,\n",
       "           -4.4186e-01,  8.1550e-01],\n",
       "          [-8.4883e-01,  1.1405e+00,  3.9514e+00,  ..., -8.9962e-01,\n",
       "            8.8353e-01,  6.8582e-01],\n",
       "          [-1.3210e+00,  3.6640e-01,  1.0307e+00,  ..., -1.2999e+00,\n",
       "           -1.3513e+00,  1.2238e+00]],\n",
       "\n",
       "         [[-2.0858e-01, -1.2702e+00,  1.9573e-01,  ..., -3.9903e-01,\n",
       "           -1.8976e-01, -3.9894e-01],\n",
       "          [-6.2094e-01,  2.3047e+00,  1.9430e-01,  ..., -6.6607e-01,\n",
       "            3.4605e-01,  1.4502e+00],\n",
       "          [ 8.4173e-01,  2.3703e+00, -1.4100e+00,  ..., -1.1902e+00,\n",
       "            9.1521e-01,  1.0841e+00],\n",
       "          ...,\n",
       "          [ 1.6060e-01,  4.3514e+00, -5.3491e-01,  ..., -2.2029e+00,\n",
       "            1.3793e+00,  9.2671e-01],\n",
       "          [-1.0584e-02,  4.0503e+00, -1.0501e+00,  ..., -1.8136e+00,\n",
       "            9.8153e-01,  2.3668e+00],\n",
       "          [-2.5186e-01,  3.8149e+00,  4.2011e-02,  ..., -1.2926e+00,\n",
       "            2.5279e+00,  2.7940e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.5133e-01, -1.0222e+00,  6.1902e-01,  ..., -1.6796e+00,\n",
       "           -1.5670e-01, -6.9791e-01],\n",
       "          [-1.2126e+00, -2.3896e+00,  9.1677e-01,  ...,  2.9181e-01,\n",
       "           -1.7417e+00,  5.7122e-01],\n",
       "          [-5.1778e-01, -2.1021e+00,  3.1923e+00,  ...,  3.0047e+00,\n",
       "           -1.2340e+00,  3.0353e+00],\n",
       "          ...,\n",
       "          [ 1.2530e-01, -1.3964e+00, -4.6695e-01,  ...,  2.1448e+00,\n",
       "            2.7623e+00,  1.2372e+00],\n",
       "          [-1.8492e-01, -1.5864e+00,  2.4275e+00,  ...,  1.0139e+00,\n",
       "            2.3902e+00,  3.5652e+00],\n",
       "          [ 6.1069e-01, -2.2619e+00,  1.1960e+00,  ..., -1.4115e-01,\n",
       "            5.4768e-01,  2.2002e+00]],\n",
       "\n",
       "         [[ 4.3180e-01,  2.5124e-01,  3.0954e-01,  ...,  5.9666e-02,\n",
       "           -3.6656e-01,  5.2774e-01],\n",
       "          [-3.1267e-01,  1.1111e+00,  1.6110e+00,  ..., -5.0611e-01,\n",
       "           -1.3151e+00,  2.6756e-01],\n",
       "          [ 1.1761e-01, -3.6986e-01,  1.4999e+00,  ..., -5.6305e-01,\n",
       "            9.2696e-01,  1.1926e+00],\n",
       "          ...,\n",
       "          [-4.5742e-01,  6.1401e-01, -3.1995e-01,  ..., -4.3216e-01,\n",
       "            3.6245e-01,  8.8647e-02],\n",
       "          [-1.5482e-01, -1.3523e-01,  1.4161e+00,  ..., -4.8807e-01,\n",
       "            4.7376e-01,  7.2905e-02],\n",
       "          [ 3.4857e-01,  9.8764e-01,  1.1996e+00,  ..., -4.1149e-02,\n",
       "            9.1532e-01, -7.9954e-01]],\n",
       "\n",
       "         [[ 3.4184e-01,  3.2998e-01, -1.2764e-01,  ..., -1.0288e+00,\n",
       "           -1.1629e-01, -5.5326e-01],\n",
       "          [ 1.0352e+00, -4.3673e-01, -1.5014e-01,  ...,  5.6617e-01,\n",
       "            1.5817e+00,  1.3346e+00],\n",
       "          [ 2.0307e+00, -1.6123e+00, -8.9831e-01,  ...,  2.2787e+00,\n",
       "            1.3050e-02,  1.6079e-01],\n",
       "          ...,\n",
       "          [ 7.3401e-01,  8.1587e-01, -9.7700e-01,  ...,  3.2886e-01,\n",
       "           -1.3018e-01,  4.9216e-01],\n",
       "          [ 1.6430e+00, -1.0061e+00, -2.0258e+00,  ...,  1.1757e+00,\n",
       "            5.2159e-01,  4.7838e-01],\n",
       "          [ 2.4508e+00,  3.3986e-03, -1.6646e+00,  ...,  1.1032e+00,\n",
       "            1.1833e+00,  5.3564e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-1.2740e-03,  1.7693e-03, -1.0365e-02,  ...,  6.4400e-03,\n",
       "           -4.8118e-03, -2.4332e-03],\n",
       "          [ 7.8530e-01,  1.4971e-01, -1.0578e-01,  ..., -1.4508e-01,\n",
       "           -2.3411e-01, -8.4722e-02],\n",
       "          [-7.7185e-01, -1.9059e+00, -3.8721e-01,  ..., -6.1104e-01,\n",
       "           -1.3461e+00, -2.7705e-01],\n",
       "          ...,\n",
       "          [ 2.4970e-01,  3.9245e-01,  7.2852e-02,  ..., -2.2838e-01,\n",
       "            2.6357e-01,  5.7402e-01],\n",
       "          [ 5.2482e-01, -9.6859e-02,  1.7681e+00,  ...,  1.3766e+00,\n",
       "            4.9325e-01,  8.4791e-02],\n",
       "          [ 7.6694e-01,  8.3788e-02,  7.3994e-01,  ...,  6.9551e-01,\n",
       "            4.0573e-01, -4.1893e-01]],\n",
       "\n",
       "         [[ 2.6579e-03, -2.4218e-02, -7.2848e-03,  ...,  4.7805e-03,\n",
       "            2.2620e-03,  2.4635e-03],\n",
       "          [-6.0487e-02, -4.6074e-01, -3.6973e-01,  ..., -1.8545e-01,\n",
       "            1.6062e-01,  2.9497e-01],\n",
       "          [ 3.4676e-01, -1.2427e+00,  1.0575e+00,  ...,  7.2336e-01,\n",
       "           -5.2240e-01,  7.0512e-01],\n",
       "          ...,\n",
       "          [ 8.1637e-01, -1.4915e+00, -1.2863e+00,  ..., -1.8976e-01,\n",
       "           -9.7026e-02,  1.0805e-01],\n",
       "          [ 1.0283e+00, -4.1898e-01, -2.3842e-01,  ...,  6.0770e-01,\n",
       "           -5.8620e-01, -5.3753e-01],\n",
       "          [ 4.7456e-01, -2.2626e-01, -1.0021e+00,  ...,  1.4207e-01,\n",
       "            7.9400e-01, -1.3714e-01]],\n",
       "\n",
       "         [[-9.0546e-04,  2.4383e-04,  3.9762e-03,  ..., -9.3445e-03,\n",
       "           -4.9636e-03, -3.4196e-03],\n",
       "          [ 4.1490e-01, -5.3463e-01,  3.6267e-01,  ...,  2.5468e-01,\n",
       "           -5.1461e-01,  4.4237e-01],\n",
       "          [ 7.9911e-02,  1.0059e-02, -1.9786e-01,  ..., -3.6169e-02,\n",
       "           -7.2564e-01,  4.4700e-01],\n",
       "          ...,\n",
       "          [-6.9896e-01,  1.2633e-01,  9.1112e-03,  ...,  3.2110e-01,\n",
       "           -1.0444e+00,  1.2709e+00],\n",
       "          [ 9.9495e-01, -4.9620e-01, -1.8114e-01,  ..., -1.1712e-01,\n",
       "           -4.3924e-01, -9.5727e-02],\n",
       "          [ 6.8174e-01, -8.2191e-01, -3.6254e-01,  ...,  1.1974e-01,\n",
       "            7.2763e-02, -4.0771e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.9262e-02,  2.5228e-02, -2.6849e-02,  ..., -7.0955e-02,\n",
       "            5.4123e-02, -2.9500e-02],\n",
       "          [ 8.2199e-02,  4.8609e-01,  2.1962e-01,  ..., -7.9328e-02,\n",
       "            6.1443e-02, -2.1189e-01],\n",
       "          [ 1.4587e-01,  2.7946e-01, -1.1258e-01,  ..., -3.2233e-01,\n",
       "            2.2697e-01,  1.1831e-01],\n",
       "          ...,\n",
       "          [ 1.4885e-01,  5.6176e-01, -3.0305e-01,  ...,  2.3151e-01,\n",
       "           -1.4364e-01,  1.9602e-01],\n",
       "          [-2.1668e-01,  3.2942e-01,  1.0927e-01,  ..., -2.6153e-01,\n",
       "            4.4361e-01, -5.3040e-02],\n",
       "          [-1.2410e-02,  6.4451e-02,  1.8228e-01,  ..., -2.3249e-01,\n",
       "            3.6035e-01,  3.8932e-01]],\n",
       "\n",
       "         [[-5.5943e-03, -1.3523e-02, -1.0672e-02,  ...,  7.3998e-04,\n",
       "            1.8724e-03, -1.4945e-02],\n",
       "          [ 4.8298e-01,  3.1534e-01,  5.0490e-01,  ...,  1.8777e-02,\n",
       "           -6.0877e-02,  2.9693e-01],\n",
       "          [-5.9630e-01,  5.4227e-01,  9.9582e-01,  ...,  1.5571e-01,\n",
       "            3.5219e-01, -2.3636e-01],\n",
       "          ...,\n",
       "          [ 1.2187e+00, -6.5223e-01,  8.7929e-01,  ..., -1.2794e-01,\n",
       "           -7.9300e-01,  2.8331e-01],\n",
       "          [ 6.3380e-01,  8.4990e-01,  4.7011e-01,  ..., -2.6111e-01,\n",
       "           -9.2668e-02, -9.2476e-01],\n",
       "          [-2.1613e-01,  7.1825e-01, -4.1020e-01,  ...,  4.4259e-01,\n",
       "           -2.1431e-01, -1.9069e-01]],\n",
       "\n",
       "         [[-4.0349e-04, -1.2162e-03,  2.4204e-03,  ..., -7.7100e-03,\n",
       "            2.9382e-04,  4.5203e-03],\n",
       "          [ 5.0323e-01, -1.2520e-01, -1.6059e-01,  ..., -6.0202e-01,\n",
       "           -5.7190e-01, -7.5540e-01],\n",
       "          [-7.6594e-01, -1.4562e+00,  9.5411e-01,  ...,  2.7891e-01,\n",
       "            1.0846e-01, -1.3388e+00],\n",
       "          ...,\n",
       "          [ 1.5727e-01, -7.2176e-01, -7.6599e-01,  ..., -1.4612e+00,\n",
       "            4.6317e-01, -2.7389e-01],\n",
       "          [ 8.3872e-01, -1.1122e+00,  6.3649e-01,  ...,  9.4970e-02,\n",
       "            1.3833e+00, -7.0946e-01],\n",
       "          [ 6.6897e-01, -4.6426e-02,  2.7946e-01,  ...,  2.6981e-01,\n",
       "           -1.1467e+00, -9.1970e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-1.0949e+00, -1.4555e-01,  1.4360e-01,  ...,  1.6387e-01,\n",
       "            6.0530e-01,  6.9536e-02],\n",
       "          [-1.0909e+00, -1.4500e-01,  1.4299e-01,  ...,  1.6272e-01,\n",
       "            6.0347e-01,  7.0437e-02],\n",
       "          [-8.6354e-01, -7.2069e-01, -4.7037e-01,  ...,  7.4753e-02,\n",
       "            5.9861e-01, -1.6409e+00],\n",
       "          ...,\n",
       "          [-8.7815e-01, -6.5894e-01, -7.5902e-01,  ...,  3.8982e-01,\n",
       "           -5.6528e-01, -1.1199e+00],\n",
       "          [-5.4606e-01, -9.5810e-01, -5.7654e-01,  ...,  1.7024e-02,\n",
       "           -4.4101e-01, -9.5012e-01],\n",
       "          [-6.7103e-01, -1.8273e-01, -4.7573e-01,  ..., -2.3214e-03,\n",
       "            6.4193e-02, -1.0794e+00]],\n",
       "\n",
       "         [[-3.9803e-02, -1.0620e+00,  3.1478e-01,  ...,  1.1952e-01,\n",
       "           -2.0688e-01, -5.2654e-01],\n",
       "          [-3.8996e-02, -1.0610e+00,  3.1441e-01,  ...,  1.1896e-01,\n",
       "           -2.0714e-01, -5.2549e-01],\n",
       "          [-1.1733e-01,  5.1402e-01,  3.8263e-01,  ..., -6.8164e-01,\n",
       "           -7.4678e-01, -7.9214e-02],\n",
       "          ...,\n",
       "          [-6.6119e-01,  1.2227e+00,  3.6019e-01,  ...,  1.6107e+00,\n",
       "           -2.2277e-01,  1.5579e-01],\n",
       "          [-6.2837e-01,  7.9820e-01,  1.0471e+00,  ...,  1.4103e+00,\n",
       "            1.3418e-03,  5.5007e-01],\n",
       "          [-1.9430e-01,  9.8540e-01,  1.2154e+00,  ...,  4.8095e-01,\n",
       "           -1.0135e+00,  8.6354e-01]],\n",
       "\n",
       "         [[-1.4215e+00,  1.1793e-01, -2.8267e-01,  ..., -2.1602e-01,\n",
       "           -9.8139e-02, -6.6051e-02],\n",
       "          [-1.4171e+00,  1.1632e-01, -2.8106e-01,  ..., -2.1469e-01,\n",
       "           -9.8050e-02, -6.5301e-02],\n",
       "          [ 9.1044e-01, -4.8055e-01, -2.9325e-01,  ...,  2.3940e+00,\n",
       "            4.5758e-01,  9.4896e-01],\n",
       "          ...,\n",
       "          [ 1.4561e+00,  2.4810e-02,  2.3712e-02,  ...,  1.0855e+00,\n",
       "           -2.3606e-01, -6.9784e-01],\n",
       "          [ 1.0325e+00,  5.5683e-01, -4.1206e-01,  ...,  2.0466e+00,\n",
       "           -3.1424e-01,  6.2397e-01],\n",
       "          [-5.9093e-02, -8.5058e-02,  2.8227e-01,  ...,  6.6181e-01,\n",
       "            7.3816e-02,  1.1785e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2050e-01,  5.1743e-01,  2.1186e-01,  ..., -7.5647e-03,\n",
       "            2.6254e-01, -4.4021e-01],\n",
       "          [ 2.1800e-01,  5.1628e-01,  2.1063e-01,  ..., -8.7929e-03,\n",
       "            2.6284e-01, -4.4024e-01],\n",
       "          [-5.9940e-01, -2.6710e-01, -2.9955e-01,  ...,  1.6195e+00,\n",
       "           -2.4545e-01,  5.4536e-01],\n",
       "          ...,\n",
       "          [ 5.1838e-01, -5.1578e-01,  2.2358e-01,  ...,  2.7457e-02,\n",
       "           -4.1633e-01,  6.7490e-01],\n",
       "          [-5.9834e-01, -8.1542e-01, -1.5491e+00,  ...,  2.5793e-01,\n",
       "           -9.7214e-01,  5.3357e-01],\n",
       "          [-1.1348e+00,  6.1127e-01,  4.7070e-01,  ..., -1.7086e-02,\n",
       "           -1.5415e+00, -5.3984e-01]],\n",
       "\n",
       "         [[-4.0482e-01,  5.0278e-02, -2.1591e-01,  ...,  2.9629e-01,\n",
       "            2.2025e-01,  2.0629e-01],\n",
       "          [-4.0259e-01,  5.0904e-02, -2.1477e-01,  ...,  2.9502e-01,\n",
       "            2.1968e-01,  2.0544e-01],\n",
       "          [ 3.4583e-01, -4.6914e-01, -2.5432e-01,  ...,  5.5580e-01,\n",
       "            8.4897e-03,  7.0840e-01],\n",
       "          ...,\n",
       "          [ 1.3381e+00, -5.7678e-01, -3.5566e-01,  ...,  8.5270e-01,\n",
       "           -1.2263e-01, -6.8965e-01],\n",
       "          [ 8.0879e-01, -7.0905e-01, -1.0186e+00,  ...,  6.6913e-01,\n",
       "           -6.7209e-02, -2.7898e-01],\n",
       "          [ 2.7805e-01, -5.6271e-02,  4.9499e-02,  ..., -6.2476e-03,\n",
       "           -4.6875e-01,  8.7091e-01]],\n",
       "\n",
       "         [[ 3.0394e-01,  4.7263e-01, -1.3034e-01,  ...,  3.7412e-01,\n",
       "           -6.9410e-02,  1.3994e-01],\n",
       "          [ 3.0284e-01,  4.7132e-01, -1.3197e-01,  ...,  3.7208e-01,\n",
       "           -6.8801e-02,  1.4061e-01],\n",
       "          [ 4.8640e-02, -8.7799e-01,  1.0749e+00,  ...,  5.7411e-02,\n",
       "           -4.1148e-01,  1.6726e-01],\n",
       "          ...,\n",
       "          [-1.2909e+00,  1.6154e-01,  2.5361e-01,  ...,  2.9352e-01,\n",
       "           -6.9063e-01,  4.9023e-01],\n",
       "          [-1.5230e-01, -3.9699e-01,  5.9492e-01,  ..., -5.9380e-02,\n",
       "           -4.3787e-01,  6.3825e-01],\n",
       "          [-4.5271e-02, -7.7721e-02, -2.4670e-02,  ...,  4.4247e-01,\n",
       "            2.6199e-01,  1.5201e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 3.3033e-02, -1.4500e-02, -1.7797e-02,  ...,  1.4616e-02,\n",
       "           -2.5652e-02,  7.7046e-04],\n",
       "          [ 3.3009e-02, -1.4209e-02, -1.7566e-02,  ...,  1.4867e-02,\n",
       "           -2.4989e-02,  1.8459e-04],\n",
       "          [ 9.0619e-01,  3.2263e-01, -6.4360e-01,  ...,  1.0736e+00,\n",
       "            2.0175e-01,  5.9813e-02],\n",
       "          ...,\n",
       "          [-1.9184e-01, -8.2316e-02, -7.1716e-01,  ...,  6.0201e-01,\n",
       "            4.9842e-01, -3.6285e-01],\n",
       "          [ 3.2138e-01, -2.5406e-01, -7.8976e-01,  ...,  3.7483e-01,\n",
       "            2.9278e-01,  1.6178e-01],\n",
       "          [-1.9292e-01,  2.4384e-01,  2.0681e-01,  ...,  8.4548e-01,\n",
       "            3.6550e-01, -1.0383e-01]],\n",
       "\n",
       "         [[-3.4060e-01, -5.8079e-02, -8.6125e-02,  ...,  1.3870e-02,\n",
       "            1.0752e-01,  8.1692e-02],\n",
       "          [-3.4140e-01, -5.8640e-02, -8.5677e-02,  ...,  1.3729e-02,\n",
       "            1.0760e-01,  8.2903e-02],\n",
       "          [-7.7295e-02, -1.2108e-01, -1.6528e-01,  ..., -1.1292e-01,\n",
       "           -1.0266e+00,  2.3244e-01],\n",
       "          ...,\n",
       "          [-3.1896e-01,  2.0611e-01, -2.3013e-01,  ...,  1.3926e-02,\n",
       "           -7.4695e-01,  5.3381e-01],\n",
       "          [-2.4907e-02,  2.4914e-01, -8.2066e-02,  ..., -2.3518e-01,\n",
       "           -3.6190e-01,  4.1974e-01],\n",
       "          [-3.6569e-01, -3.3590e-01, -5.0630e-01,  ...,  1.8172e-01,\n",
       "           -5.1327e-01, -9.4108e-03]],\n",
       "\n",
       "         [[ 4.4856e-02, -1.0597e-01, -1.3277e-02,  ...,  5.4296e-02,\n",
       "           -1.9382e-02, -7.0034e-02],\n",
       "          [ 4.5097e-02, -1.0594e-01, -1.3707e-02,  ...,  5.5238e-02,\n",
       "           -1.9480e-02, -7.0487e-02],\n",
       "          [ 4.7640e-02, -2.6251e-01, -3.0324e-01,  ..., -1.1773e-03,\n",
       "           -1.3074e+00,  2.4431e-02],\n",
       "          ...,\n",
       "          [-2.4458e-01,  1.4040e-01, -4.2240e-01,  ..., -9.0479e-01,\n",
       "           -5.3253e-01, -1.4671e-01],\n",
       "          [-7.3491e-01, -2.3617e-01,  1.4316e-01,  ...,  8.7947e-01,\n",
       "           -4.2632e-01, -2.4547e-01],\n",
       "          [ 2.8064e-01, -2.3785e-01, -2.4160e-01,  ...,  1.7670e-02,\n",
       "           -6.8817e-02, -1.7256e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.0117e-03, -1.3135e-01, -3.2849e-02,  ..., -3.7927e-02,\n",
       "            1.6521e-01,  1.3842e-02],\n",
       "          [ 8.3537e-03, -1.3211e-01, -3.2850e-02,  ..., -3.8113e-02,\n",
       "            1.6533e-01,  1.3955e-02],\n",
       "          [ 7.4008e-02, -1.0923e-01, -1.9697e-01,  ...,  4.6261e-01,\n",
       "           -1.5904e-01,  4.8792e-02],\n",
       "          ...,\n",
       "          [ 5.2568e-01,  5.8468e-01, -1.7859e-01,  ...,  4.5239e-01,\n",
       "           -1.5273e-01,  7.4144e-02],\n",
       "          [ 3.1506e-01, -1.6022e-01,  4.5486e-01,  ...,  5.4850e-01,\n",
       "            2.5474e-01, -2.0339e-02],\n",
       "          [ 8.0614e-02,  5.5584e-01, -5.0304e-01,  ...,  4.0693e-01,\n",
       "            4.6695e-04,  3.8803e-01]],\n",
       "\n",
       "         [[-1.4496e-02,  1.0415e-02,  4.8580e-02,  ..., -1.8565e-02,\n",
       "            2.0918e-02,  1.7622e-02],\n",
       "          [-1.4460e-02,  9.8821e-03,  4.9024e-02,  ..., -1.8283e-02,\n",
       "            2.1113e-02,  1.7785e-02],\n",
       "          [-5.8599e-01, -2.7398e-01, -6.3055e-01,  ..., -1.3734e+00,\n",
       "           -1.7833e-02, -1.2103e+00],\n",
       "          ...,\n",
       "          [ 3.4931e-01, -1.2440e-01, -9.7221e-01,  ..., -4.4109e-01,\n",
       "            7.1297e-01, -9.1320e-01],\n",
       "          [-4.2676e-01,  1.2946e-01,  1.6130e-01,  ..., -1.2110e+00,\n",
       "            1.8049e-01, -8.3820e-01],\n",
       "          [ 8.7927e-01, -7.8980e-01,  1.2241e-01,  ...,  2.9661e-01,\n",
       "            2.5502e-01,  5.5379e-01]],\n",
       "\n",
       "         [[-9.1704e-02, -1.5296e-02,  4.2387e-02,  ..., -7.4903e-02,\n",
       "            1.0874e-01,  3.7281e-02],\n",
       "          [-9.2097e-02, -1.5747e-02,  4.2009e-02,  ..., -7.4610e-02,\n",
       "            1.0912e-01,  3.7092e-02],\n",
       "          [-2.0339e-02, -3.7648e-01, -4.9949e-01,  ..., -6.1118e-01,\n",
       "            2.4869e-01,  9.8546e-03],\n",
       "          ...,\n",
       "          [-1.9401e-01, -1.5999e-01, -3.6309e-01,  ..., -3.8859e-02,\n",
       "            3.7109e-01,  2.8909e-01],\n",
       "          [-1.5217e-02, -2.9053e-01, -1.1992e+00,  ..., -4.5536e-01,\n",
       "            5.4940e-01, -3.3064e-01],\n",
       "          [ 4.7065e-01, -2.5371e-03, -9.4105e-02,  ..., -4.0254e-01,\n",
       "            1.3955e-01, -1.2678e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-1.9139e-01, -2.6386e-01, -3.2293e-01,  ...,  1.4034e-01,\n",
       "           -4.6297e-01, -3.2519e-01],\n",
       "          [-2.8966e+00,  4.3105e-01, -4.6979e-03,  ...,  1.2015e+00,\n",
       "           -5.6724e-02,  8.4513e-03],\n",
       "          [-1.7380e+00, -7.7041e-02,  2.5301e+00,  ...,  1.2177e+00,\n",
       "            9.8426e-01, -1.0807e-01],\n",
       "          ...,\n",
       "          [-2.2654e+00,  1.0807e+00,  1.1564e+00,  ...,  1.0835e+00,\n",
       "            1.3930e+00,  5.0162e-01],\n",
       "          [-1.9402e+00,  1.0569e+00,  1.5621e+00,  ...,  9.6259e-01,\n",
       "            9.1102e-01,  1.7396e+00],\n",
       "          [-2.6880e+00,  1.7192e+00,  1.3874e+00,  ...,  1.1696e+00,\n",
       "           -4.1343e-01,  1.4258e+00]],\n",
       "\n",
       "         [[ 2.5220e-02, -2.9267e-02,  3.6087e-01,  ...,  1.2217e-01,\n",
       "           -1.4011e-01, -6.3787e-01],\n",
       "          [ 8.9944e-03,  6.7954e-03,  5.3467e-01,  ..., -5.0571e-01,\n",
       "            2.1507e+00,  1.1116e+00],\n",
       "          [ 3.9029e-01, -4.0759e-01,  1.4910e+00,  ...,  8.5944e-01,\n",
       "            2.4518e+00,  1.5315e+00],\n",
       "          ...,\n",
       "          [-1.5866e+00,  1.9938e+00,  1.0015e-01,  ..., -7.3971e-01,\n",
       "            7.3220e-02,  2.7359e-01],\n",
       "          [-2.5076e-01,  4.2135e-01, -1.4874e+00,  ..., -9.2273e-01,\n",
       "            1.3646e+00,  2.1851e+00],\n",
       "          [ 2.2462e-02, -8.6937e-01, -2.2042e+00,  ..., -1.3302e+00,\n",
       "            7.3414e-01,  1.0509e+00]],\n",
       "\n",
       "         [[-6.2650e-02,  8.5371e-02, -1.1155e-01,  ...,  5.3661e-02,\n",
       "            1.2514e-01, -1.8471e-01],\n",
       "          [-2.9937e-02, -6.4977e-01,  2.6266e-01,  ...,  9.2115e-01,\n",
       "           -9.7767e-01,  3.9749e-01],\n",
       "          [ 5.9698e-01, -2.0817e-01,  9.2915e-01,  ...,  1.2993e+00,\n",
       "            2.3659e-01,  1.4021e+00],\n",
       "          ...,\n",
       "          [-3.4135e-01,  1.5795e-01,  7.0257e-02,  ...,  3.3144e-02,\n",
       "            1.1329e-02, -8.0601e-01],\n",
       "          [ 9.0839e-02,  2.3418e-01, -4.8022e-01,  ..., -4.4064e-01,\n",
       "           -2.5934e-01,  4.2512e-01],\n",
       "          [ 3.7765e-02,  4.5093e-02, -1.0252e+00,  ..., -6.4332e-01,\n",
       "           -5.8979e-01,  1.2554e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.1645e-01,  5.2752e-01,  3.0248e-01,  ..., -2.8818e-01,\n",
       "           -1.1389e-01,  6.7110e-02],\n",
       "          [-1.2495e+00, -7.3010e-01, -2.0265e+00,  ...,  1.7930e+00,\n",
       "            3.5116e-02,  1.0515e+00],\n",
       "          [-1.8646e+00, -2.6713e-02, -1.6316e+00,  ...,  2.3551e+00,\n",
       "            7.2663e-01,  1.1081e+00],\n",
       "          ...,\n",
       "          [-2.1895e+00, -1.5165e+00, -7.9366e-01,  ...,  2.2482e+00,\n",
       "            9.8686e-01, -2.0690e+00],\n",
       "          [-1.8045e+00, -1.8562e+00, -8.6120e-01,  ...,  1.8966e+00,\n",
       "            5.9577e-01,  1.1626e-01],\n",
       "          [-2.4964e+00, -1.8988e-01, -1.2337e+00,  ...,  2.1445e+00,\n",
       "           -4.1839e-01,  1.2015e-01]],\n",
       "\n",
       "         [[-8.4916e-01, -8.2619e-03, -9.7071e-02,  ..., -1.8895e-01,\n",
       "            3.6501e-01,  1.8196e+00],\n",
       "          [ 8.0582e-01, -8.3946e-01, -5.4531e-01,  ...,  7.9207e-01,\n",
       "            9.6404e-01, -3.1408e+00],\n",
       "          [ 2.6637e+00,  1.3076e-01, -5.2917e-01,  ...,  1.8814e+00,\n",
       "            1.6526e+00, -4.6724e+00],\n",
       "          ...,\n",
       "          [ 1.2627e+00, -1.8384e-01, -1.5662e+00,  ...,  1.3824e+00,\n",
       "           -2.3885e-01, -5.3969e+00],\n",
       "          [ 1.1661e+00,  2.0925e-01, -3.8965e+00,  ...,  1.3849e+00,\n",
       "           -3.0373e-01, -4.8255e+00],\n",
       "          [ 1.3973e+00, -5.4724e-01, -2.0282e+00,  ...,  2.4875e+00,\n",
       "            7.6660e-01, -5.4863e+00]],\n",
       "\n",
       "         [[-1.6434e-01,  6.4504e-01, -9.0899e-02,  ..., -2.4530e-01,\n",
       "            7.4350e-02, -1.6524e-01],\n",
       "          [ 1.1307e+00, -1.7373e+00,  1.1269e-01,  ...,  8.0962e-01,\n",
       "            4.5844e-01,  5.6499e-01],\n",
       "          [ 1.9409e+00, -1.4300e+00,  1.5773e-02,  ..., -9.6730e-01,\n",
       "            1.2367e+00,  1.1225e+00],\n",
       "          ...,\n",
       "          [ 1.7378e+00, -3.1253e+00,  1.8914e+00,  ...,  6.1440e-01,\n",
       "            1.0298e+00,  3.8425e-01],\n",
       "          [ 2.4309e+00, -1.6852e+00,  2.4186e+00,  ..., -4.1162e-01,\n",
       "            1.4343e+00,  2.0447e+00],\n",
       "          [ 1.8215e+00, -2.4374e+00,  8.3879e-01,  ...,  1.0718e+00,\n",
       "            2.4078e+00,  8.6542e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.1805e-03, -1.1312e-03, -1.7807e-03,  ..., -1.9467e-03,\n",
       "            4.7948e-04, -1.1399e-02],\n",
       "          [ 2.1602e-01,  8.7780e-01,  4.1305e-01,  ..., -2.3475e-01,\n",
       "           -4.1958e-01, -2.4471e-01],\n",
       "          [ 4.0489e-01, -5.0141e-01, -4.3246e-01,  ..., -5.2074e-01,\n",
       "            5.4696e-01,  4.0910e-01],\n",
       "          ...,\n",
       "          [ 5.8312e-01,  1.2899e+00, -1.9146e-01,  ..., -1.9694e-01,\n",
       "            3.6963e-01,  6.0804e-01],\n",
       "          [-3.4647e-01,  8.6755e-01,  1.9283e-01,  ...,  3.1541e-01,\n",
       "            4.0557e-01, -2.8564e-01],\n",
       "          [ 5.9817e-01,  1.0452e+00, -8.3639e-02,  ..., -7.2438e-02,\n",
       "            1.4508e-01,  3.6792e-01]],\n",
       "\n",
       "         [[-6.1793e-03, -7.9269e-03,  4.7249e-03,  ...,  9.8316e-03,\n",
       "           -1.1235e-02, -1.3562e-02],\n",
       "          [ 3.6614e-01,  4.0704e-01,  1.3883e-01,  ...,  3.2730e-01,\n",
       "           -6.7503e-01, -1.9586e-01],\n",
       "          [-7.8850e-01, -7.7307e-01, -1.2654e+00,  ..., -2.0247e+00,\n",
       "            5.1664e-01,  2.9717e-01],\n",
       "          ...,\n",
       "          [-6.4001e-01,  5.9576e-01,  7.6072e-01,  ...,  4.0350e-01,\n",
       "            1.3244e+00,  2.8392e-01],\n",
       "          [ 1.8481e-01, -7.1747e-01,  2.0278e-01,  ...,  2.2887e-01,\n",
       "            1.2972e+00,  3.1110e-01],\n",
       "          [ 1.2987e-01, -6.5421e-01,  3.7788e-01,  ...,  8.1204e-01,\n",
       "           -7.2767e-01,  3.3155e-01]],\n",
       "\n",
       "         [[ 8.5893e-03,  1.0518e-02,  4.9222e-03,  ..., -3.7223e-03,\n",
       "           -2.4355e-03,  2.7071e-03],\n",
       "          [ 9.0706e-01,  2.2070e-01,  1.9430e+00,  ...,  6.0154e-01,\n",
       "           -1.7208e-02,  1.0293e-01],\n",
       "          [ 3.3179e-01,  7.8524e-01,  3.8928e-01,  ...,  4.5416e-01,\n",
       "            6.5248e-01, -6.6452e-01],\n",
       "          ...,\n",
       "          [ 3.4351e-01, -2.5146e-01,  1.6346e+00,  ...,  1.4821e-01,\n",
       "           -1.3295e-01, -4.7660e-03],\n",
       "          [ 7.5400e-01, -2.8219e-01,  3.7321e-01,  ...,  7.6533e-01,\n",
       "            1.3940e-01,  6.6524e-01],\n",
       "          [ 1.0104e+00, -1.0457e-01,  6.0510e-01,  ...,  8.5260e-01,\n",
       "            3.2131e-01,  3.3137e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.9503e-03,  2.3463e-03,  3.6643e-02,  ..., -1.3475e-02,\n",
       "            1.6485e-02,  1.5460e-02],\n",
       "          [-2.9681e-01, -5.5714e-01,  1.1750e+00,  ...,  3.5112e-01,\n",
       "           -1.7497e-01, -1.2139e-01],\n",
       "          [-2.5887e-01, -3.6941e-04,  9.9100e-01,  ...,  9.9776e-01,\n",
       "           -2.2396e-01,  6.4841e-02],\n",
       "          ...,\n",
       "          [-7.4051e-01,  8.9989e-01,  3.7413e-01,  ...,  4.5358e-01,\n",
       "            1.4905e-01,  6.6005e-01],\n",
       "          [-5.2535e-01,  2.8766e-01,  1.5637e-01,  ...,  8.9579e-01,\n",
       "            5.5506e-01,  2.4811e-01],\n",
       "          [-1.0496e-02, -3.4480e-02,  8.5761e-01,  ...,  4.9425e-01,\n",
       "            2.3215e-01,  3.5785e-01]],\n",
       "\n",
       "         [[-8.7343e-04,  7.5434e-03, -7.5635e-03,  ..., -2.6574e-03,\n",
       "           -8.2291e-03,  5.9423e-03],\n",
       "          [ 3.8845e-01,  2.7002e-01, -4.4719e-01,  ..., -2.3068e-01,\n",
       "           -7.3258e-01, -5.6313e-01],\n",
       "          [ 3.1232e-01,  2.6027e-03,  9.6728e-01,  ..., -1.1996e+00,\n",
       "           -2.6623e-02, -1.1896e+00],\n",
       "          ...,\n",
       "          [ 4.6165e-01, -1.5622e-01, -5.1836e-02,  ..., -5.8793e-01,\n",
       "            3.6791e-02,  4.3705e-01],\n",
       "          [ 1.1209e+00, -9.2478e-01,  4.1633e-01,  ...,  1.1380e+00,\n",
       "            5.7140e-01,  3.2408e-02],\n",
       "          [ 3.4463e-01,  9.5582e-01, -2.0228e-01,  ...,  4.8862e-01,\n",
       "           -1.8211e-01, -1.8423e-01]],\n",
       "\n",
       "         [[-1.5176e-02,  7.4097e-04,  4.1446e-03,  ...,  3.1502e-03,\n",
       "           -2.9568e-03, -2.5384e-03],\n",
       "          [-5.0837e-01,  4.8829e-01, -5.3446e-01,  ..., -3.3031e-01,\n",
       "            9.2303e-02,  5.0329e-01],\n",
       "          [ 7.0313e-01,  2.1466e+00,  7.7696e-01,  ..., -2.9229e-01,\n",
       "           -2.0207e-01, -3.4971e-01],\n",
       "          ...,\n",
       "          [-5.0330e-01,  2.5490e-01, -2.6003e-01,  ..., -6.8165e-01,\n",
       "           -8.6775e-01,  4.4523e-01],\n",
       "          [ 6.8506e-01,  6.3415e-01, -6.6515e-01,  ..., -3.9891e-01,\n",
       "            4.5751e-03,  4.2918e-01],\n",
       "          [ 1.9173e-01,  2.8524e-01, -6.9922e-01,  ...,  7.1718e-02,\n",
       "            1.3461e-01,  2.4647e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 9.1740e-02, -2.2556e-01, -5.4315e-01,  ..., -1.2604e-01,\n",
       "            4.5427e-01, -1.7350e-01],\n",
       "          [ 9.0234e-02, -2.2459e-01, -5.4217e-01,  ..., -1.2641e-01,\n",
       "            4.5363e-01, -1.7231e-01],\n",
       "          [ 9.1944e-01, -2.3019e-01, -2.6739e-01,  ..., -1.1787e+00,\n",
       "            3.0494e-01, -2.0129e-02],\n",
       "          ...,\n",
       "          [ 1.8359e-01,  8.1769e-02,  1.0269e-01,  ...,  8.2206e-01,\n",
       "           -6.7536e-01, -8.7534e-03],\n",
       "          [ 3.5968e-01, -1.3795e-01,  4.9217e-01,  ...,  3.5594e-01,\n",
       "            4.8718e-01, -6.2783e-01],\n",
       "          [-2.1110e-01,  2.0599e-02,  5.0704e-01,  ..., -8.5874e-02,\n",
       "            1.1000e-01, -1.6217e-02]],\n",
       "\n",
       "         [[-2.2745e-01, -2.7265e-01,  4.3854e-03,  ..., -2.8394e-01,\n",
       "            4.9760e-01,  1.5274e-01],\n",
       "          [-2.2542e-01, -2.7218e-01,  5.3189e-03,  ..., -2.8217e-01,\n",
       "            4.9647e-01,  1.5305e-01],\n",
       "          [ 7.2081e-01, -6.7692e-01, -5.9403e-01,  ...,  1.4422e-01,\n",
       "           -4.8160e-01, -3.1788e-01],\n",
       "          ...,\n",
       "          [ 1.1330e+00,  3.8897e-01, -6.4997e-02,  ...,  1.9621e-01,\n",
       "           -2.2873e-01, -4.9326e-01],\n",
       "          [ 1.0509e+00, -1.8479e-01, -2.2050e-01,  ...,  1.8852e-01,\n",
       "           -2.0338e-01, -1.2738e-01],\n",
       "          [ 9.4421e-01, -2.0034e-01, -5.1446e-01,  ...,  3.2402e-01,\n",
       "           -3.1448e-01, -2.8019e-02]],\n",
       "\n",
       "         [[ 4.0727e-01, -4.8777e-01,  4.7301e-01,  ...,  4.4775e-01,\n",
       "           -4.2347e-01,  4.6952e-01],\n",
       "          [ 4.0644e-01, -4.8750e-01,  4.7240e-01,  ...,  4.4686e-01,\n",
       "           -4.2323e-01,  4.6950e-01],\n",
       "          [-1.7510e-01,  2.2601e+00, -1.3541e+00,  ...,  9.2394e-01,\n",
       "           -7.2269e-01,  1.0748e+00],\n",
       "          ...,\n",
       "          [ 2.8057e-01,  6.4199e-02,  4.8231e-01,  ..., -1.1634e+00,\n",
       "           -5.2027e-04, -2.6240e-01],\n",
       "          [ 1.9913e-01,  5.5685e-01,  1.4031e-02,  ..., -7.9611e-02,\n",
       "           -7.7200e-01,  6.4444e-02],\n",
       "          [ 1.2468e+00,  2.2610e-01,  9.4811e-01,  ..., -2.3455e-02,\n",
       "           -2.2924e-03,  2.8507e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3940e-01,  1.5901e-01, -2.5022e-01,  ..., -3.4283e-01,\n",
       "            7.0742e-02,  5.2652e-03],\n",
       "          [ 1.3848e-01,  1.5791e-01, -2.4828e-01,  ..., -3.4147e-01,\n",
       "            6.9348e-02,  5.5818e-03],\n",
       "          [-1.0051e-01, -1.6447e-01,  6.8401e-01,  ...,  3.1025e-01,\n",
       "            7.2989e-01,  5.0267e-01],\n",
       "          ...,\n",
       "          [-3.8963e-01,  7.8133e-01,  7.1089e-02,  ..., -7.4507e-01,\n",
       "            4.8866e-01,  8.6442e-01],\n",
       "          [ 8.8632e-02,  3.6025e-01,  6.5343e-01,  ..., -3.2644e-01,\n",
       "            6.9457e-01,  4.1613e-01],\n",
       "          [ 4.1604e-01, -1.7033e-01,  3.4595e-01,  ...,  2.4185e-01,\n",
       "           -3.0956e-01,  4.4122e-01]],\n",
       "\n",
       "         [[-1.2071e-01,  1.3641e-01, -4.8072e-01,  ...,  6.8399e-01,\n",
       "           -3.6701e-01, -1.7854e-01],\n",
       "          [-1.2037e-01,  1.3634e-01, -4.8092e-01,  ...,  6.8211e-01,\n",
       "           -3.6750e-01, -1.7866e-01],\n",
       "          [ 1.6843e-01, -8.2300e-01, -6.3326e-01,  ..., -2.2692e-01,\n",
       "           -8.6437e-01, -2.0319e-01],\n",
       "          ...,\n",
       "          [ 2.8191e-01, -5.2315e-01, -7.0182e-02,  ..., -2.7920e-01,\n",
       "           -3.6156e-01,  8.9165e-01],\n",
       "          [ 1.6245e-01, -7.0680e-01,  3.4684e-02,  ..., -1.0016e+00,\n",
       "            3.2591e-01, -3.2905e-01],\n",
       "          [-2.1468e-01, -1.5581e-01,  5.4228e-01,  ..., -6.6547e-02,\n",
       "           -2.4393e-01, -6.9771e-01]],\n",
       "\n",
       "         [[ 4.9352e-01,  4.0380e-01, -1.1077e-01,  ...,  3.0761e-01,\n",
       "            3.7672e-02,  1.3636e+00],\n",
       "          [ 4.9249e-01,  4.0322e-01, -1.1164e-01,  ...,  3.0713e-01,\n",
       "            3.8810e-02,  1.3611e+00],\n",
       "          [-7.7245e-01, -8.8617e-01, -1.4324e-01,  ..., -3.0507e-01,\n",
       "            1.0888e+00,  8.9592e-01],\n",
       "          ...,\n",
       "          [ 2.6561e-02,  5.7792e-02,  5.0618e-01,  ...,  5.7406e-01,\n",
       "           -5.9745e-01,  3.3709e-01],\n",
       "          [ 2.0026e-01, -3.9505e-01,  2.6948e-01,  ..., -6.5571e-01,\n",
       "            3.3490e-01, -8.6361e-01],\n",
       "          [-5.5016e-01, -1.0206e+00, -5.7069e-01,  ..., -4.5568e-01,\n",
       "            4.9023e-01,  3.6691e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-0.1826,  0.0071,  0.0187,  ..., -0.0513,  0.0088, -0.0087],\n",
       "          [-0.1830,  0.0066,  0.0195,  ..., -0.0512,  0.0088, -0.0079],\n",
       "          [ 0.3588, -0.8017,  1.2655,  ...,  0.4414,  0.6545,  0.5615],\n",
       "          ...,\n",
       "          [ 0.4100, -0.0313,  0.4208,  ..., -0.2743, -0.3412, -0.1177],\n",
       "          [ 0.3571, -0.6627,  0.4888,  ..., -0.4908,  0.1494,  0.0987],\n",
       "          [-0.1133, -0.2821,  0.2976,  ..., -0.1514,  0.3595,  0.6718]],\n",
       "\n",
       "         [[ 0.0161,  0.0148,  0.0402,  ..., -0.0292, -0.0235,  0.0258],\n",
       "          [ 0.0165,  0.0149,  0.0406,  ..., -0.0294, -0.0238,  0.0260],\n",
       "          [-0.4752,  0.2501, -0.2852,  ...,  0.6850,  0.2691, -0.0101],\n",
       "          ...,\n",
       "          [-1.0570, -0.9865, -0.3575,  ...,  1.2114,  0.1409, -1.0196],\n",
       "          [-0.4436, -1.0916, -1.4102,  ...,  0.7525, -0.0956, -0.4621],\n",
       "          [ 0.4135, -0.2329, -0.0402,  ..., -0.3341,  0.2026,  0.0995]],\n",
       "\n",
       "         [[ 0.0813,  0.0327,  0.0684,  ..., -0.0155,  0.0051,  0.0157],\n",
       "          [ 0.0816,  0.0325,  0.0688,  ..., -0.0152,  0.0059,  0.0157],\n",
       "          [ 0.7033,  0.1236,  0.0060,  ...,  0.1845, -0.5443, -0.1370],\n",
       "          ...,\n",
       "          [ 0.0948,  0.2802,  0.1211,  ...,  0.0148, -0.5293, -0.0141],\n",
       "          [ 0.1314,  0.0611, -0.1037,  ..., -0.0338, -0.3860, -0.1198],\n",
       "          [ 0.1236,  0.4417, -0.0094,  ...,  0.1662,  0.0823,  0.0809]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0242,  0.0464, -0.2137,  ..., -0.1044,  0.1984, -0.0084],\n",
       "          [-0.0242,  0.0467, -0.2143,  ..., -0.1042,  0.1984, -0.0081],\n",
       "          [-0.5387, -0.7368, -0.1616,  ..., -0.5564,  0.0465, -0.3666],\n",
       "          ...,\n",
       "          [-0.3297,  0.1224,  0.2715,  ...,  1.1101,  0.9511, -0.0560],\n",
       "          [-1.2135, -0.1340,  0.2520,  ..., -0.1031,  0.2127, -0.4521],\n",
       "          [ 0.9327,  0.3990, -0.7476,  ..., -0.6514,  0.0126,  0.1016]],\n",
       "\n",
       "         [[ 0.0204, -0.1020, -0.0971,  ..., -0.0045,  0.0243, -0.0643],\n",
       "          [ 0.0205, -0.1025, -0.0977,  ..., -0.0044,  0.0241, -0.0643],\n",
       "          [ 0.8708, -0.7732,  0.3453,  ...,  0.2115, -0.0678,  0.9977],\n",
       "          ...,\n",
       "          [ 0.5788,  0.4841,  1.1314,  ..., -0.2268,  0.2093,  0.6071],\n",
       "          [ 0.1516,  0.7721,  0.7622,  ...,  0.3022, -0.1356,  0.4553],\n",
       "          [ 0.5720, -0.4474,  0.3495,  ..., -0.8308, -0.2403,  0.2706]],\n",
       "\n",
       "         [[-0.0471,  0.0374,  0.0133,  ..., -0.0477, -0.0095,  0.0450],\n",
       "          [-0.0471,  0.0375,  0.0132,  ..., -0.0474, -0.0093,  0.0455],\n",
       "          [-0.3328,  0.3732, -0.3017,  ...,  0.7156, -0.0093, -0.4450],\n",
       "          ...,\n",
       "          [ 0.5278, -0.0755, -0.7883,  ...,  0.5326,  0.3133,  0.1081],\n",
       "          [ 0.9634, -0.7874, -0.8503,  ...,  0.3961,  0.0057, -0.5296],\n",
       "          [ 0.2936,  0.1594, -0.5792,  ...,  0.5163,  0.4428,  0.4557]]]],\n",
       "       grad_fn=<CloneBackward0>)), (tensor([[[[ 2.5950e-01, -1.3474e-02,  9.2311e-01,  ...,  6.9637e-01,\n",
       "           -1.6676e-01, -2.4603e-01],\n",
       "          [-6.2819e-01, -1.0488e-01, -5.0192e-01,  ...,  3.4907e-01,\n",
       "            5.2327e-01,  4.7795e-01],\n",
       "          [-6.0644e-01,  1.3729e+00, -2.4054e+00,  ...,  5.9289e-01,\n",
       "            5.5595e-03,  7.5210e-01],\n",
       "          ...,\n",
       "          [-1.3567e+00,  1.0821e+00, -8.4562e-01,  ...,  4.3822e-01,\n",
       "            1.8203e+00,  8.0725e-01],\n",
       "          [-1.2944e+00,  2.5904e+00, -2.9742e+00,  ...,  1.1507e-01,\n",
       "            1.5841e+00,  2.2298e-01],\n",
       "          [-1.6184e+00,  1.5391e+00, -1.6222e+00,  ...,  8.4488e-01,\n",
       "            1.7064e+00, -3.7779e-01]],\n",
       "\n",
       "         [[-3.2359e-01, -1.4873e-03,  8.3150e-02,  ...,  1.8698e-01,\n",
       "           -7.0195e-02, -2.9112e-01],\n",
       "          [-1.2020e+00, -1.4414e+00,  6.5976e-01,  ..., -9.3286e-01,\n",
       "            2.4301e-01, -5.1546e-01],\n",
       "          [-2.2354e-01, -4.1242e-01,  3.3483e-01,  ..., -1.4708e+00,\n",
       "           -3.4813e-01, -7.9814e-01],\n",
       "          ...,\n",
       "          [-1.4243e+00, -5.0767e-01, -6.7244e-01,  ..., -4.0913e-01,\n",
       "           -2.1180e+00,  8.7858e-01],\n",
       "          [ 7.4558e-01, -4.4042e-01, -9.2598e-01,  ..., -6.0125e-01,\n",
       "           -1.0159e+00, -9.6043e-01],\n",
       "          [-1.1383e+00,  6.2099e-02, -1.6145e+00,  ...,  1.0442e+00,\n",
       "           -1.3223e+00,  1.3248e+00]],\n",
       "\n",
       "         [[-9.5875e-03, -1.2754e-01,  9.5684e-02,  ...,  7.2441e-02,\n",
       "            3.3491e-02,  1.4946e-01],\n",
       "          [ 2.9232e-01,  1.6283e-01, -9.2778e-01,  ..., -2.8114e-01,\n",
       "            6.8215e-01,  4.5484e-01],\n",
       "          [ 1.6379e-01,  1.0051e-01, -1.2820e+00,  ...,  9.6996e-01,\n",
       "           -1.4151e-02,  4.4628e-01],\n",
       "          ...,\n",
       "          [-6.6700e-01,  1.7858e+00,  1.7365e-01,  ...,  8.2771e-01,\n",
       "            8.6195e-01, -5.1864e-01],\n",
       "          [-4.5379e-01, -6.3853e-01, -9.3741e-01,  ...,  1.5482e+00,\n",
       "           -1.1190e-01,  1.2498e+00],\n",
       "          [-2.5791e-01, -9.5387e-01, -4.8480e-01,  ..., -3.2717e-01,\n",
       "            7.1283e-01,  1.4921e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.7062e-01, -1.1141e-01,  5.8497e-01,  ..., -2.2698e-01,\n",
       "           -1.8844e-01,  1.2908e-01],\n",
       "          [ 2.2590e+00,  1.1092e-01,  1.7412e+00,  ...,  1.7599e+00,\n",
       "            5.1848e-01, -3.1243e-01],\n",
       "          [ 2.8604e-01,  7.9748e-01, -5.7646e-01,  ...,  1.2426e+00,\n",
       "            8.7745e-01, -9.3335e-01],\n",
       "          ...,\n",
       "          [ 1.2922e-01, -4.3125e-01, -7.5795e-01,  ...,  4.8092e-01,\n",
       "            1.8742e+00, -1.0981e+00],\n",
       "          [-1.1906e-01, -4.5142e-01, -4.5814e-01,  ...,  8.3040e-01,\n",
       "            1.2315e+00, -1.1866e+00],\n",
       "          [ 2.3458e-01, -1.0319e+00, -5.8058e-01,  ...,  1.4532e+00,\n",
       "            1.1242e+00, -5.0334e-02]],\n",
       "\n",
       "         [[-1.5984e-01,  1.9257e-01, -6.9461e-02,  ...,  2.8649e-01,\n",
       "            2.7326e-01, -2.1102e-01],\n",
       "          [-1.3002e+00, -2.9667e-01,  4.3429e-01,  ..., -5.6723e-01,\n",
       "            6.8135e-01,  1.3240e+00],\n",
       "          [-9.7974e-01, -9.6681e-01,  9.9266e-01,  ..., -1.4496e+00,\n",
       "           -4.6362e-01,  1.2850e+00],\n",
       "          ...,\n",
       "          [-1.0715e+00,  2.3614e+00, -2.7587e-01,  ..., -2.1879e+00,\n",
       "            7.5050e-02,  1.8332e+00],\n",
       "          [-9.8965e-02, -3.8045e-01, -2.1070e+00,  ..., -1.2871e+00,\n",
       "           -3.8753e-01,  1.6615e+00],\n",
       "          [ 7.9990e-02,  6.1257e-02, -6.8489e-01,  ..., -6.8059e-01,\n",
       "            1.4361e-01,  9.1385e-01]],\n",
       "\n",
       "         [[ 4.7752e-02,  1.0886e-01, -5.4496e-02,  ...,  3.9591e-02,\n",
       "           -8.5308e-02,  2.4593e-01],\n",
       "          [ 1.7534e+00, -1.2646e+00, -2.3551e-01,  ..., -1.9631e+00,\n",
       "            5.6811e-02,  1.1113e+00],\n",
       "          [-1.3655e-01, -1.6397e+00, -9.1397e-01,  ..., -2.6043e-01,\n",
       "            1.1442e+00, -4.5805e-01],\n",
       "          ...,\n",
       "          [ 1.0860e+00, -9.4317e-02, -1.0343e+00,  ..., -1.5145e+00,\n",
       "           -1.1384e+00,  3.0620e-01],\n",
       "          [ 7.7869e-01, -1.5399e+00, -1.2015e+00,  ..., -1.1296e+00,\n",
       "            9.1078e-01,  6.9396e-01],\n",
       "          [ 1.0331e+00, -8.4716e-01, -1.4493e+00,  ..., -8.5864e-01,\n",
       "           -1.9111e-01,  8.7600e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 5.2796e-03, -6.7783e-03,  1.5815e-03,  ...,  7.8715e-03,\n",
       "            7.1620e-03, -5.0452e-03],\n",
       "          [-3.6777e-02,  2.4607e-01, -9.3450e-01,  ..., -3.2325e-01,\n",
       "           -7.2540e-01,  1.8615e-02],\n",
       "          [-1.0670e-01,  6.9753e-01, -1.8623e-01,  ...,  3.7245e-01,\n",
       "            2.5455e-01, -4.7061e-02],\n",
       "          ...,\n",
       "          [ 4.5470e-01, -1.8231e-01, -1.4152e-01,  ...,  4.7589e-01,\n",
       "            2.8344e-03, -2.9752e-01],\n",
       "          [-3.8635e-02, -1.1604e+00,  7.5439e-01,  ...,  6.1119e-01,\n",
       "            1.5374e-02, -3.4394e-02],\n",
       "          [ 4.1545e-02, -2.1375e-01, -1.6105e-01,  ...,  4.1458e-01,\n",
       "           -5.7471e-01,  3.5088e-02]],\n",
       "\n",
       "         [[ 2.5582e-03,  1.1675e-03, -3.2573e-03,  ...,  4.3355e-03,\n",
       "            2.5595e-03,  3.5059e-03],\n",
       "          [-1.0856e+00,  3.6495e-01,  5.3375e-02,  ...,  2.1851e-01,\n",
       "            1.3788e-02,  6.2949e-01],\n",
       "          [-3.7263e-01,  7.7105e-02,  3.0525e-01,  ...,  9.4284e-02,\n",
       "           -1.7105e+00, -3.6514e-01],\n",
       "          ...,\n",
       "          [-9.3089e-01,  7.4212e-02, -6.9607e-01,  ...,  6.5935e-01,\n",
       "           -1.3532e+00, -1.6577e-01],\n",
       "          [-1.9277e-01, -1.0728e-01, -3.4541e-01,  ...,  1.3096e-01,\n",
       "           -8.7982e-01,  1.5370e-01],\n",
       "          [-2.8957e-03,  6.0856e-02,  6.5401e-01,  ...,  5.0673e-01,\n",
       "            2.3822e-01,  6.3305e-02]],\n",
       "\n",
       "         [[ 4.7776e-03,  2.5619e-03,  4.4406e-03,  ...,  1.0542e-03,\n",
       "           -8.6172e-03, -1.0104e-03],\n",
       "          [ 1.2151e-01, -3.9014e-01, -1.1857e-01,  ..., -5.5551e-01,\n",
       "            6.3090e-01,  3.5822e-02],\n",
       "          [ 9.4941e-01,  8.6493e-01, -3.3861e-01,  ..., -9.0382e-01,\n",
       "           -1.7403e-01, -1.2030e+00],\n",
       "          ...,\n",
       "          [-1.6124e-01, -2.7847e-01,  6.8447e-01,  ..., -7.8115e-01,\n",
       "           -3.9496e-02,  6.1206e-01],\n",
       "          [-1.8772e-01, -3.6948e-01,  9.7083e-01,  ..., -5.6648e-01,\n",
       "            8.3014e-01,  8.6605e-01],\n",
       "          [-4.7295e-01,  2.4706e-01, -1.4422e-02,  ...,  4.3259e-01,\n",
       "            7.6164e-01,  2.1569e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.7971e-02, -4.2917e-02, -6.3247e-03,  ..., -1.0367e-02,\n",
       "            2.3209e-02, -6.5704e-04],\n",
       "          [ 4.9202e-01, -3.2622e-01, -3.6505e-01,  ...,  2.2345e-01,\n",
       "           -4.7267e-01,  3.1054e-01],\n",
       "          [ 3.5439e-02,  3.0701e-01, -5.8727e-01,  ...,  5.0865e-01,\n",
       "           -5.5437e-02,  1.2995e-01],\n",
       "          ...,\n",
       "          [ 6.2530e-01, -4.3774e-01, -5.8172e-01,  ..., -1.3792e-01,\n",
       "           -2.3316e-01,  2.6657e-01],\n",
       "          [ 6.0221e-01, -4.6028e-02, -7.7728e-01,  ..., -4.3682e-01,\n",
       "           -3.3653e-01,  3.1908e-01],\n",
       "          [ 4.1091e-01, -2.7192e-01,  8.9056e-02,  ...,  2.0978e-01,\n",
       "           -2.6638e-02,  1.8835e-01]],\n",
       "\n",
       "         [[ 6.8517e-03, -9.0983e-03,  1.9433e-03,  ...,  6.8759e-04,\n",
       "           -6.4294e-03, -6.7803e-03],\n",
       "          [-6.5565e-01, -9.4902e-02,  7.7802e-01,  ..., -4.8672e-01,\n",
       "           -1.0155e+00, -8.8494e-02],\n",
       "          [ 5.7704e-01, -2.3220e-01,  5.5268e-01,  ..., -6.4073e-01,\n",
       "           -3.8660e-01,  9.2023e-01],\n",
       "          ...,\n",
       "          [-9.7835e-01,  5.9625e-01,  9.4824e-03,  ..., -4.8678e-01,\n",
       "           -6.6074e-01,  7.9767e-01],\n",
       "          [-1.2180e+00,  3.3603e-01,  1.4409e-02,  ..., -4.8198e-01,\n",
       "           -1.1540e+00,  1.1411e+00],\n",
       "          [-7.4681e-01, -5.9294e-03,  1.6209e-01,  ..., -5.0128e-01,\n",
       "           -1.3952e+00, -6.2461e-01]],\n",
       "\n",
       "         [[ 2.1611e-02,  5.1206e-03, -2.1316e-03,  ..., -1.5870e-02,\n",
       "           -1.0501e-02, -2.3240e-03],\n",
       "          [-5.1139e-01,  4.1100e-01,  1.3761e-01,  ...,  5.3390e-01,\n",
       "            2.7072e-01,  1.6112e-01],\n",
       "          [ 6.2901e-01,  1.3191e+00, -2.3207e+00,  ..., -6.4559e-01,\n",
       "            6.8574e-02,  2.1183e-01],\n",
       "          ...,\n",
       "          [-3.2687e-01, -2.4484e-01, -6.5599e-02,  ..., -1.8868e-01,\n",
       "           -3.9923e-02, -1.1347e-01],\n",
       "          [ 1.7241e-01,  8.2024e-01, -4.5803e-01,  ..., -6.5733e-01,\n",
       "            2.4216e-01, -1.2952e-01],\n",
       "          [ 5.8829e-01,  9.4329e-02, -2.7671e-01,  ...,  7.6278e-01,\n",
       "            3.1042e-01,  8.2942e-02]]]], grad_fn=<CloneBackward0>), tensor([[[[-1.1360e-01,  2.8142e-01,  7.4761e-01,  ..., -1.4953e-01,\n",
       "            7.1970e-01,  2.9277e-01],\n",
       "          [-1.1179e-01,  2.7987e-01,  7.4375e-01,  ..., -1.4958e-01,\n",
       "            7.1705e-01,  2.9281e-01],\n",
       "          [ 7.6617e-01,  3.3706e-01,  1.5538e+00,  ..., -9.1358e-01,\n",
       "            6.2374e-01,  7.4146e-01],\n",
       "          ...,\n",
       "          [-7.6824e-01, -1.3068e-01,  5.9026e-03,  ..., -7.0977e-02,\n",
       "            3.1370e-01,  1.1592e+00],\n",
       "          [-3.8549e-01, -8.1473e-01, -6.7185e-01,  ..., -3.4042e-01,\n",
       "            7.2770e-01,  9.7045e-01],\n",
       "          [-3.0795e-03,  4.6579e-01,  8.2716e-01,  ...,  4.3587e-02,\n",
       "           -3.8436e-01, -4.5495e-01]],\n",
       "\n",
       "         [[ 8.5477e-02, -1.5956e-01, -1.7027e-01,  ...,  2.2071e-01,\n",
       "           -3.8740e-02,  1.4371e-01],\n",
       "          [ 8.5538e-02, -1.5889e-01, -1.7017e-01,  ...,  2.1963e-01,\n",
       "           -3.8617e-02,  1.4316e-01],\n",
       "          [-5.8068e-01,  5.7493e-01, -1.3032e+00,  ...,  9.6058e-01,\n",
       "           -2.9846e-01, -2.3216e-02],\n",
       "          ...,\n",
       "          [ 4.8586e-01,  4.2162e-01, -2.7982e-01,  ...,  2.9774e-01,\n",
       "           -5.7500e-01, -3.3720e-02],\n",
       "          [-1.4334e-01,  3.2912e-01, -9.1117e-01,  ...,  1.1861e-01,\n",
       "            1.4311e-01,  1.8665e-01],\n",
       "          [ 1.8611e-01,  5.7249e-01, -1.3046e-01,  ...,  8.1739e-02,\n",
       "            1.6777e-01, -9.9798e-02]],\n",
       "\n",
       "         [[-4.1225e-01, -5.7223e-01, -1.5846e-01,  ..., -2.4446e-01,\n",
       "           -7.6683e-01, -2.3998e-01],\n",
       "          [-4.1095e-01, -5.7112e-01, -1.5748e-01,  ..., -2.4371e-01,\n",
       "           -7.6360e-01, -2.3950e-01],\n",
       "          [-1.1668e+00, -5.9792e-01,  1.5355e-01,  ..., -1.3902e-01,\n",
       "           -3.1124e-02, -1.2930e+00],\n",
       "          ...,\n",
       "          [ 2.9015e-01, -1.3486e+00,  5.6541e-01,  ..., -6.0502e-02,\n",
       "           -7.6847e-02,  4.2387e-01],\n",
       "          [-1.9196e-01, -1.1932e+00,  1.0045e-01,  ..., -6.4897e-01,\n",
       "            8.6696e-02,  4.0511e-01],\n",
       "          [-3.5437e-01,  6.3762e-01, -8.5435e-01,  ...,  2.0483e-01,\n",
       "           -2.3604e-01,  4.4864e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.3511e-01, -3.5569e-01,  1.3653e-01,  ..., -2.4121e-01,\n",
       "            1.1283e-01,  8.4079e-01],\n",
       "          [ 2.3519e-01, -3.5414e-01,  1.3599e-01,  ..., -2.3968e-01,\n",
       "            1.1245e-01,  8.3781e-01],\n",
       "          [ 8.9699e-01, -7.7710e-01, -7.2857e-01,  ...,  5.7896e-01,\n",
       "            1.3574e+00, -2.9390e-01],\n",
       "          ...,\n",
       "          [ 4.6640e-01,  3.9425e-01,  1.2766e+00,  ...,  5.3157e-01,\n",
       "            8.0794e-01, -4.3597e-01],\n",
       "          [ 1.3247e-01,  2.8365e-01,  5.8115e-01,  ...,  7.3939e-01,\n",
       "            3.8827e-01, -1.1408e-01],\n",
       "          [ 8.0556e-01,  6.4216e-02,  2.7608e-01,  ...,  4.0155e-01,\n",
       "            5.6515e-01, -6.7531e-01]],\n",
       "\n",
       "         [[ 1.3664e-01,  4.1051e-01, -2.0304e-01,  ...,  1.0597e+00,\n",
       "           -8.7673e-04, -4.9457e-01],\n",
       "          [ 1.3596e-01,  4.0813e-01, -2.0261e-01,  ...,  1.0552e+00,\n",
       "           -1.1524e-03, -4.9324e-01],\n",
       "          [ 2.7554e-01, -8.9001e-01, -5.5739e-01,  ...,  1.3892e+00,\n",
       "            1.5639e+00, -7.6106e-01],\n",
       "          ...,\n",
       "          [ 1.8755e-01, -2.1810e-01, -7.3359e-02,  ..., -6.8917e-01,\n",
       "           -3.4791e-01,  4.8295e-01],\n",
       "          [ 9.4435e-02, -5.5493e-01, -6.1208e-01,  ..., -1.4558e-01,\n",
       "            1.2389e-01,  2.5773e-02],\n",
       "          [ 3.8889e-01,  5.5978e-01,  2.0278e-01,  ...,  1.0324e+00,\n",
       "            4.1268e-01,  1.8212e-02]],\n",
       "\n",
       "         [[ 5.0739e-01,  2.1645e-02,  4.2266e-03,  ...,  3.9736e-01,\n",
       "            7.0873e-01,  1.0457e+00],\n",
       "          [ 5.0535e-01,  2.0214e-02,  3.8832e-03,  ...,  3.9651e-01,\n",
       "            7.0691e-01,  1.0423e+00],\n",
       "          [-2.3295e-01,  1.0281e+00,  5.8924e-01,  ..., -2.4755e+00,\n",
       "           -4.9640e-01,  1.5208e+00],\n",
       "          ...,\n",
       "          [-1.5578e+00, -8.6772e-02,  3.3438e-01,  ...,  1.0228e+00,\n",
       "            3.7775e-01, -2.8179e-01],\n",
       "          [-9.5726e-01,  3.7753e-01,  2.1975e-01,  ...,  7.0207e-02,\n",
       "            2.1476e-01,  2.4331e-01],\n",
       "          [-4.1846e-02, -1.1708e+00, -3.9650e-01,  ..., -7.0120e-01,\n",
       "           -3.2791e-01, -5.0446e-02]]]], grad_fn=<CloneBackward0>), tensor([[[[ 7.8307e-02,  5.1141e-03, -5.5587e-02,  ...,  4.7830e-02,\n",
       "           -3.3416e-02, -2.7312e-02],\n",
       "          [ 7.8298e-02,  4.7612e-03, -5.5864e-02,  ...,  4.8049e-02,\n",
       "           -3.3946e-02, -2.7388e-02],\n",
       "          [-1.0738e+00,  2.0048e-01, -3.5074e-02,  ...,  3.0170e-01,\n",
       "           -8.0337e-02, -1.9596e-01],\n",
       "          ...,\n",
       "          [-3.8992e-01,  1.5900e-01, -4.0389e-01,  ...,  8.1764e-01,\n",
       "           -5.1418e-02,  7.4102e-01],\n",
       "          [-4.3447e-01,  6.0840e-01,  6.1409e-01,  ...,  2.9387e-01,\n",
       "           -1.5310e-01,  9.7662e-01],\n",
       "          [ 3.2177e-01, -2.3245e-01, -1.5262e-02,  ..., -1.1703e-01,\n",
       "           -6.8018e-01, -8.4584e-02]],\n",
       "\n",
       "         [[-7.5751e-03, -1.7170e-02,  3.4078e-02,  ..., -2.8686e-02,\n",
       "            1.9499e-03, -1.9276e-02],\n",
       "          [-7.5715e-03, -1.7345e-02,  3.3886e-02,  ..., -2.8705e-02,\n",
       "            1.6975e-03, -1.9099e-02],\n",
       "          [ 6.2391e-01,  3.2421e-01, -3.1777e-01,  ..., -4.5532e-01,\n",
       "           -1.1876e+00, -1.8351e-01],\n",
       "          ...,\n",
       "          [ 2.4995e-01,  7.3973e-02, -2.0323e-01,  ..., -4.3040e-01,\n",
       "           -6.4248e-01, -4.6946e-02],\n",
       "          [ 1.2936e+00,  3.8097e-01,  3.1753e-01,  ..., -3.7130e-01,\n",
       "           -8.5843e-01,  4.0464e-01],\n",
       "          [-3.4472e-01,  1.0196e+00,  8.6430e-01,  ..., -1.1153e-01,\n",
       "            9.9354e-01, -6.1098e-01]],\n",
       "\n",
       "         [[-5.3739e-03, -1.0894e-02, -1.2822e-02,  ..., -5.1351e-03,\n",
       "            1.1449e-02,  4.9906e-03],\n",
       "          [-5.5214e-03, -1.0681e-02, -1.2766e-02,  ..., -5.2241e-03,\n",
       "            1.1328e-02,  4.6258e-03],\n",
       "          [-2.0753e-01,  1.7798e-01, -2.1323e-01,  ...,  1.4061e+00,\n",
       "            9.1904e-01,  8.2547e-01],\n",
       "          ...,\n",
       "          [ 4.8372e-01,  1.0818e+00,  8.3542e-01,  ...,  8.5797e-01,\n",
       "            1.0462e+00, -1.7748e-01],\n",
       "          [-5.9411e-01,  9.5172e-01,  5.0860e-01,  ...,  9.4321e-01,\n",
       "            1.1515e+00,  1.2019e+00],\n",
       "          [ 9.5727e-01, -4.7648e-02, -1.2269e-01,  ..., -2.2382e-01,\n",
       "            6.9889e-02, -1.0422e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2130e-02, -2.4152e-02, -5.5883e-02,  ..., -1.3453e-02,\n",
       "            1.7605e-02,  4.0368e-04],\n",
       "          [ 2.1768e-02, -2.4192e-02, -5.5898e-02,  ..., -1.3539e-02,\n",
       "            1.7875e-02,  3.3854e-04],\n",
       "          [ 6.1098e-01, -1.8785e-01,  4.4954e-01,  ..., -7.2637e-01,\n",
       "           -6.8463e-01, -1.7321e-01],\n",
       "          ...,\n",
       "          [-6.8175e-02,  1.5764e-03,  1.6985e-01,  ...,  5.6709e-02,\n",
       "           -3.9239e-01,  8.9507e-02],\n",
       "          [ 2.6198e-01,  4.2744e-02,  3.4022e-01,  ...,  1.5166e-01,\n",
       "           -9.2566e-01,  4.7028e-01],\n",
       "          [ 3.5382e-01, -5.1514e-01, -5.2383e-02,  ..., -3.3974e-01,\n",
       "           -2.2388e-01,  6.7656e-01]],\n",
       "\n",
       "         [[ 1.1837e-01,  6.8162e-02, -1.1942e-02,  ...,  1.8835e-02,\n",
       "            2.8594e-02,  5.4411e-02],\n",
       "          [ 1.1823e-01,  6.7849e-02, -1.2199e-02,  ...,  1.8656e-02,\n",
       "            2.8916e-02,  5.4149e-02],\n",
       "          [ 8.5505e-02, -2.8878e-01,  3.2264e-01,  ..., -2.8435e-01,\n",
       "            1.3568e-02,  2.0891e-01],\n",
       "          ...,\n",
       "          [-7.6207e-01, -5.9650e-01, -1.1624e-01,  ..., -4.6307e-02,\n",
       "           -7.5684e-01,  6.0244e-01],\n",
       "          [-5.0402e-01, -8.5380e-01,  8.3149e-01,  ...,  5.2584e-01,\n",
       "            1.2261e-01,  2.9108e-01],\n",
       "          [-3.5197e-01, -4.4961e-01,  5.3156e-01,  ..., -5.5306e-02,\n",
       "            6.7036e-01, -3.9574e-01]],\n",
       "\n",
       "         [[-3.6058e-02, -1.3084e-02,  3.2308e-03,  ..., -2.9630e-02,\n",
       "           -2.3138e-02, -6.9157e-02],\n",
       "          [-3.6355e-02, -1.2958e-02,  3.3276e-03,  ..., -2.9710e-02,\n",
       "           -2.3516e-02, -6.9244e-02],\n",
       "          [-8.1144e-01, -1.3894e-01,  4.5574e-01,  ...,  2.5388e-01,\n",
       "           -3.2074e-01, -7.9765e-01],\n",
       "          ...,\n",
       "          [ 1.6599e-01, -3.1800e-01,  1.5635e-01,  ..., -2.6318e-01,\n",
       "            2.6149e-01, -2.1058e-01],\n",
       "          [ 4.1034e-01,  3.2142e-01,  3.5186e-01,  ...,  9.4573e-02,\n",
       "           -1.1775e-01,  1.5579e-02],\n",
       "          [ 9.3593e-02, -1.7243e-01,  4.5396e-01,  ...,  4.8652e-01,\n",
       "           -1.8462e-02,  6.7213e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[ 1.9638e-01, -1.2013e-02, -9.9801e-01,  ...,  7.7553e-02,\n",
       "            9.9888e-02,  1.7252e-01],\n",
       "          [-2.3611e+00,  1.0837e+00,  3.4206e+00,  ...,  1.0625e+00,\n",
       "            1.5995e-01, -6.3216e-01],\n",
       "          [-3.2280e+00,  3.4234e-01,  2.9371e+00,  ...,  5.5421e-01,\n",
       "           -1.1506e-01, -1.6933e+00],\n",
       "          ...,\n",
       "          [-1.8506e+00,  1.6273e+00,  4.3250e+00,  ...,  3.8337e-02,\n",
       "            3.9370e-01, -1.5452e-01],\n",
       "          [-1.6218e+00,  1.1368e+00,  3.9177e+00,  ...,  1.1933e+00,\n",
       "           -2.3220e-01, -4.4329e-01],\n",
       "          [-2.3054e+00,  6.3722e-01,  3.2980e+00,  ...,  1.7687e+00,\n",
       "            4.9169e-01, -1.4810e+00]],\n",
       "\n",
       "         [[ 2.6172e-01,  2.2905e-01, -1.3140e-01,  ...,  3.1257e-01,\n",
       "           -7.2847e-02,  1.6751e-01],\n",
       "          [-9.2530e-01, -1.8504e-02, -8.1002e-01,  ..., -1.2196e-01,\n",
       "            1.0691e-01, -1.9151e-01],\n",
       "          [-9.8738e-01, -3.7364e-01,  6.4836e-01,  ..., -1.7091e+00,\n",
       "            2.1972e-01,  2.4354e-01],\n",
       "          ...,\n",
       "          [ 2.3893e-01, -6.0830e-01, -1.4072e+00,  ..., -1.0772e+00,\n",
       "            7.6566e-01, -5.1329e-01],\n",
       "          [-9.6358e-01, -3.3280e-01, -2.5778e-01,  ..., -9.1344e-01,\n",
       "            1.6751e+00, -1.2153e+00],\n",
       "          [ 1.7788e-01, -3.1692e-01, -3.3597e-01,  ..., -7.0615e-01,\n",
       "            1.2857e+00,  1.2751e-01]],\n",
       "\n",
       "         [[ 3.8982e-01,  9.5744e-02,  9.8538e-02,  ..., -2.7862e-01,\n",
       "           -7.5469e-02,  1.6630e-01],\n",
       "          [-2.4663e+00,  5.8621e-01,  1.4790e-01,  ...,  6.7767e-01,\n",
       "           -9.7232e-01,  2.2031e-02],\n",
       "          [-1.6356e+00,  7.3098e-01,  3.1418e-01,  ...,  1.6933e+00,\n",
       "           -4.5581e-02,  8.6568e-01],\n",
       "          ...,\n",
       "          [-2.3238e+00, -1.4147e+00,  2.3455e+00,  ...,  3.8541e-01,\n",
       "           -3.0758e-01, -7.3651e-01],\n",
       "          [-2.3347e+00, -1.2157e+00,  1.5523e+00,  ...,  1.1254e+00,\n",
       "            2.0670e-01,  7.3327e-01],\n",
       "          [-2.2567e+00, -1.2693e+00,  1.5643e+00,  ...,  1.1379e+00,\n",
       "           -7.4472e-01, -5.0752e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0912e-01, -4.1371e-02, -5.0827e-02,  ...,  6.5793e-02,\n",
       "           -1.5733e-01,  1.4854e-01],\n",
       "          [ 3.5529e-01,  3.7429e-02,  2.1870e-01,  ..., -1.1293e-01,\n",
       "            3.0425e-01,  1.2132e-01],\n",
       "          [-3.1695e-01, -8.0790e-01,  2.8133e+00,  ..., -7.0138e-01,\n",
       "            6.4363e-01,  1.7643e-01],\n",
       "          ...,\n",
       "          [-2.9216e-01,  2.2638e+00,  1.1797e-01,  ...,  8.5914e-02,\n",
       "           -2.2318e-01, -7.7563e-02],\n",
       "          [ 2.7737e-01, -7.5729e-02,  2.5144e+00,  ...,  6.4986e-01,\n",
       "           -1.6545e-01, -1.0455e+00],\n",
       "          [-1.6875e-01,  8.8889e-02, -2.0476e-01,  ..., -1.9440e-01,\n",
       "            9.6915e-02,  3.3367e-01]],\n",
       "\n",
       "         [[-6.8864e-02, -7.6506e-02,  7.7103e-02,  ...,  1.8827e-01,\n",
       "           -5.8909e-02, -7.6313e-01],\n",
       "          [-1.6387e-01, -1.5352e-01,  6.2438e-01,  ...,  1.4394e-01,\n",
       "            5.3449e-01,  2.0659e+00],\n",
       "          [ 1.0921e-02, -1.1334e-01,  1.0749e-01,  ...,  1.6472e+00,\n",
       "            1.8990e+00,  5.7002e-01],\n",
       "          ...,\n",
       "          [ 2.7430e-01, -6.1211e-02, -6.2525e-02,  ...,  5.9556e-01,\n",
       "            8.7288e-01,  1.6466e+00],\n",
       "          [ 5.5245e-03, -5.0202e-01, -5.2395e-02,  ...,  7.5166e-01,\n",
       "            1.5773e+00,  6.8047e-01],\n",
       "          [ 9.5307e-01, -2.1417e-01, -7.7366e-01,  ...,  4.1937e-01,\n",
       "            2.0720e+00,  2.0160e+00]],\n",
       "\n",
       "         [[-1.9715e-02, -1.9797e-01,  8.1792e-05,  ..., -1.1796e-01,\n",
       "           -1.5730e-01, -1.7517e-02],\n",
       "          [ 5.9409e-01,  9.3508e-01, -5.6446e-01,  ..., -1.0766e+00,\n",
       "           -1.7175e+00, -7.8023e-01],\n",
       "          [ 1.3386e+00,  9.7642e-01,  3.6842e-01,  ..., -2.2708e+00,\n",
       "           -1.4367e+00,  1.4347e-01],\n",
       "          ...,\n",
       "          [ 1.8706e+00, -5.9086e-01, -5.3321e-01,  ...,  9.1144e-02,\n",
       "            1.6831e-01, -1.0045e+00],\n",
       "          [ 1.9653e+00,  4.2324e-01, -7.9629e-01,  ..., -7.4824e-01,\n",
       "           -1.3531e+00, -4.1344e-01],\n",
       "          [ 9.3689e-01,  4.5085e-01,  6.2123e-03,  ..., -8.7563e-01,\n",
       "           -9.1429e-02, -1.0953e+00]]]], grad_fn=<CloneBackward0>), tensor([[[[ 2.3315e-03, -6.5336e-03, -1.0173e-03,  ...,  2.4768e-03,\n",
       "           -9.0890e-04, -4.2734e-03],\n",
       "          [ 7.0197e-01, -3.1692e-01, -4.7294e-02,  ...,  2.2311e-01,\n",
       "           -2.6331e-01,  2.3280e-01],\n",
       "          [ 9.7530e-01,  2.7394e-01, -1.1726e-01,  ...,  4.0817e-01,\n",
       "            3.3910e-01,  5.6793e-01],\n",
       "          ...,\n",
       "          [ 2.4235e-01, -1.4236e-03, -4.1796e-02,  ...,  2.1202e-01,\n",
       "            6.6446e-01, -2.7266e-01],\n",
       "          [-3.0727e-01, -2.2476e-01,  9.7351e-01,  ..., -4.8785e-01,\n",
       "            1.1892e-01,  6.8502e-01],\n",
       "          [ 3.0206e-01,  2.0013e-01,  1.5636e-01,  ...,  2.1060e-01,\n",
       "           -1.3157e-01,  2.9703e-01]],\n",
       "\n",
       "         [[-5.3506e-03,  3.7707e-03, -4.2116e-03,  ...,  2.2810e-03,\n",
       "           -2.5115e-03, -4.0327e-03],\n",
       "          [-1.0983e+00, -1.0948e+00, -6.2181e-02,  ..., -3.3001e-01,\n",
       "            2.9139e-02,  7.4004e-01],\n",
       "          [-2.7462e-01,  9.0407e-01,  5.4779e-01,  ..., -1.7487e+00,\n",
       "           -5.2494e-01, -2.4078e-01],\n",
       "          ...,\n",
       "          [-8.0952e-01, -4.1343e-01, -2.9213e-01,  ..., -5.6615e-01,\n",
       "           -3.7193e-01,  6.4463e-02],\n",
       "          [-9.3220e-01,  1.3517e-01,  6.1450e-01,  ..., -1.3937e+00,\n",
       "            3.9612e-01,  5.7312e-01],\n",
       "          [-2.5326e-01, -5.2584e-01, -2.2154e-02,  ...,  2.1444e-01,\n",
       "            3.7462e-01,  4.1257e-01]],\n",
       "\n",
       "         [[ 7.7331e-03,  2.2394e-03,  6.0514e-03,  ...,  9.8464e-04,\n",
       "            1.7076e-03,  9.1721e-04],\n",
       "          [ 8.8949e-03, -1.6076e+00, -1.1402e+00,  ..., -1.2882e+00,\n",
       "           -1.6673e-01,  6.8217e-01],\n",
       "          [-7.7268e-02, -1.8112e+00, -1.5726e-02,  ..., -1.3807e+00,\n",
       "            6.4781e-01,  9.3238e-01],\n",
       "          ...,\n",
       "          [ 5.5471e-01, -1.9669e-01, -4.7247e-01,  ..., -2.7984e-01,\n",
       "            2.3049e-01,  5.9558e-01],\n",
       "          [ 1.2542e+00, -1.5351e+00,  2.8084e-02,  ..., -1.2786e+00,\n",
       "           -4.0053e-01,  7.1670e-01],\n",
       "          [ 4.5965e-01, -1.0072e+00, -1.0389e+00,  ..., -1.1957e+00,\n",
       "           -2.4178e-01,  3.9573e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.7242e-03, -4.2245e-03,  9.6411e-05,  ..., -4.4479e-03,\n",
       "           -2.9221e-04,  3.6490e-03],\n",
       "          [ 1.4417e-01,  5.3485e-02, -1.2957e+00,  ...,  8.8461e-01,\n",
       "           -9.7883e-01,  2.8524e-01],\n",
       "          [ 1.1328e+00,  5.7795e-01, -2.3076e+00,  ..., -2.1657e-01,\n",
       "            1.3563e+00,  4.9834e-01],\n",
       "          ...,\n",
       "          [-4.9618e-01, -4.0116e-01, -5.5950e-01,  ...,  4.3203e-01,\n",
       "           -2.1019e-02, -1.2768e-01],\n",
       "          [ 8.8758e-01, -3.8701e-01,  9.5374e-01,  ...,  6.3988e-01,\n",
       "           -7.8901e-01,  7.8285e-01],\n",
       "          [ 7.2834e-01, -2.2705e-01, -4.6864e-01,  ...,  2.5152e-01,\n",
       "            2.5819e-01,  6.0149e-01]],\n",
       "\n",
       "         [[-1.6211e-03,  1.2145e-03, -4.6349e-03,  ...,  2.9642e-03,\n",
       "           -1.3554e-03,  2.1290e-03],\n",
       "          [ 3.0953e-02,  1.0354e-01, -1.3458e+00,  ..., -1.3374e+00,\n",
       "           -8.8207e-02, -1.4633e+00],\n",
       "          [-8.4870e-01,  1.0405e-01,  2.7339e-01,  ..., -5.1269e-01,\n",
       "           -1.2239e+00, -2.9116e-01],\n",
       "          ...,\n",
       "          [-2.3962e-01,  2.1613e-01, -2.1088e-02,  ..., -1.2220e+00,\n",
       "            2.4318e-01,  1.4055e-01],\n",
       "          [-8.6474e-01, -6.6156e-01,  2.4164e-02,  ..., -4.6728e-01,\n",
       "           -9.4564e-01,  3.0493e-01],\n",
       "          [ 6.2352e-01, -4.6017e-01, -1.2123e+00,  ..., -7.7867e-01,\n",
       "            7.6844e-01, -4.6891e-01]],\n",
       "\n",
       "         [[-1.2698e-02,  7.1049e-04, -9.6848e-03,  ..., -9.7750e-03,\n",
       "            4.6410e-03,  2.0134e-03],\n",
       "          [ 1.1618e+00,  6.3828e-01,  2.5593e-02,  ...,  5.5417e-01,\n",
       "           -9.6741e-01,  1.2636e+00],\n",
       "          [ 1.1705e+00,  9.1238e-01,  2.1309e+00,  ..., -1.5031e-01,\n",
       "            1.1024e-01,  1.3375e+00],\n",
       "          ...,\n",
       "          [ 9.8156e-01,  1.2002e+00,  5.8031e-01,  ...,  7.4442e-01,\n",
       "           -9.7354e-01,  6.0671e-01],\n",
       "          [ 1.0438e+00,  1.3792e+00,  1.1784e+00,  ..., -4.0062e-01,\n",
       "           -2.2676e-01,  1.3999e+00],\n",
       "          [ 4.7332e-01, -1.2160e-01, -1.4650e-01,  ..., -9.1078e-01,\n",
       "           -3.7062e-01,  1.5889e-02]]]], grad_fn=<CloneBackward0>), tensor([[[[-3.9109e-01, -1.9972e-01, -4.1116e-01,  ..., -5.0645e-01,\n",
       "            3.8878e-01,  2.6983e-01],\n",
       "          [-3.8986e-01, -1.9941e-01, -4.0897e-01,  ..., -5.0394e-01,\n",
       "            3.8612e-01,  2.6971e-01],\n",
       "          [ 9.1928e-01, -5.2453e-01,  1.0543e+00,  ...,  1.1235e-03,\n",
       "           -1.4081e-01, -1.5056e-01],\n",
       "          ...,\n",
       "          [ 1.0318e+00, -5.4488e-02,  1.3002e+00,  ..., -4.5472e-01,\n",
       "           -7.7557e-01, -7.5430e-01],\n",
       "          [ 1.4962e+00, -7.4849e-01,  8.0491e-01,  ..., -7.3058e-01,\n",
       "           -5.8206e-01, -4.3702e-01],\n",
       "          [ 6.3342e-01, -6.9004e-01,  2.3524e-03,  ...,  9.7656e-01,\n",
       "           -3.2803e-01, -3.3789e-01]],\n",
       "\n",
       "         [[-1.3543e-03,  7.2680e-02, -5.4713e-01,  ...,  5.4779e-02,\n",
       "           -2.6860e-01,  2.3494e-01],\n",
       "          [-3.4171e-04,  7.1474e-02, -5.4211e-01,  ...,  5.3882e-02,\n",
       "           -2.6614e-01,  2.3576e-01],\n",
       "          [-1.9367e-01, -2.8676e-02,  6.3485e-01,  ..., -1.0765e+00,\n",
       "            6.0647e-01,  5.3686e-01],\n",
       "          ...,\n",
       "          [ 9.7015e-01, -6.3849e-01,  8.8757e-01,  ..., -1.4180e+00,\n",
       "            3.8416e-01,  7.6252e-01],\n",
       "          [ 8.6284e-02, -6.2211e-01,  1.3458e+00,  ..., -1.0679e+00,\n",
       "            3.4535e-01,  6.6150e-01],\n",
       "          [ 6.2327e-01, -1.7240e-01, -1.0652e+00,  ..., -9.6337e-03,\n",
       "           -3.5837e-01,  5.3656e-01]],\n",
       "\n",
       "         [[ 1.0812e-01,  6.5839e-02, -6.8781e-02,  ..., -8.0515e-01,\n",
       "           -1.7602e-01,  9.9593e-02],\n",
       "          [ 1.0759e-01,  6.5303e-02, -6.8275e-02,  ..., -8.0386e-01,\n",
       "           -1.7437e-01,  9.8862e-02],\n",
       "          [ 3.0791e-01,  9.9146e-01,  5.3386e-01,  ...,  1.7805e-01,\n",
       "            3.0381e+00,  3.7540e-01],\n",
       "          ...,\n",
       "          [-9.8857e-01, -7.4571e-01,  8.6384e-01,  ..., -1.6102e-01,\n",
       "            5.2906e-01,  6.0180e-01],\n",
       "          [-5.1175e-01, -1.7031e-01,  1.1401e-01,  ...,  2.7134e-01,\n",
       "            1.2164e+00, -2.1735e-01],\n",
       "          [-5.8985e-01, -6.7887e-01,  7.5204e-01,  ..., -1.2215e-01,\n",
       "            5.4121e-01, -6.6693e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.8052e-01, -1.9524e-01,  2.7559e-01,  ..., -4.6711e-01,\n",
       "           -1.4586e-01,  3.0836e-01],\n",
       "          [-5.7748e-01, -1.9395e-01,  2.7365e-01,  ..., -4.6567e-01,\n",
       "           -1.4442e-01,  3.0834e-01],\n",
       "          [ 9.0972e-01,  6.7356e-02,  7.2318e-01,  ...,  6.4940e-01,\n",
       "            1.4859e+00,  9.9388e-01],\n",
       "          ...,\n",
       "          [ 1.7742e-01,  6.1124e-01, -9.2173e-01,  ..., -5.4325e-01,\n",
       "            5.7717e-01,  4.6564e-01],\n",
       "          [ 1.6468e+00, -5.0352e-01,  9.3333e-01,  ...,  5.6498e-02,\n",
       "            1.0750e+00, -1.4068e-02],\n",
       "          [-1.5232e-01, -3.8653e-01, -7.2653e-01,  ..., -3.1007e-01,\n",
       "           -2.1499e-03,  2.7475e-01]],\n",
       "\n",
       "         [[ 9.9632e-02,  1.5003e-01,  2.8986e-01,  ...,  3.4622e-01,\n",
       "            1.7759e-01,  3.1161e-01],\n",
       "          [ 9.7389e-02,  1.5051e-01,  2.8822e-01,  ...,  3.4315e-01,\n",
       "            1.7814e-01,  3.1035e-01],\n",
       "          [ 1.3813e-01,  8.3585e-01,  4.1428e-02,  ...,  9.1023e-01,\n",
       "            9.0158e-01, -2.5764e+00],\n",
       "          ...,\n",
       "          [-9.5355e-01,  9.3855e-01, -7.2865e-01,  ..., -1.6425e+00,\n",
       "            8.0767e-01, -9.2536e-01],\n",
       "          [ 3.5847e-01,  9.2917e-02, -2.5349e-01,  ..., -2.3624e-01,\n",
       "            1.6513e+00, -2.2001e+00],\n",
       "          [-5.3580e-02,  6.1230e-01, -7.1128e-01,  ...,  1.5110e-01,\n",
       "            6.4074e-01, -9.7865e-02]],\n",
       "\n",
       "         [[-2.7911e-01, -2.9812e-01,  1.8314e-01,  ..., -2.9879e-01,\n",
       "            8.0974e-02,  3.9533e-01],\n",
       "          [-2.7788e-01, -2.9726e-01,  1.8332e-01,  ..., -2.9733e-01,\n",
       "            8.0889e-02,  3.9264e-01],\n",
       "          [-3.7107e-01, -7.5661e-01,  7.6687e-01,  ..., -7.2640e-02,\n",
       "           -1.0136e-01,  4.8561e-01],\n",
       "          ...,\n",
       "          [-5.6262e-02, -3.4411e-01,  5.6781e-02,  ...,  9.6755e-01,\n",
       "           -1.6591e-01, -5.8159e-01],\n",
       "          [-4.8138e-01, -4.2550e-01,  7.3393e-01,  ...,  2.9139e-01,\n",
       "            4.3740e-01, -8.6674e-01],\n",
       "          [ 8.5169e-01,  2.0410e-01,  1.6889e-01,  ..., -7.4340e-01,\n",
       "           -7.1812e-01, -5.0348e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.4546e-02, -7.2161e-03, -1.6084e-02,  ..., -3.4336e-02,\n",
       "           -1.6239e-01, -1.3550e-02],\n",
       "          [ 1.4586e-02, -7.0612e-03, -1.5908e-02,  ..., -3.3960e-02,\n",
       "           -1.6406e-01, -1.3554e-02],\n",
       "          [-1.0482e+00,  4.9548e-01,  7.4722e-01,  ...,  5.8852e-01,\n",
       "           -2.5718e-01,  2.9555e-01],\n",
       "          ...,\n",
       "          [-7.9576e-01, -3.5885e-01,  1.0263e-01,  ..., -5.5390e-02,\n",
       "           -8.5560e-01,  5.2166e-04],\n",
       "          [-6.8714e-01, -4.6694e-02,  8.1742e-01,  ...,  5.8774e-01,\n",
       "           -6.7880e-01,  5.3481e-01],\n",
       "          [-5.3329e-01,  5.8378e-01,  6.3561e-02,  ..., -8.5913e-03,\n",
       "           -1.0451e+00, -8.8406e-01]],\n",
       "\n",
       "         [[-8.3152e-02, -9.0031e-03, -4.3089e-02,  ...,  1.4346e-02,\n",
       "           -4.0324e-03, -2.7426e-02],\n",
       "          [-8.2719e-02, -8.8754e-03, -4.3040e-02,  ...,  1.4214e-02,\n",
       "           -4.1358e-03, -2.7181e-02],\n",
       "          [-2.2438e-01, -2.1744e-01, -3.1507e-02,  ..., -2.7871e-01,\n",
       "           -4.1306e-01, -2.4665e-02],\n",
       "          ...,\n",
       "          [-8.2910e-02,  1.3584e+00,  3.3732e-02,  ..., -1.9112e-01,\n",
       "           -2.1243e-01,  4.4529e-01],\n",
       "          [-7.5154e-01,  7.9966e-01, -3.1493e-01,  ..., -3.0927e-02,\n",
       "           -3.2598e-01, -2.4759e-01],\n",
       "          [-2.6439e-02,  5.2963e-01,  1.9001e-01,  ...,  2.3486e-01,\n",
       "            5.2069e-02,  3.3304e-01]],\n",
       "\n",
       "         [[ 1.0289e-02,  3.0383e-02, -2.0575e-01,  ...,  6.8216e-02,\n",
       "           -8.1783e-02, -4.3272e-02],\n",
       "          [ 1.0333e-02,  3.0337e-02, -2.0606e-01,  ...,  6.8173e-02,\n",
       "           -8.1783e-02, -4.3523e-02],\n",
       "          [-6.7532e-01, -2.9955e-01, -7.2015e-01,  ...,  6.6624e-01,\n",
       "            1.0665e-01, -1.0514e-01],\n",
       "          ...,\n",
       "          [ 3.3208e-02,  5.8283e-01, -1.2684e-01,  ..., -2.0810e-01,\n",
       "           -3.1604e-01,  9.4584e-01],\n",
       "          [ 6.9662e-03, -2.4417e-01, -2.8213e-01,  ...,  5.9541e-02,\n",
       "            2.2968e-01,  1.0504e+00],\n",
       "          [-4.0538e-02, -1.9497e-01, -4.8705e-01,  ...,  2.9446e-01,\n",
       "           -9.0886e-02,  1.2927e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.4923e-02, -8.5197e-03, -1.8308e-02,  ...,  1.3814e-02,\n",
       "           -6.1458e-03,  2.6890e-02],\n",
       "          [-1.4946e-02, -8.6387e-03, -1.8131e-02,  ...,  1.3892e-02,\n",
       "           -6.0466e-03,  2.6739e-02],\n",
       "          [-1.3487e-01, -3.7033e-02, -1.9702e-01,  ...,  2.9928e-01,\n",
       "            5.8424e-01, -2.3826e-01],\n",
       "          ...,\n",
       "          [ 2.5233e-01, -5.0061e-01, -1.3130e-01,  ...,  6.0540e-02,\n",
       "            1.8874e-01, -3.9753e-02],\n",
       "          [ 5.7418e-02,  8.9471e-03,  3.0538e-01,  ...,  5.0276e-01,\n",
       "           -7.7745e-02, -2.8145e-01],\n",
       "          [-5.6818e-02,  1.0546e-01, -2.7790e-02,  ...,  3.3517e-02,\n",
       "            3.8887e-01,  2.8474e-01]],\n",
       "\n",
       "         [[-2.4397e-02,  4.1131e-03, -9.5681e-03,  ...,  9.3442e-03,\n",
       "           -8.5328e-03, -2.2613e-02],\n",
       "          [-2.4172e-02,  4.0919e-03, -9.2967e-03,  ...,  9.0557e-03,\n",
       "           -8.5011e-03, -2.2225e-02],\n",
       "          [-1.3084e-01,  4.2507e-01,  1.7754e-01,  ..., -4.7574e-01,\n",
       "            5.4741e-01,  4.6382e-02],\n",
       "          ...,\n",
       "          [ 9.3250e-02, -1.0530e-01, -1.3188e-01,  ..., -3.0672e-01,\n",
       "            4.5079e-01,  3.9748e-01],\n",
       "          [ 2.6999e-01,  8.7832e-01, -7.5431e-02,  ..., -1.6050e-01,\n",
       "            5.2195e-01,  3.3953e-01],\n",
       "          [ 2.4869e-01, -2.4068e-01, -2.1085e-01,  ...,  2.7782e-01,\n",
       "            1.8390e-01, -4.6677e-02]],\n",
       "\n",
       "         [[-3.1041e-03, -3.8398e-02,  9.2227e-04,  ...,  1.9587e-03,\n",
       "            3.5053e-02, -2.4527e-02],\n",
       "          [-2.9623e-03, -3.8516e-02,  7.9902e-04,  ...,  1.8536e-03,\n",
       "            3.5211e-02, -2.4365e-02],\n",
       "          [ 6.7829e-01, -2.1354e-01,  2.3520e-02,  ..., -1.4279e+00,\n",
       "            1.2840e-01,  5.5805e-01],\n",
       "          ...,\n",
       "          [ 3.1572e-01,  3.6993e-01,  1.3589e+00,  ..., -3.7934e-01,\n",
       "           -2.9314e-01, -4.6661e-01],\n",
       "          [ 6.5247e-02,  8.3647e-01, -7.7610e-02,  ..., -7.9563e-01,\n",
       "            6.4834e-01, -5.4962e-01],\n",
       "          [ 2.0955e-01, -3.1712e-01, -4.7822e-01,  ...,  4.5940e-01,\n",
       "           -7.6140e-01, -4.7291e-01]]]], grad_fn=<CloneBackward0>))), decoder_hidden_states=(tensor([[[ 0.0038, -0.2090,  0.0579,  ..., -0.2814,  0.1574, -0.0896],\n",
       "         [ 0.0135, -0.1294,  0.0930,  ..., -0.1309,  0.2236,  0.0425],\n",
       "         [-0.1294,  0.1225,  0.7608,  ...,  1.3444,  0.6627, -0.2707],\n",
       "         ...,\n",
       "         [ 1.0641,  1.1366,  0.3223,  ..., -0.2354, -0.1350, -0.0591],\n",
       "         [ 0.6092,  0.4361, -0.2479,  ..., -0.5145,  0.7043, -0.6137],\n",
       "         [-0.6111, -1.0765,  0.2229,  ...,  0.1325, -0.2265,  0.2552]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0062,  0.0217, -0.0673,  ..., -0.0160,  0.0885, -0.0447],\n",
       "         [-0.0070,  0.0230,  0.0689,  ...,  0.0182,  0.3003, -0.0153],\n",
       "         [ 1.3598, -1.0898,  1.3895,  ...,  1.2122,  0.1885, -0.2103],\n",
       "         ...,\n",
       "         [-0.0681,  0.7502, -0.0982,  ...,  0.3461,  0.0724, -0.2082],\n",
       "         [ 0.8829,  0.1717, -0.0121,  ..., -0.0948,  1.2878, -0.2506],\n",
       "         [ 0.0080, -0.9269,  1.1230,  ...,  1.0309, -0.0182,  0.1640]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0354,  0.0453, -0.0615,  ...,  0.0129,  0.0708, -0.0235],\n",
       "         [-0.1692,  0.4461,  0.0497,  ..., -0.1246,  0.0745,  0.2925],\n",
       "         [ 1.0445, -1.3481,  1.3283,  ...,  0.9415, -0.0186,  0.1760],\n",
       "         ...,\n",
       "         [-1.0369,  0.1924,  0.1074,  ..., -0.0883,  0.7157, -0.3602],\n",
       "         [ 1.0333, -0.3456,  0.1622,  ..., -0.1240,  1.0628, -0.1201],\n",
       "         [-0.1505, -0.6163,  1.8625,  ...,  1.1863,  0.2313,  0.1924]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0080,  0.0023, -0.0568,  ..., -0.0068,  0.0796, -0.0436],\n",
       "         [-0.1904,  0.6728, -0.0190,  ..., -0.1304,  0.4408,  0.1477],\n",
       "         [ 0.7201, -1.3219,  0.8852,  ...,  0.4398,  0.4450,  0.4579],\n",
       "         ...,\n",
       "         [-1.1824,  0.1683, -0.3158,  ...,  0.0968,  0.6130, -0.6090],\n",
       "         [ 1.4341, -0.4202, -0.1136,  ..., -0.2065,  0.7997, -0.3177],\n",
       "         [-0.1092, -0.3657,  1.3617,  ...,  1.1018,  0.4356,  0.2284]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0317, -0.0425, -0.0330,  ..., -0.0265,  0.0148, -0.0380],\n",
       "         [-0.0363,  0.5050, -0.1645,  ..., -0.0700,  0.6213,  0.1486],\n",
       "         [ 0.9187, -1.2692,  1.0935,  ...,  0.8251, -0.0652,  0.3520],\n",
       "         ...,\n",
       "         [-0.9013,  0.1709, -0.3639,  ...,  0.1735,  0.3655, -0.5301],\n",
       "         [ 1.0082, -0.6287, -0.4919,  ..., -0.0494,  0.5316, -0.6907],\n",
       "         [-0.3617, -0.2912,  1.0043,  ...,  0.8622,  0.2270,  0.3539]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 2.7711e-02, -6.9532e-02,  1.0536e-03,  ..., -2.2116e-02,\n",
       "          -2.5357e-03, -2.5351e-02],\n",
       "         [ 2.1373e-01,  2.4677e-01, -2.7140e-01,  ...,  1.5792e-01,\n",
       "           7.9759e-01, -6.5774e-02],\n",
       "         [ 7.7350e-01, -1.2325e+00,  7.0926e-01,  ...,  5.8364e-01,\n",
       "          -1.5073e-01,  4.8915e-01],\n",
       "         ...,\n",
       "         [-5.3875e-01,  2.4103e-01, -5.6863e-01,  ...,  4.6429e-01,\n",
       "           6.3453e-01, -5.2921e-01],\n",
       "         [ 1.1677e+00, -4.4280e-01, -7.7577e-01,  ...,  8.6505e-02,\n",
       "           8.2623e-01, -3.7847e-01],\n",
       "         [-6.3489e-01, -2.7508e-01,  9.8472e-01,  ...,  8.1443e-01,\n",
       "           4.5874e-01,  4.5106e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0301, -0.0596,  0.0158,  ..., -0.0202, -0.0071, -0.0221],\n",
       "         [ 0.3307,  0.0856, -0.6087,  ...,  0.2134,  0.6206, -0.1823],\n",
       "         [ 0.5726, -1.0400,  1.0517,  ...,  0.1967, -0.1466,  0.4104],\n",
       "         ...,\n",
       "         [ 0.0692,  0.4569, -0.7528,  ...,  0.4833,  0.6357, -0.6166],\n",
       "         [ 1.2802, -0.1213, -0.6210,  ...,  0.0387,  0.9695, -0.2609],\n",
       "         [-0.6066, -0.0199,  1.1302,  ...,  0.8197,  0.6008,  0.2841]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0212, -0.0556,  0.0597,  ..., -0.0100, -0.0253, -0.0142],\n",
       "         [ 0.6984,  0.2977, -0.6908,  ...,  0.2241,  0.5303, -0.2113],\n",
       "         [ 0.0722, -1.0841,  0.8717,  ...,  0.1415, -0.0482,  0.4358],\n",
       "         ...,\n",
       "         [ 0.7851,  0.6619, -0.8826,  ...,  0.2957,  0.3430,  0.0071],\n",
       "         [ 1.6104,  0.1554, -0.7207,  ...,  0.2051,  0.9021,  0.1134],\n",
       "         [-0.3600,  0.0304,  1.2648,  ...,  0.2832,  0.5663,  0.4316]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0066, -0.0504,  0.0347,  ..., -0.0272, -0.0319,  0.0139],\n",
       "         [ 0.1930,  0.3873, -0.8910,  ...,  0.3601,  0.6449, -0.2238],\n",
       "         [ 0.3709, -1.1417,  0.8821,  ...,  0.0191, -0.2280,  0.0113],\n",
       "         ...,\n",
       "         [ 0.6673,  0.5990, -0.7206,  ...,  0.0598,  0.5567, -0.0210],\n",
       "         [ 1.6747,  0.0352, -0.5192,  ...,  0.2522,  0.8474,  0.3321],\n",
       "         [-0.0892,  0.3172,  1.1096,  ..., -0.1028,  0.7940, -0.0160]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0150, -0.0513,  0.0243,  ..., -0.0427, -0.0264,  0.0146],\n",
       "         [ 0.0838,  0.1738, -0.6455,  ...,  0.0051,  0.5057, -0.3616],\n",
       "         [ 0.3901, -1.3440,  0.7847,  ..., -0.3519,  0.1837,  0.3336],\n",
       "         ...,\n",
       "         [ 0.8857,  0.3307, -0.8861,  ...,  0.4157,  0.6635, -0.1424],\n",
       "         [ 1.7518, -0.2166, -0.5052,  ...,  0.5434,  0.9383,  0.4869],\n",
       "         [ 0.0621,  0.2685,  0.9617,  ...,  0.0998,  0.5695, -0.0464]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0069, -0.0602,  0.0298,  ..., -0.0293, -0.0248,  0.0169],\n",
       "         [ 0.1399,  0.0908,  0.1202,  ..., -0.0958,  0.3953, -0.2873],\n",
       "         [ 0.4811, -1.1304,  0.7100,  ..., -0.6136, -0.0886,  0.1403],\n",
       "         ...,\n",
       "         [ 0.4601,  0.3531, -0.7147,  ...,  0.3392,  0.8391,  0.1449],\n",
       "         [ 1.5559,  0.0195, -0.3294,  ...,  0.0864,  0.9787,  0.7279],\n",
       "         [ 0.0292,  0.2044,  1.0614,  ..., -0.0839,  0.4776,  0.4108]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0146, -0.0640,  0.0275,  ..., -0.0263, -0.0421,  0.0114],\n",
       "         [ 0.0035,  0.2163,  0.4079,  ..., -0.3880,  0.7157,  0.0064],\n",
       "         [ 0.6908, -1.2792,  0.8263,  ..., -0.7141,  0.0341,  0.1280],\n",
       "         ...,\n",
       "         [ 0.2085,  0.1361, -0.6615,  ...,  0.5127,  0.9667, -0.0397],\n",
       "         [ 1.0990,  0.4180, -0.2369,  ...,  0.2322,  1.2964,  0.6128],\n",
       "         [-0.2781,  0.4488,  0.6407,  ...,  0.0651,  0.4550,  0.1728]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5551, -1.8611,  0.9896,  ..., -0.5664, -0.8344,  0.0317],\n",
       "         [ 0.2866,  0.1888,  0.4884,  ..., -1.1999,  2.0805, -0.3317],\n",
       "         [ 2.1752, -2.4211,  1.7303,  ..., -1.2864,  0.4603, -0.1465],\n",
       "         ...,\n",
       "         [ 0.4252, -0.2099,  0.2378,  ...,  0.1191,  1.2401,  0.0688],\n",
       "         [ 1.8238,  1.4597,  0.7265,  ..., -0.7583,  2.7914,  1.1179],\n",
       "         [-0.6064,  0.6652,  0.6637,  ...,  0.1313,  0.9097, -0.0692]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.0057, -0.0451, -0.0271,  ...,  0.0019, -0.0054,  0.0376],\n",
       "         [-0.0057, -0.0449, -0.0273,  ...,  0.0020, -0.0048,  0.0375],\n",
       "         [ 0.1932, -0.7076,  0.1937,  ...,  0.3464, -0.4557, -0.2646],\n",
       "         ...,\n",
       "         [ 0.3067, -0.2734, -0.2441,  ...,  0.6774,  0.3688,  0.0286],\n",
       "         [ 0.1894,  0.0632, -0.0278,  ...,  0.5632,  0.0625,  0.2552],\n",
       "         [ 0.1851, -0.2994, -0.2106,  ..., -0.0947,  0.3478,  0.3963]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=(tensor([[[ 0.1071, -0.0960,  0.0521,  ...,  0.2915,  0.1229, -0.4346],\n",
       "         [ 0.3002, -0.1339,  0.3452,  ...,  1.2577,  1.0378, -0.2701],\n",
       "         [ 0.4256, -1.1637,  0.9138,  ..., -0.6922,  0.2204, -0.1654],\n",
       "         ...,\n",
       "         [ 0.7620,  0.4470, -0.1648,  ..., -0.6428,  0.9229, -0.5803],\n",
       "         [-0.1697, -1.1507,  0.4515,  ...,  0.0230,  0.5232,  0.3146],\n",
       "         [-0.1200, -0.0520, -0.0052,  ..., -0.3081,  0.6461, -0.1283]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0219,  0.0829,  0.1927,  ...,  0.4198, -0.0791, -0.1076],\n",
       "         [-0.0511, -0.2573,  0.1872,  ...,  1.1256,  0.4547, -0.2551],\n",
       "         [-0.1028, -1.4368,  0.7325,  ..., -0.5143,  0.5010, -0.4702],\n",
       "         ...,\n",
       "         [ 1.1255,  0.1317, -0.6410,  ..., -1.1931,  1.1908, -0.9839],\n",
       "         [-0.7813, -0.6584,  0.2822,  ...,  0.2327, -0.1344, -0.1378],\n",
       "         [-0.1299, -0.0461, -0.0719,  ..., -0.2044,  0.2427,  0.6029]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2465,  0.1634,  0.0348,  ...,  0.0043, -0.1889, -0.3244],\n",
       "         [-0.0978, -0.9067,  0.3744,  ...,  1.4985, -0.0435, -0.4309],\n",
       "         [-0.6922, -1.1495,  0.7301,  ..., -0.6354, -0.2939, -0.8656],\n",
       "         ...,\n",
       "         [ 0.9911, -0.1487, -0.4439,  ..., -0.7348,  1.3533, -1.2159],\n",
       "         [-0.3706, -0.6563,  0.7061,  ...,  0.5476, -0.5368, -0.6964],\n",
       "         [ 0.2175,  0.1324,  0.3520,  ..., -0.4549,  0.2349,  0.3541]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0791,  0.1055,  0.1040,  ..., -0.0540,  0.1407,  0.0337],\n",
       "         [ 0.2204, -0.8008,  0.0605,  ...,  1.2875,  0.0185, -0.3293],\n",
       "         [-0.7048, -0.6783,  0.1694,  ..., -1.0251, -0.8273, -0.5324],\n",
       "         ...,\n",
       "         [ 0.6900, -0.0807, -0.2598,  ..., -0.4885,  1.5166, -0.9038],\n",
       "         [-0.1199, -0.2396,  1.7147,  ...,  0.3324, -0.2201, -0.2537],\n",
       "         [-0.3998,  0.0388,  0.3661,  ..., -0.6429,  0.7727,  0.8215]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0936,  0.0666,  0.0448,  ..., -0.0305,  0.0715,  0.0151],\n",
       "         [ 0.1387, -0.5530, -0.5461,  ...,  0.8473,  0.3758, -0.4174],\n",
       "         [-0.7604, -0.7296, -0.0968,  ..., -0.4887, -0.8128, -0.1969],\n",
       "         ...,\n",
       "         [ 0.0792,  0.0430, -0.2052,  ..., -0.3467,  1.0057, -0.5836],\n",
       "         [-0.5741, -0.8025,  1.1230,  ...,  0.5469, -0.3543, -0.1474],\n",
       "         [-0.5665,  0.2290,  0.5764,  ..., -0.4050,  0.7113,  0.2236]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0737,  0.0626,  0.0232,  ..., -0.0673, -0.0087,  0.0183],\n",
       "         [ 0.1711, -0.7343, -0.4401,  ...,  0.8616,  0.4844, -0.2881],\n",
       "         [-0.8913, -0.6853,  0.1265,  ..., -0.6506, -0.3497,  0.0921],\n",
       "         ...,\n",
       "         [-0.1718, -0.5129, -0.4948,  ..., -0.0458,  1.2280, -0.3026],\n",
       "         [-0.7807, -0.3876,  0.8191,  ...,  0.3613,  0.2108,  0.0144],\n",
       "         [-0.3961,  0.1993,  0.2999,  ..., -0.3723,  0.7895,  0.3354]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0149,  0.0293, -0.0509,  ..., -0.1522, -0.0079,  0.0143],\n",
       "         [ 0.2844, -0.6925, -0.3976,  ...,  0.5258,  0.4393, -0.3497],\n",
       "         [-0.6076, -0.9053, -0.1687,  ..., -0.5752, -0.0829, -0.2236],\n",
       "         ...,\n",
       "         [-0.0215, -0.8607, -0.7128,  ..., -0.2556,  1.3253,  0.0293],\n",
       "         [-0.4070, -0.5349,  0.2191,  ..., -0.0095,  0.2139, -0.0387],\n",
       "         [-0.1659, -0.2104,  0.1390,  ..., -0.1471,  1.0899,  0.3126]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0912, -0.0045, -0.0881,  ..., -0.1125,  0.0120,  0.0055],\n",
       "         [ 0.3988, -0.5807, -0.0742,  ...,  0.3142,  0.5643, -0.1907],\n",
       "         [-0.2116, -0.6278,  0.1197,  ..., -0.3327,  0.2358,  0.1616],\n",
       "         ...,\n",
       "         [-0.1477, -0.9000, -0.5643,  ...,  0.1194,  1.2173,  0.2431],\n",
       "         [-0.0109, -0.2744,  0.2203,  ...,  0.4852,  0.8465,  0.0970],\n",
       "         [-0.1979, -0.4114,  0.0521,  ...,  0.0706,  0.9816,  0.6223]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0504,  0.1067, -0.0767,  ..., -0.0365, -0.0038,  0.0262],\n",
       "         [ 0.5185, -0.3560, -0.1664,  ...,  0.3138,  0.5231, -0.0649],\n",
       "         [ 0.2406, -0.6902,  0.1542,  ..., -0.3904,  0.1062, -0.2439],\n",
       "         ...,\n",
       "         [-0.2655, -0.8364, -0.6283,  ...,  0.4408,  1.3166,  0.5442],\n",
       "         [ 0.1883, -0.4421,  0.1198,  ...,  1.1317,  1.1333,  0.2210],\n",
       "         [-0.1335, -0.2601, -0.1855,  ..., -0.1258,  1.3044,  0.4823]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0586, -0.0972, -0.1673,  ..., -0.0515,  0.0144,  0.0625],\n",
       "         [ 0.3403, -0.1902, -0.0973,  ...,  0.1797,  0.1791,  0.0339],\n",
       "         [ 0.1110, -0.7677,  0.0219,  ..., -0.1965,  0.1918, -0.0754],\n",
       "         ...,\n",
       "         [-0.1353, -0.6447, -0.5337,  ...,  0.8620,  1.0110,  0.0826],\n",
       "         [ 0.0960, -0.6215,  0.1592,  ...,  1.2420,  0.9684,  0.1649],\n",
       "         [ 0.2146, -0.4939, -0.1808,  ..., -0.0889,  1.3190,  0.7016]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0361, -0.0812, -0.1027,  ..., -0.0518, -0.0361,  0.0804],\n",
       "         [ 0.0712, -0.0462, -0.0620,  ..., -0.0166, -0.0093,  0.0920],\n",
       "         [ 0.3952, -0.9624,  0.1137,  ...,  0.0596, -0.1401, -0.1790],\n",
       "         ...,\n",
       "         [ 0.0097,  0.0303, -0.3427,  ...,  1.3678,  0.9314,  0.1488],\n",
       "         [ 0.1832, -0.0225, -0.0948,  ...,  1.4062,  0.6943,  0.2994],\n",
       "         [ 0.3642, -0.5584, -0.5970,  ..., -0.3736,  1.2663,  0.6779]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0219, -0.0704, -0.0439,  ..., -0.0375, -0.1119,  0.0812],\n",
       "         [ 0.0238, -0.0689, -0.0411,  ..., -0.0361, -0.1051,  0.0789],\n",
       "         [ 0.0316, -1.0926,  0.0947,  ...,  0.3242, -0.5642, -0.1313],\n",
       "         ...,\n",
       "         [ 0.1886, -0.4938, -0.1064,  ...,  1.2876,  0.6340, -0.1897],\n",
       "         [ 0.2964, -0.1972, -0.0464,  ...,  1.1530,  0.3421,  0.2441],\n",
       "         [ 0.5471, -0.6370, -0.3706,  ..., -0.3439,  0.9311,  0.7605]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0057, -0.0451, -0.0271,  ...,  0.0019, -0.0054,  0.0376],\n",
       "         [-0.0057, -0.0449, -0.0273,  ...,  0.0020, -0.0048,  0.0375],\n",
       "         [ 0.1932, -0.7076,  0.1937,  ...,  0.3464, -0.4557, -0.2646],\n",
       "         ...,\n",
       "         [ 0.3067, -0.2734, -0.2441,  ...,  0.6774,  0.3688,  0.0286],\n",
       "         [ 0.1894,  0.0632, -0.0278,  ...,  0.5632,  0.0625,  0.2552],\n",
       "         [ 0.1851, -0.2994, -0.2106,  ..., -0.0947,  0.3478,  0.3963]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), encoder_attentions=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model(**pytorch_inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "209d42d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlaxSeq2SeqLMOutput(logits=DeviceArray([[[-3.5004735, -3.2618244, -3.7565782, ..., -3.9783325,\n",
       "               -3.6071012, -2.3760293],\n",
       "              [-7.4865804, -7.159775 , -6.7540746, ..., -6.5293894,\n",
       "               -6.8198233, -6.578088 ],\n",
       "              [-6.832736 , -6.937727 , -6.67869  , ..., -6.662727 ,\n",
       "               -6.6321497, -6.6068544],\n",
       "              ...,\n",
       "              [-7.5577974, -7.112214 , -7.2961287, ..., -7.2895164,\n",
       "               -7.2791457, -6.317009 ],\n",
       "              [-8.066658 , -6.989978 , -6.8684077, ..., -8.239759 ,\n",
       "               -7.054138 , -6.739692 ],\n",
       "              [-6.215106 , -6.2383475, -6.0314503, ..., -5.751667 ,\n",
       "               -6.4203243, -5.491976 ]]], dtype=float32), past_key_values=None, decoder_hidden_states=(DeviceArray([[[ 0.00379658, -0.20896576,  0.05788542, ..., -0.2813811 ,\n",
       "                0.15738876, -0.08956083],\n",
       "              [ 0.01348088, -0.12935533,  0.0930235 , ..., -0.13094474,\n",
       "                0.2236439 ,  0.04246446],\n",
       "              [-0.12939073,  0.12250718,  0.76082724, ...,  1.3444369 ,\n",
       "                0.6627474 , -0.27073112],\n",
       "              ...,\n",
       "              [ 1.0640653 ,  1.1365745 ,  0.32230645, ..., -0.23541376,\n",
       "               -0.13495275, -0.05909568],\n",
       "              [ 0.60915095,  0.43609774, -0.24786648, ..., -0.5145229 ,\n",
       "                0.70426816, -0.61366606],\n",
       "              [-0.6111339 , -1.0765069 ,  0.22289401, ...,  0.13247392,\n",
       "               -0.2264598 ,  0.25519323]]], dtype=float32), DeviceArray([[[-0.00611788,  0.02156191, -0.06736555, ..., -0.01600264,\n",
       "                0.08847235, -0.04473604],\n",
       "              [-0.0065445 ,  0.02334916,  0.06902739, ...,  0.01875495,\n",
       "                0.30007988, -0.01579919],\n",
       "              [ 1.3609298 , -1.0906022 ,  1.3899945 , ...,  1.210545  ,\n",
       "                0.18592526, -0.20939404],\n",
       "              ...,\n",
       "              [-0.06694059,  0.75136083, -0.09644223, ...,  0.34584403,\n",
       "                0.07307255, -0.20893064],\n",
       "              [ 0.8847067 ,  0.17278545, -0.01079247, ..., -0.09488973,\n",
       "                1.2845641 , -0.25118744],\n",
       "              [ 0.0081312 , -0.9260086 ,  1.1224896 , ...,  1.030034  ,\n",
       "               -0.0220956 ,  0.16324803]]], dtype=float32), DeviceArray([[[-0.0352475 ,  0.04528138, -0.06153402, ...,  0.01305256,\n",
       "                0.0708907 , -0.02361811],\n",
       "              [-0.1687176 ,  0.44471595,  0.04947469, ..., -0.12471941,\n",
       "                0.07551924,  0.29104248],\n",
       "              [ 1.0443823 , -1.3488122 ,  1.3285462 , ...,  0.9401314 ,\n",
       "               -0.01944067,  0.17860098],\n",
       "              ...,\n",
       "              [-1.036976  ,  0.19181974,  0.10644065, ..., -0.08998522,\n",
       "                0.71635836, -0.35954463],\n",
       "              [ 1.0361718 , -0.3459703 ,  0.16379182, ..., -0.12386973,\n",
       "                1.0585539 , -0.12208267],\n",
       "              [-0.14971656, -0.6165952 ,  1.8599666 , ...,  1.1847805 ,\n",
       "                0.23039146,  0.19084097]]], dtype=float32), DeviceArray([[[-0.00796167,  0.00224218, -0.056833  , ..., -0.00666395,\n",
       "                0.07963111, -0.04354007],\n",
       "              [-0.18982333,  0.67000085, -0.0191092 , ..., -0.12938733,\n",
       "                0.44278854,  0.14686549],\n",
       "              [ 0.7212103 , -1.3234464 ,  0.8889581 , ...,  0.43770424,\n",
       "                0.44503006,  0.4618078 ],\n",
       "              ...,\n",
       "              [-1.1840963 ,  0.16495584, -0.31633902, ...,  0.09413273,\n",
       "                0.6145007 , -0.60592276],\n",
       "              [ 1.437082  , -0.42137057, -0.11372834, ..., -0.20520565,\n",
       "                0.79617554, -0.31720668],\n",
       "              [-0.11033988, -0.36668754,  1.3584672 , ...,  1.097424  ,\n",
       "                0.43781856,  0.22790375]]], dtype=float32), DeviceArray([[[ 0.03174984, -0.04253286, -0.03305863, ..., -0.0265012 ,\n",
       "                0.01483483, -0.03807115],\n",
       "              [-0.03513146,  0.5021768 , -0.16368233, ..., -0.06847241,\n",
       "                0.62144464,  0.1482789 ],\n",
       "              [ 0.9205701 , -1.2733556 ,  1.0958843 , ...,  0.8229258 ,\n",
       "               -0.06502193,  0.35180113],\n",
       "              ...,\n",
       "              [-0.90861905,  0.16585524, -0.36521405, ...,  0.16890018,\n",
       "                0.37003806, -0.5276783 ],\n",
       "              [ 1.0123289 , -0.63077146, -0.4897899 , ..., -0.04948908,\n",
       "                0.5297685 , -0.68949586],\n",
       "              [-0.36263758, -0.29179114,  1.0026944 , ...,  0.8581504 ,\n",
       "                0.22964104,  0.35351577]]], dtype=float32), DeviceArray([[[ 2.7753593e-02, -6.9520116e-02,  1.0663047e-03, ...,\n",
       "               -2.2151049e-02, -2.5593098e-03, -2.5404343e-02],\n",
       "              [ 2.1455291e-01,  2.4469942e-01, -2.7139181e-01, ...,\n",
       "                1.5887484e-01,  7.9781497e-01, -6.4755879e-02],\n",
       "              [ 7.7462780e-01, -1.2354711e+00,  7.1136981e-01, ...,\n",
       "                5.8323568e-01, -1.4930186e-01,  4.8999786e-01],\n",
       "              ...,\n",
       "              [-5.4162550e-01,  2.3739645e-01, -5.6862867e-01, ...,\n",
       "                4.6021974e-01,  6.3849711e-01, -5.2664602e-01],\n",
       "              [ 1.1703638e+00, -4.4749931e-01, -7.7337831e-01, ...,\n",
       "                8.4172443e-02,  8.2415891e-01, -3.7718487e-01],\n",
       "              [-6.3712919e-01, -2.7708247e-01,  9.7969532e-01, ...,\n",
       "                8.1104243e-01,  4.5754033e-01,  4.5142239e-01]]],            dtype=float32), DeviceArray([[[ 0.03010474, -0.05956794,  0.01588638, ..., -0.02023844,\n",
       "               -0.00709177, -0.02211302],\n",
       "              [ 0.33165395,  0.08406791, -0.6088194 , ...,  0.21535441,\n",
       "                0.6212209 , -0.18109187],\n",
       "              [ 0.57396054, -1.0423073 ,  1.0520177 , ...,  0.19512247,\n",
       "               -0.14411119,  0.41313317],\n",
       "              ...,\n",
       "              [ 0.06551348,  0.45231348, -0.7511802 , ...,  0.48466974,\n",
       "                0.6389861 , -0.6178745 ],\n",
       "              [ 1.2828071 , -0.12604547, -0.61909616, ...,  0.03661691,\n",
       "                0.96956295, -0.26238456],\n",
       "              [-0.6068779 , -0.02253505,  1.1226914 , ...,  0.8172079 ,\n",
       "                0.60004187,  0.28151277]]], dtype=float32), DeviceArray([[[ 0.02124612, -0.05557089,  0.05969502, ..., -0.00997339,\n",
       "               -0.02529966, -0.01419653],\n",
       "              [ 0.6967744 ,  0.29604626, -0.6893321 , ...,  0.22582573,\n",
       "                0.53240204, -0.21024102],\n",
       "              [ 0.07142739, -1.0865965 ,  0.87159485, ...,  0.14043488,\n",
       "               -0.04797786,  0.43947124],\n",
       "              ...,\n",
       "              [ 0.78054005,  0.6586532 , -0.8819998 , ...,  0.2957295 ,\n",
       "                0.3461745 ,  0.00578999],\n",
       "              [ 1.610612  ,  0.15035641, -0.718687  , ...,  0.20420825,\n",
       "                0.90446323,  0.11152808],\n",
       "              [-0.36325285,  0.02647875,  1.2608554 , ...,  0.28302366,\n",
       "                0.5679273 ,  0.42426932]]], dtype=float32), DeviceArray([[[-0.00662902, -0.05041133,  0.03470002, ..., -0.02721693,\n",
       "               -0.03191705,  0.01392538],\n",
       "              [ 0.19232224,  0.38686728, -0.89175296, ...,  0.36166722,\n",
       "                0.64851993, -0.2229343 ],\n",
       "              [ 0.37065172, -1.1400685 ,  0.8825922 , ...,  0.0203626 ,\n",
       "               -0.22858699,  0.01623104],\n",
       "              ...,\n",
       "              [ 0.66736096,  0.598514  , -0.7159669 , ...,  0.06299006,\n",
       "                0.5592906 , -0.02730709],\n",
       "              [ 1.6720955 ,  0.03059234, -0.51498264, ...,  0.2507211 ,\n",
       "                0.8499655 ,  0.33180973],\n",
       "              [-0.09494431,  0.31677794,  1.1089826 , ..., -0.10341479,\n",
       "                0.79247427, -0.02327358]]], dtype=float32), DeviceArray([[[-0.01497872, -0.05133641,  0.02429494, ..., -0.04274584,\n",
       "               -0.02645815,  0.01456006],\n",
       "              [ 0.08418674,  0.17250015, -0.64353895, ...,  0.00368808,\n",
       "                0.50939006, -0.36048648],\n",
       "              [ 0.39088568, -1.345148  ,  0.78692013, ..., -0.35643855,\n",
       "                0.1820729 ,  0.33778724],\n",
       "              ...,\n",
       "              [ 0.8833507 ,  0.33264562, -0.88480294, ...,  0.4148804 ,\n",
       "                0.6662223 , -0.14565872],\n",
       "              [ 1.7483118 , -0.22035985, -0.50154   , ...,  0.54223335,\n",
       "                0.93895084,  0.48650968],\n",
       "              [ 0.05602477,  0.26835352,  0.9627355 , ...,  0.09848814,\n",
       "                0.5720685 , -0.05186738]]], dtype=float32), DeviceArray([[[-0.00692681, -0.06016624,  0.02980332, ..., -0.02928597,\n",
       "               -0.02480368,  0.01692992],\n",
       "              [ 0.14070463,  0.09114459,  0.12147157, ..., -0.09614856,\n",
       "                0.39828894, -0.28499877],\n",
       "              [ 0.48165   , -1.1320595 ,  0.7131319 , ..., -0.6162902 ,\n",
       "               -0.09101382,  0.14536369],\n",
       "              ...,\n",
       "              [ 0.45720828,  0.35059044, -0.71581006, ...,  0.33827284,\n",
       "                0.8423796 ,  0.14330082],\n",
       "              [ 1.5524014 ,  0.01561003, -0.32539397, ...,  0.08569263,\n",
       "                0.97970223,  0.72744477],\n",
       "              [ 0.02471882,  0.20508832,  1.062834  , ..., -0.08400936,\n",
       "                0.48135483,  0.40833306]]], dtype=float32), DeviceArray([[[-0.01458361, -0.06397342,  0.02747731, ..., -0.02632721,\n",
       "               -0.04210596,  0.01144213],\n",
       "              [ 0.00860975,  0.21595144,  0.40919966, ..., -0.39080447,\n",
       "                0.71668726,  0.01110596],\n",
       "              [ 0.69103926, -1.2800761 ,  0.8296205 , ..., -0.7195111 ,\n",
       "                0.03205485,  0.1339153 ],\n",
       "              ...,\n",
       "              [ 0.20661734,  0.13088486, -0.6594126 , ...,  0.506733  ,\n",
       "                0.97211134, -0.04134549],\n",
       "              [ 1.0978394 ,  0.4124716 , -0.23446219, ...,  0.23056895,\n",
       "                1.2977985 ,  0.6164185 ],\n",
       "              [-0.28531972,  0.45088506,  0.64070237, ...,  0.06111408,\n",
       "                0.46045798,  0.17227289]]], dtype=float32), DeviceArray([[[ 0.5546612 , -1.8649766 ,  0.9874723 , ..., -0.5667428 ,\n",
       "               -0.83543754,  0.02769691],\n",
       "              [ 0.29952812,  0.18621963,  0.48955423, ..., -1.2009684 ,\n",
       "                2.0850844 , -0.31623775],\n",
       "              [ 2.1645486 , -2.4089553 ,  1.7288736 , ..., -1.2961254 ,\n",
       "                0.4488714 , -0.1313912 ],\n",
       "              ...,\n",
       "              [ 0.42486537, -0.21351397,  0.23980474, ...,  0.10843197,\n",
       "                1.242966  ,  0.07191142],\n",
       "              [ 1.8247108 ,  1.4527673 ,  0.7270483 , ..., -0.7590579 ,\n",
       "                2.792979  ,  1.1297797 ],\n",
       "              [-0.61157143,  0.66341096,  0.66076785, ...,  0.12650286,\n",
       "                0.91739637, -0.06820086]]], dtype=float32)), decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=DeviceArray([[[-0.00565069, -0.04506724, -0.02707809, ...,  0.00201641,\n",
       "               -0.00540399,  0.03763571],\n",
       "              [-0.00570531, -0.04482179, -0.02721935, ...,  0.00211693,\n",
       "               -0.00483827,  0.03747253],\n",
       "              [ 0.19419202, -0.7112492 ,  0.19369853, ...,  0.34371915,\n",
       "               -0.4576627 , -0.2657903 ],\n",
       "              ...,\n",
       "              [ 0.30658963, -0.27588403, -0.24333625, ...,  0.67852193,\n",
       "                0.36913538,  0.02680317],\n",
       "              [ 0.18850312,  0.06072142, -0.02829674, ...,  0.5640527 ,\n",
       "                0.06140786,  0.25440294],\n",
       "              [ 0.18477756, -0.30004922, -0.20897329, ..., -0.09487949,\n",
       "                0.3451631 ,  0.39601344]]], dtype=float32), encoder_hidden_states=(DeviceArray([[[ 0.10707586, -0.09602189,  0.05214308, ...,  0.2914837 ,\n",
       "                0.12291211, -0.43456393],\n",
       "              [ 0.30015025, -0.1339403 ,  0.34517637, ...,  1.2576814 ,\n",
       "                1.0378296 , -0.2700727 ],\n",
       "              [ 0.4256205 , -1.1637211 ,  0.91384   , ..., -0.6921894 ,\n",
       "                0.22038129, -0.16539198],\n",
       "              ...,\n",
       "              [ 0.7619778 ,  0.44696206, -0.1647913 , ..., -0.64284885,\n",
       "                0.9229034 , -0.5802957 ],\n",
       "              [-0.16968386, -1.1507031 ,  0.4514702 , ...,  0.02304493,\n",
       "                0.5231793 ,  0.31463453],\n",
       "              [-0.1200065 , -0.0519992 , -0.00518003, ..., -0.3080709 ,\n",
       "                0.6460623 , -0.1283052 ]]], dtype=float32), DeviceArray([[[ 0.02247131,  0.07941604,  0.1948323 , ...,  0.41901812,\n",
       "               -0.07939051, -0.1087119 ],\n",
       "              [-0.05346794, -0.2611006 ,  0.18951198, ...,  1.1267086 ,\n",
       "                0.45392042, -0.25583303],\n",
       "              [-0.10296088, -1.4396913 ,  0.732158  , ..., -0.5130054 ,\n",
       "                0.50241184, -0.46921232],\n",
       "              ...,\n",
       "              [ 1.1244966 ,  0.13091229, -0.64078414, ..., -1.1946399 ,\n",
       "                1.189625  , -0.984071  ],\n",
       "              [-0.77845645, -0.6593824 ,  0.28062442, ...,  0.23402603,\n",
       "               -0.13300905, -0.13743639],\n",
       "              [-0.12819438, -0.04834033, -0.07256019, ..., -0.20785293,\n",
       "                0.24541292,  0.60087913]]], dtype=float32), DeviceArray([[[-0.24687546,  0.16079955,  0.03520738, ...,  0.00401403,\n",
       "               -0.18892762, -0.32359663],\n",
       "              [-0.10149385, -0.9113113 ,  0.37812716, ...,  1.4994212 ,\n",
       "               -0.04530587, -0.4296504 ],\n",
       "              [-0.69246024, -1.1527121 ,  0.7333899 , ..., -0.63410497,\n",
       "               -0.2955724 , -0.861195  ],\n",
       "              ...,\n",
       "              [ 0.9904696 , -0.15415424, -0.44418675, ..., -0.7348642 ,\n",
       "                1.3524383 , -1.2147272 ],\n",
       "              [-0.36632735, -0.65688646,  0.70499283, ...,  0.5481044 ,\n",
       "               -0.5355717 , -0.6944239 ],\n",
       "              [ 0.21865314,  0.13117132,  0.3538208 , ..., -0.45570526,\n",
       "                0.23579535,  0.35356763]]], dtype=float32), DeviceArray([[[-0.07917136,  0.10554951,  0.10382888, ..., -0.05416267,\n",
       "                0.14092547,  0.03354467],\n",
       "              [ 0.21902397, -0.80499923,  0.05989484, ...,  1.287396  ,\n",
       "                0.01822653, -0.3283902 ],\n",
       "              [-0.70676357, -0.6758366 ,  0.17277448, ..., -1.0238918 ,\n",
       "               -0.82701993, -0.52956927],\n",
       "              ...,\n",
       "              [ 0.6895688 , -0.08545014, -0.2615021 , ..., -0.48736724,\n",
       "                1.5128043 , -0.90354127],\n",
       "              [-0.11913875, -0.23785284,  1.7202275 , ...,  0.33323586,\n",
       "               -0.22018763, -0.25622302],\n",
       "              [-0.39524248,  0.0363726 ,  0.36648422, ..., -0.643233  ,\n",
       "                0.77050024,  0.8221887 ]]], dtype=float32), DeviceArray([[[-0.09368605,  0.06664128,  0.04473772, ..., -0.03039414,\n",
       "                0.07153903,  0.01511071],\n",
       "              [ 0.13752694, -0.55458087, -0.5479649 , ...,  0.8472802 ,\n",
       "                0.37592563, -0.41817278],\n",
       "              [-0.76180804, -0.7290179 , -0.09588906, ..., -0.4867342 ,\n",
       "               -0.8147502 , -0.19444728],\n",
       "              ...,\n",
       "              [ 0.07817678,  0.03491461, -0.20456584, ..., -0.34576792,\n",
       "                1.0025692 , -0.5851432 ],\n",
       "              [-0.5725891 , -0.80003726,  1.1252614 , ...,  0.5467266 ,\n",
       "               -0.35138828, -0.14860755],\n",
       "              [-0.56701815,  0.22833891,  0.57569313, ..., -0.40608495,\n",
       "                0.70840585,  0.22164132]]], dtype=float32), DeviceArray([[[-0.0737436 ,  0.06254588,  0.02309851, ..., -0.06725881,\n",
       "               -0.00873379,  0.01828456],\n",
       "              [ 0.16760868, -0.73469615, -0.43943426, ...,  0.86164683,\n",
       "                0.48685402, -0.28884226],\n",
       "              [-0.892503  , -0.6820287 ,  0.12615341, ..., -0.6501374 ,\n",
       "               -0.3525703 ,  0.09371606],\n",
       "              ...,\n",
       "              [-0.17399098, -0.5182053 , -0.4939642 , ..., -0.04591969,\n",
       "                1.2265807 , -0.30384964],\n",
       "              [-0.78135616, -0.38596198,  0.81965643, ...,  0.36152035,\n",
       "                0.2105824 ,  0.0156537 ],\n",
       "              [-0.3973098 ,  0.20012647,  0.30029166, ..., -0.37454778,\n",
       "                0.7906981 ,  0.3331025 ]]], dtype=float32), DeviceArray([[[-0.01490163,  0.02903511, -0.05138975, ..., -0.15207711,\n",
       "               -0.00816346,  0.0144785 ],\n",
       "              [ 0.28497455, -0.691478  , -0.39780122, ...,  0.5265135 ,\n",
       "                0.43932223, -0.34951133],\n",
       "              [-0.6089291 , -0.9006052 , -0.16954514, ..., -0.5759275 ,\n",
       "               -0.08456668, -0.22143061],\n",
       "              ...,\n",
       "              [-0.01968427, -0.86137974, -0.7157308 , ..., -0.25780904,\n",
       "                1.3261945 ,  0.02723351],\n",
       "              [-0.40519783, -0.5308202 ,  0.21762556, ..., -0.00912771,\n",
       "                0.21595412, -0.037339  ],\n",
       "              [-0.16516069, -0.2097003 ,  0.13667396, ..., -0.14760853,\n",
       "                1.0897939 ,  0.31588674]]], dtype=float32), DeviceArray([[[-0.09102628, -0.00490063, -0.08775676, ..., -0.11188497,\n",
       "                0.01161923,  0.00560199],\n",
       "              [ 0.3996476 , -0.57617486, -0.07358619, ...,  0.31441   ,\n",
       "                0.56366533, -0.19014251],\n",
       "              [-0.2115773 , -0.62384653,  0.11870801, ..., -0.33341146,\n",
       "                0.23532552,  0.16249415],\n",
       "              ...,\n",
       "              [-0.1456622 , -0.8986965 , -0.5661348 , ...,  0.11841154,\n",
       "                1.2195096 ,  0.24163046],\n",
       "              [-0.00945175, -0.27152264,  0.21824634, ...,  0.48631394,\n",
       "                0.8492751 ,  0.0990262 ],\n",
       "              [-0.19714311, -0.41090107,  0.05046774, ...,  0.07412332,\n",
       "                0.981598  ,  0.6260164 ]]], dtype=float32), DeviceArray([[[-0.05020963,  0.10659561, -0.07676826, ..., -0.03700664,\n",
       "               -0.00392935,  0.02592246],\n",
       "              [ 0.5199213 , -0.35326475, -0.16387862, ...,  0.31435332,\n",
       "                0.52009374, -0.0649244 ],\n",
       "              [ 0.24218932, -0.6876023 ,  0.15365732, ..., -0.3908682 ,\n",
       "                0.10591523, -0.2458947 ],\n",
       "              ...,\n",
       "              [-0.26182252, -0.83433074, -0.6312219 , ...,  0.43902004,\n",
       "                1.3211062 ,  0.5460066 ],\n",
       "              [ 0.18795204, -0.4407122 ,  0.11568713, ...,  1.1311351 ,\n",
       "                1.136042  ,  0.22124825],\n",
       "              [-0.13421553, -0.25751835, -0.18682024, ..., -0.12706852,\n",
       "                1.3018965 ,  0.48334312]]], dtype=float32), DeviceArray([[[ 0.05857422, -0.0972866 , -0.16782096, ..., -0.05136352,\n",
       "                0.01445211,  0.062418  ],\n",
       "              [ 0.3385392 , -0.18862148, -0.09686619, ...,  0.18003264,\n",
       "                0.17789377,  0.03339133],\n",
       "              [ 0.10930246, -0.7691555 ,  0.02086149, ..., -0.19355634,\n",
       "                0.18852508, -0.07797205],\n",
       "              ...,\n",
       "              [-0.13210487, -0.64175236, -0.5362357 , ...,  0.8637195 ,\n",
       "                1.014268  ,  0.07886054],\n",
       "              [ 0.09705959, -0.62199575,  0.15939191, ...,  1.2414513 ,\n",
       "                0.9719062 ,  0.16300176],\n",
       "              [ 0.21443342, -0.48881337, -0.17773822, ..., -0.08941987,\n",
       "                1.3162106 ,  0.70188445]]], dtype=float32), DeviceArray([[[ 0.03640016, -0.08115433, -0.10267673, ..., -0.05164602,\n",
       "               -0.03618464,  0.08042955],\n",
       "              [ 0.07097307, -0.04625664, -0.06233639, ..., -0.01669583,\n",
       "               -0.0099449 ,  0.09164999],\n",
       "              [ 0.39638346, -0.962401  ,  0.1123416 , ...,  0.06047405,\n",
       "               -0.14311369, -0.18138938],\n",
       "              ...,\n",
       "              [ 0.01221767,  0.02847271, -0.34521055, ...,  1.3706136 ,\n",
       "                0.9354418 ,  0.1440902 ],\n",
       "              [ 0.18393597, -0.02342267, -0.09645826, ...,  1.4042805 ,\n",
       "                0.69819   ,  0.29548937],\n",
       "              [ 0.36394346, -0.5567981 , -0.59792805, ..., -0.37640706,\n",
       "                1.2645428 ,  0.6744683 ]]], dtype=float32), DeviceArray([[[ 0.02162673, -0.07053047, -0.04371509, ..., -0.03758502,\n",
       "               -0.11205785,  0.08127103],\n",
       "              [ 0.02359237, -0.0688919 , -0.04099545, ..., -0.03621904,\n",
       "               -0.1054996 ,  0.07902186],\n",
       "              [ 0.03350875, -1.0925353 ,  0.09320319, ...,  0.32307   ,\n",
       "               -0.5671207 , -0.13281699],\n",
       "              ...,\n",
       "              [ 0.19142656, -0.4957745 , -0.10869542, ...,  1.2883238 ,\n",
       "                0.6356608 , -0.19315365],\n",
       "              [ 0.29610923, -0.19920087, -0.0463354 , ...,  1.1520318 ,\n",
       "                0.34370705,  0.24146403],\n",
       "              [ 0.54702055, -0.6392667 , -0.3707397 , ..., -0.34683317,\n",
       "                0.92917943,  0.75824773]]], dtype=float32), DeviceArray([[[-0.00565069, -0.04506724, -0.02707809, ...,  0.00201641,\n",
       "               -0.00540399,  0.03763571],\n",
       "              [-0.00570531, -0.04482179, -0.02721935, ...,  0.00211693,\n",
       "               -0.00483827,  0.03747253],\n",
       "              [ 0.19419202, -0.7112492 ,  0.19369853, ...,  0.34371915,\n",
       "               -0.4576627 , -0.2657903 ],\n",
       "              ...,\n",
       "              [ 0.30658963, -0.27588403, -0.24333625, ...,  0.67852193,\n",
       "                0.36913538,  0.02680317],\n",
       "              [ 0.18850312,  0.06072142, -0.02829674, ...,  0.5640527 ,\n",
       "                0.06140786,  0.25440294],\n",
       "              [ 0.18477756, -0.30004922, -0.20897329, ..., -0.09487949,\n",
       "                0.3451631 ,  0.39601344]]], dtype=float32)), encoder_attentions=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flax_model(**flax_inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d364b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1222172672 bytes == 0x1e3700000 @  0x7fcd8bd5d680 0x7fcd8bd7dbdd 0x7fcd7eae526f 0x7fcd7eaf4290 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaefd74 0x7fcd7eaf052e 0x503fb6 0x56b1da 0x56939a 0x5f6a13 0x56c28c 0x56939a 0x5f6a13 0x56c28c 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x56b1da 0x56939a 0x68d047 0x6003a4 0x5c4a40 0x56b0ae\n",
      "tcmalloc: large alloc 2460884992 bytes == 0x22c48e000 @  0x7fcd8bd5d680 0x7fcd8bd7dbdd 0x7fcd7eae526f 0x7fcd7eaf4290 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaf5324 0x7fcd7eaefd74 0x7fcd7eaf052e 0x503fb6 0x56b1da 0x56939a 0x5f6a13 0x56c28c 0x56939a 0x5f6a13 0x56c28c 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x56b1da 0x56939a 0x68d047 0x6003a4 0x5c4a40 0x56b0ae 0x5002d8\n",
      "tcmalloc: large alloc 1501732864 bytes == 0x19f4f4000 @  0x7fcd8bd5d680 0x7fcd8bd7e824 0x5f8a01 0x7fcd7eaefe19 0x7fcd7eaf052e 0x503fb6 0x56b1da 0x56939a 0x5f6a13 0x56c28c 0x56939a 0x5f6a13 0x56c28c 0x5f6836 0x56b0ae 0x56939a 0x5f6a13 0x56b1da 0x56939a 0x68d047 0x6003a4 0x5c4a40 0x56b0ae 0x5002d8 0x56cadf 0x5002d8 0x56cadf 0x5002d8 0x503fb6 0x56b1da 0x5f6836\n"
     ]
    }
   ],
   "source": [
    "# # 保存模型参数(随机初始化的)\n",
    "flax_model.save_pretrained(\"fnlp_jax_version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fcf1f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fnlp_jax_version/tokenizer_config.json',\n",
       " 'fnlp_jax_version/special_tokens_map.json',\n",
       " 'fnlp_jax_version/vocab.txt',\n",
       " 'fnlp_jax_version/added_tokens.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"fnlp_jax_version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678b1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8dfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
